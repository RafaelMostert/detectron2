Setup detectron2 logger
Import some common detectron2 utilities
Beginning of paths: /data1/mostertrij
Loaded configuration file ../configs/lofar_detection/precomputed_iterations_v3_constantLR.yaml
Experiment: precomputed
Rotation enabled: False
Precomputed bboxes: MIN_SIZE: 0
NAME: PrecomputedProposals
Output path: /data1/mostertrij/tridentnet/output/v3_precomputed_constantLR/
Attempt to load training data from: /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/
Load our data
Sample and plot input data as sanity check
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
Load model
[32m[12/18 23:28:34 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv2): Conv2d(
            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv3): Conv2d(
            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv2): Conv2d(
            2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv3): Conv2d(
            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv2): Conv2d(
            2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv3): Conv2d(
            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv2): Conv2d(
            2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv3): Conv2d(
            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=2, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
    )
  )
)
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
[32m[12/18 23:28:35 d2.data.build]: [0mRemoved 0 images with no usable annotations. 3618 images left.
[32m[12/18 23:28:35 d2.data.build]: [0mDistribution of instances among all 1 categories:
[36m|   category   | #instances   |
|:------------:|:-------------|
| radio_source | 3618         |
|              |              |[0m
[32m[12/18 23:28:35 d2.data.common]: [0mSerializing 3618 elements to byte tensors and concatenating them all ...
[32m[12/18 23:28:35 d2.data.common]: [0mSerialized dataset takes 33.05 MiB
[32m[12/18 23:28:35 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200,), max_size=200, sample_style='choice'), RandomFlip()]
[32m[12/18 23:28:35 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mRemapping C2 weights ......
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.conv1.norm.bias            loaded from res2_0_branch2a_bn_b              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.conv1.norm.running_mean    loaded from res2_0_branch2a_bn_rm             of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.conv1.norm.running_var     loaded from res2_0_branch2a_bn_riv            of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.conv1.norm.weight          loaded from res2_0_branch2a_bn_s              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.conv1.weight               loaded from res2_0_branch2a_w                 of shape (256, 64, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.conv2.norm.bias            loaded from res2_0_branch2b_bn_b              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.conv2.norm.running_mean    loaded from res2_0_branch2b_bn_rm             of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.conv2.norm.running_var     loaded from res2_0_branch2b_bn_riv            of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.conv2.norm.weight          loaded from res2_0_branch2b_bn_s              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.conv2.weight               loaded from res2_0_branch2b_w                 of shape (256, 8, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.conv3.norm.bias            loaded from res2_0_branch2c_bn_b              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.conv3.norm.running_mean    loaded from res2_0_branch2c_bn_rm             of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.conv3.norm.running_var     loaded from res2_0_branch2c_bn_riv            of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.conv3.norm.weight          loaded from res2_0_branch2c_bn_s              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.conv3.weight               loaded from res2_0_branch2c_w                 of shape (256, 256, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.shortcut.norm.bias         loaded from res2_0_branch1_bn_b               of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.shortcut.norm.running_mean loaded from res2_0_branch1_bn_rm              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.shortcut.norm.running_var  loaded from res2_0_branch1_bn_riv             of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.shortcut.norm.weight       loaded from res2_0_branch1_bn_s               of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.0.shortcut.weight            loaded from res2_0_branch1_w                  of shape (256, 64, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.1.conv1.norm.bias            loaded from res2_1_branch2a_bn_b              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.1.conv1.norm.running_mean    loaded from res2_1_branch2a_bn_rm             of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.1.conv1.norm.running_var     loaded from res2_1_branch2a_bn_riv            of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.1.conv1.norm.weight          loaded from res2_1_branch2a_bn_s              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.1.conv1.weight               loaded from res2_1_branch2a_w                 of shape (256, 256, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.1.conv2.norm.bias            loaded from res2_1_branch2b_bn_b              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.1.conv2.norm.running_mean    loaded from res2_1_branch2b_bn_rm             of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.1.conv2.norm.running_var     loaded from res2_1_branch2b_bn_riv            of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.1.conv2.norm.weight          loaded from res2_1_branch2b_bn_s              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.1.conv2.weight               loaded from res2_1_branch2b_w                 of shape (256, 8, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.1.conv3.norm.bias            loaded from res2_1_branch2c_bn_b              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.1.conv3.norm.running_mean    loaded from res2_1_branch2c_bn_rm             of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.1.conv3.norm.running_var     loaded from res2_1_branch2c_bn_riv            of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.1.conv3.norm.weight          loaded from res2_1_branch2c_bn_s              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.1.conv3.weight               loaded from res2_1_branch2c_w                 of shape (256, 256, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.2.conv1.norm.bias            loaded from res2_2_branch2a_bn_b              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.2.conv1.norm.running_mean    loaded from res2_2_branch2a_bn_rm             of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.2.conv1.norm.running_var     loaded from res2_2_branch2a_bn_riv            of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.2.conv1.norm.weight          loaded from res2_2_branch2a_bn_s              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.2.conv1.weight               loaded from res2_2_branch2a_w                 of shape (256, 256, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.2.conv2.norm.bias            loaded from res2_2_branch2b_bn_b              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.2.conv2.norm.running_mean    loaded from res2_2_branch2b_bn_rm             of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.2.conv2.norm.running_var     loaded from res2_2_branch2b_bn_riv            of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.2.conv2.norm.weight          loaded from res2_2_branch2b_bn_s              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.2.conv2.weight               loaded from res2_2_branch2b_w                 of shape (256, 8, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.2.conv3.norm.bias            loaded from res2_2_branch2c_bn_b              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.2.conv3.norm.running_mean    loaded from res2_2_branch2c_bn_rm             of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.2.conv3.norm.running_var     loaded from res2_2_branch2c_bn_riv            of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.2.conv3.norm.weight          loaded from res2_2_branch2c_bn_s              of shape (256,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res2.2.conv3.weight               loaded from res2_2_branch2c_w                 of shape (256, 256, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.conv1.norm.bias            loaded from res3_0_branch2a_bn_b              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.conv1.norm.running_mean    loaded from res3_0_branch2a_bn_rm             of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.conv1.norm.running_var     loaded from res3_0_branch2a_bn_riv            of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.conv1.norm.weight          loaded from res3_0_branch2a_bn_s              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.conv1.weight               loaded from res3_0_branch2a_w                 of shape (512, 256, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.conv2.norm.bias            loaded from res3_0_branch2b_bn_b              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.conv2.norm.running_mean    loaded from res3_0_branch2b_bn_rm             of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.conv2.norm.running_var     loaded from res3_0_branch2b_bn_riv            of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.conv2.norm.weight          loaded from res3_0_branch2b_bn_s              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.conv2.weight               loaded from res3_0_branch2b_w                 of shape (512, 16, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.conv3.norm.bias            loaded from res3_0_branch2c_bn_b              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.conv3.norm.running_mean    loaded from res3_0_branch2c_bn_rm             of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.conv3.norm.running_var     loaded from res3_0_branch2c_bn_riv            of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.conv3.norm.weight          loaded from res3_0_branch2c_bn_s              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.conv3.weight               loaded from res3_0_branch2c_w                 of shape (512, 512, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.shortcut.norm.bias         loaded from res3_0_branch1_bn_b               of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.shortcut.norm.running_mean loaded from res3_0_branch1_bn_rm              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.shortcut.norm.running_var  loaded from res3_0_branch1_bn_riv             of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.shortcut.norm.weight       loaded from res3_0_branch1_bn_s               of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.0.shortcut.weight            loaded from res3_0_branch1_w                  of shape (512, 256, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.1.conv1.norm.bias            loaded from res3_1_branch2a_bn_b              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.1.conv1.norm.running_mean    loaded from res3_1_branch2a_bn_rm             of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.1.conv1.norm.running_var     loaded from res3_1_branch2a_bn_riv            of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.1.conv1.norm.weight          loaded from res3_1_branch2a_bn_s              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.1.conv1.weight               loaded from res3_1_branch2a_w                 of shape (512, 512, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.1.conv2.norm.bias            loaded from res3_1_branch2b_bn_b              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.1.conv2.norm.running_mean    loaded from res3_1_branch2b_bn_rm             of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.1.conv2.norm.running_var     loaded from res3_1_branch2b_bn_riv            of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.1.conv2.norm.weight          loaded from res3_1_branch2b_bn_s              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.1.conv2.weight               loaded from res3_1_branch2b_w                 of shape (512, 16, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.1.conv3.norm.bias            loaded from res3_1_branch2c_bn_b              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.1.conv3.norm.running_mean    loaded from res3_1_branch2c_bn_rm             of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.1.conv3.norm.running_var     loaded from res3_1_branch2c_bn_riv            of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.1.conv3.norm.weight          loaded from res3_1_branch2c_bn_s              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.1.conv3.weight               loaded from res3_1_branch2c_w                 of shape (512, 512, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.2.conv1.norm.bias            loaded from res3_2_branch2a_bn_b              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.2.conv1.norm.running_mean    loaded from res3_2_branch2a_bn_rm             of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.2.conv1.norm.running_var     loaded from res3_2_branch2a_bn_riv            of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.2.conv1.norm.weight          loaded from res3_2_branch2a_bn_s              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.2.conv1.weight               loaded from res3_2_branch2a_w                 of shape (512, 512, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.2.conv2.norm.bias            loaded from res3_2_branch2b_bn_b              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.2.conv2.norm.running_mean    loaded from res3_2_branch2b_bn_rm             of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.2.conv2.norm.running_var     loaded from res3_2_branch2b_bn_riv            of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.2.conv2.norm.weight          loaded from res3_2_branch2b_bn_s              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.2.conv2.weight               loaded from res3_2_branch2b_w                 of shape (512, 16, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.2.conv3.norm.bias            loaded from res3_2_branch2c_bn_b              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.2.conv3.norm.running_mean    loaded from res3_2_branch2c_bn_rm             of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.2.conv3.norm.running_var     loaded from res3_2_branch2c_bn_riv            of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.2.conv3.norm.weight          loaded from res3_2_branch2c_bn_s              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.2.conv3.weight               loaded from res3_2_branch2c_w                 of shape (512, 512, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.3.conv1.norm.bias            loaded from res3_3_branch2a_bn_b              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.3.conv1.norm.running_mean    loaded from res3_3_branch2a_bn_rm             of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.3.conv1.norm.running_var     loaded from res3_3_branch2a_bn_riv            of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.3.conv1.norm.weight          loaded from res3_3_branch2a_bn_s              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.3.conv1.weight               loaded from res3_3_branch2a_w                 of shape (512, 512, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.3.conv2.norm.bias            loaded from res3_3_branch2b_bn_b              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.3.conv2.norm.running_mean    loaded from res3_3_branch2b_bn_rm             of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.3.conv2.norm.running_var     loaded from res3_3_branch2b_bn_riv            of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.3.conv2.norm.weight          loaded from res3_3_branch2b_bn_s              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.3.conv2.weight               loaded from res3_3_branch2b_w                 of shape (512, 16, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.3.conv3.norm.bias            loaded from res3_3_branch2c_bn_b              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.3.conv3.norm.running_mean    loaded from res3_3_branch2c_bn_rm             of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.3.conv3.norm.running_var     loaded from res3_3_branch2c_bn_riv            of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.3.conv3.norm.weight          loaded from res3_3_branch2c_bn_s              of shape (512,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res3.3.conv3.weight               loaded from res3_3_branch2c_w                 of shape (512, 512, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.conv1.norm.bias            loaded from res4_0_branch2a_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.conv1.norm.running_mean    loaded from res4_0_branch2a_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.conv1.norm.running_var     loaded from res4_0_branch2a_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.conv1.norm.weight          loaded from res4_0_branch2a_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.conv1.weight               loaded from res4_0_branch2a_w                 of shape (1024, 512, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.conv2.norm.bias            loaded from res4_0_branch2b_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.conv2.norm.running_mean    loaded from res4_0_branch2b_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.conv2.norm.running_var     loaded from res4_0_branch2b_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.conv2.norm.weight          loaded from res4_0_branch2b_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.conv2.weight               loaded from res4_0_branch2b_w                 of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.conv3.norm.bias            loaded from res4_0_branch2c_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.conv3.norm.running_mean    loaded from res4_0_branch2c_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.conv3.norm.running_var     loaded from res4_0_branch2c_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.conv3.norm.weight          loaded from res4_0_branch2c_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.conv3.weight               loaded from res4_0_branch2c_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.shortcut.norm.bias         loaded from res4_0_branch1_bn_b               of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.shortcut.norm.running_mean loaded from res4_0_branch1_bn_rm              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.shortcut.norm.running_var  loaded from res4_0_branch1_bn_riv             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.shortcut.norm.weight       loaded from res4_0_branch1_bn_s               of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.0.shortcut.weight            loaded from res4_0_branch1_w                  of shape (1024, 512, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.1.conv1.norm.bias            loaded from res4_1_branch2a_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.1.conv1.norm.running_mean    loaded from res4_1_branch2a_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.1.conv1.norm.running_var     loaded from res4_1_branch2a_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.1.conv1.norm.weight          loaded from res4_1_branch2a_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.1.conv1.weight               loaded from res4_1_branch2a_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.1.conv2.norm.bias            loaded from res4_1_branch2b_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.1.conv2.norm.running_mean    loaded from res4_1_branch2b_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.1.conv2.norm.running_var     loaded from res4_1_branch2b_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.1.conv2.norm.weight          loaded from res4_1_branch2b_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.1.conv2.weight               loaded from res4_1_branch2b_w                 of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.1.conv3.norm.bias            loaded from res4_1_branch2c_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.1.conv3.norm.running_mean    loaded from res4_1_branch2c_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.1.conv3.norm.running_var     loaded from res4_1_branch2c_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.1.conv3.norm.weight          loaded from res4_1_branch2c_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.1.conv3.weight               loaded from res4_1_branch2c_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.10.conv1.norm.bias           loaded from res4_10_branch2a_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.10.conv1.norm.running_mean   loaded from res4_10_branch2a_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.10.conv1.norm.running_var    loaded from res4_10_branch2a_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.10.conv1.norm.weight         loaded from res4_10_branch2a_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.10.conv1.weight              loaded from res4_10_branch2a_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.10.conv2.norm.bias           loaded from res4_10_branch2b_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.10.conv2.norm.running_mean   loaded from res4_10_branch2b_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.10.conv2.norm.running_var    loaded from res4_10_branch2b_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.10.conv2.norm.weight         loaded from res4_10_branch2b_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.10.conv2.weight              loaded from res4_10_branch2b_w                of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.10.conv3.norm.bias           loaded from res4_10_branch2c_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.10.conv3.norm.running_mean   loaded from res4_10_branch2c_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.10.conv3.norm.running_var    loaded from res4_10_branch2c_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.10.conv3.norm.weight         loaded from res4_10_branch2c_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.10.conv3.weight              loaded from res4_10_branch2c_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.11.conv1.norm.bias           loaded from res4_11_branch2a_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.11.conv1.norm.running_mean   loaded from res4_11_branch2a_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.11.conv1.norm.running_var    loaded from res4_11_branch2a_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.11.conv1.norm.weight         loaded from res4_11_branch2a_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.11.conv1.weight              loaded from res4_11_branch2a_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.11.conv2.norm.bias           loaded from res4_11_branch2b_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.11.conv2.norm.running_mean   loaded from res4_11_branch2b_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.11.conv2.norm.running_var    loaded from res4_11_branch2b_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.11.conv2.norm.weight         loaded from res4_11_branch2b_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.11.conv2.weight              loaded from res4_11_branch2b_w                of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.11.conv3.norm.bias           loaded from res4_11_branch2c_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.11.conv3.norm.running_mean   loaded from res4_11_branch2c_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.11.conv3.norm.running_var    loaded from res4_11_branch2c_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.11.conv3.norm.weight         loaded from res4_11_branch2c_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.11.conv3.weight              loaded from res4_11_branch2c_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.12.conv1.norm.bias           loaded from res4_12_branch2a_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.12.conv1.norm.running_mean   loaded from res4_12_branch2a_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.12.conv1.norm.running_var    loaded from res4_12_branch2a_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.12.conv1.norm.weight         loaded from res4_12_branch2a_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.12.conv1.weight              loaded from res4_12_branch2a_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.12.conv2.norm.bias           loaded from res4_12_branch2b_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.12.conv2.norm.running_mean   loaded from res4_12_branch2b_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.12.conv2.norm.running_var    loaded from res4_12_branch2b_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.12.conv2.norm.weight         loaded from res4_12_branch2b_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.12.conv2.weight              loaded from res4_12_branch2b_w                of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.12.conv3.norm.bias           loaded from res4_12_branch2c_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.12.conv3.norm.running_mean   loaded from res4_12_branch2c_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.12.conv3.norm.running_var    loaded from res4_12_branch2c_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.12.conv3.norm.weight         loaded from res4_12_branch2c_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.12.conv3.weight              loaded from res4_12_branch2c_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.13.conv1.norm.bias           loaded from res4_13_branch2a_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.13.conv1.norm.running_mean   loaded from res4_13_branch2a_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.13.conv1.norm.running_var    loaded from res4_13_branch2a_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.13.conv1.norm.weight         loaded from res4_13_branch2a_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.13.conv1.weight              loaded from res4_13_branch2a_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.13.conv2.norm.bias           loaded from res4_13_branch2b_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.13.conv2.norm.running_mean   loaded from res4_13_branch2b_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.13.conv2.norm.running_var    loaded from res4_13_branch2b_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.13.conv2.norm.weight         loaded from res4_13_branch2b_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.13.conv2.weight              loaded from res4_13_branch2b_w                of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.13.conv3.norm.bias           loaded from res4_13_branch2c_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.13.conv3.norm.running_mean   loaded from res4_13_branch2c_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.13.conv3.norm.running_var    loaded from res4_13_branch2c_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.13.conv3.norm.weight         loaded from res4_13_branch2c_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.13.conv3.weight              loaded from res4_13_branch2c_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.14.conv1.norm.bias           loaded from res4_14_branch2a_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.14.conv1.norm.running_mean   loaded from res4_14_branch2a_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.14.conv1.norm.running_var    loaded from res4_14_branch2a_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.14.conv1.norm.weight         loaded from res4_14_branch2a_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.14.conv1.weight              loaded from res4_14_branch2a_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.14.conv2.norm.bias           loaded from res4_14_branch2b_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.14.conv2.norm.running_mean   loaded from res4_14_branch2b_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.14.conv2.norm.running_var    loaded from res4_14_branch2b_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.14.conv2.norm.weight         loaded from res4_14_branch2b_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.14.conv2.weight              loaded from res4_14_branch2b_w                of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.14.conv3.norm.bias           loaded from res4_14_branch2c_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.14.conv3.norm.running_mean   loaded from res4_14_branch2c_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.14.conv3.norm.running_var    loaded from res4_14_branch2c_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.14.conv3.norm.weight         loaded from res4_14_branch2c_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.14.conv3.weight              loaded from res4_14_branch2c_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.15.conv1.norm.bias           loaded from res4_15_branch2a_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.15.conv1.norm.running_mean   loaded from res4_15_branch2a_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.15.conv1.norm.running_var    loaded from res4_15_branch2a_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.15.conv1.norm.weight         loaded from res4_15_branch2a_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.15.conv1.weight              loaded from res4_15_branch2a_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.15.conv2.norm.bias           loaded from res4_15_branch2b_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.15.conv2.norm.running_mean   loaded from res4_15_branch2b_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.15.conv2.norm.running_var    loaded from res4_15_branch2b_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.15.conv2.norm.weight         loaded from res4_15_branch2b_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.15.conv2.weight              loaded from res4_15_branch2b_w                of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.15.conv3.norm.bias           loaded from res4_15_branch2c_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.15.conv3.norm.running_mean   loaded from res4_15_branch2c_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.15.conv3.norm.running_var    loaded from res4_15_branch2c_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.15.conv3.norm.weight         loaded from res4_15_branch2c_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.15.conv3.weight              loaded from res4_15_branch2c_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.16.conv1.norm.bias           loaded from res4_16_branch2a_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.16.conv1.norm.running_mean   loaded from res4_16_branch2a_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.16.conv1.norm.running_var    loaded from res4_16_branch2a_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.16.conv1.norm.weight         loaded from res4_16_branch2a_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.16.conv1.weight              loaded from res4_16_branch2a_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.16.conv2.norm.bias           loaded from res4_16_branch2b_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.16.conv2.norm.running_mean   loaded from res4_16_branch2b_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.16.conv2.norm.running_var    loaded from res4_16_branch2b_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.16.conv2.norm.weight         loaded from res4_16_branch2b_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.16.conv2.weight              loaded from res4_16_branch2b_w                of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.16.conv3.norm.bias           loaded from res4_16_branch2c_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.16.conv3.norm.running_mean   loaded from res4_16_branch2c_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.16.conv3.norm.running_var    loaded from res4_16_branch2c_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.16.conv3.norm.weight         loaded from res4_16_branch2c_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.16.conv3.weight              loaded from res4_16_branch2c_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.17.conv1.norm.bias           loaded from res4_17_branch2a_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.17.conv1.norm.running_mean   loaded from res4_17_branch2a_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.17.conv1.norm.running_var    loaded from res4_17_branch2a_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.17.conv1.norm.weight         loaded from res4_17_branch2a_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.17.conv1.weight              loaded from res4_17_branch2a_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.17.conv2.norm.bias           loaded from res4_17_branch2b_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.17.conv2.norm.running_mean   loaded from res4_17_branch2b_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.17.conv2.norm.running_var    loaded from res4_17_branch2b_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.17.conv2.norm.weight         loaded from res4_17_branch2b_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.17.conv2.weight              loaded from res4_17_branch2b_w                of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.17.conv3.norm.bias           loaded from res4_17_branch2c_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.17.conv3.norm.running_mean   loaded from res4_17_branch2c_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.17.conv3.norm.running_var    loaded from res4_17_branch2c_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.17.conv3.norm.weight         loaded from res4_17_branch2c_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.17.conv3.weight              loaded from res4_17_branch2c_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.18.conv1.norm.bias           loaded from res4_18_branch2a_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.18.conv1.norm.running_mean   loaded from res4_18_branch2a_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.18.conv1.norm.running_var    loaded from res4_18_branch2a_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.18.conv1.norm.weight         loaded from res4_18_branch2a_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.18.conv1.weight              loaded from res4_18_branch2a_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.18.conv2.norm.bias           loaded from res4_18_branch2b_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.18.conv2.norm.running_mean   loaded from res4_18_branch2b_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.18.conv2.norm.running_var    loaded from res4_18_branch2b_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.18.conv2.norm.weight         loaded from res4_18_branch2b_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.18.conv2.weight              loaded from res4_18_branch2b_w                of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.18.conv3.norm.bias           loaded from res4_18_branch2c_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.18.conv3.norm.running_mean   loaded from res4_18_branch2c_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.18.conv3.norm.running_var    loaded from res4_18_branch2c_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.18.conv3.norm.weight         loaded from res4_18_branch2c_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.18.conv3.weight              loaded from res4_18_branch2c_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.19.conv1.norm.bias           loaded from res4_19_branch2a_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.19.conv1.norm.running_mean   loaded from res4_19_branch2a_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.19.conv1.norm.running_var    loaded from res4_19_branch2a_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.19.conv1.norm.weight         loaded from res4_19_branch2a_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.19.conv1.weight              loaded from res4_19_branch2a_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.19.conv2.norm.bias           loaded from res4_19_branch2b_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.19.conv2.norm.running_mean   loaded from res4_19_branch2b_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.19.conv2.norm.running_var    loaded from res4_19_branch2b_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.19.conv2.norm.weight         loaded from res4_19_branch2b_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.19.conv2.weight              loaded from res4_19_branch2b_w                of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.19.conv3.norm.bias           loaded from res4_19_branch2c_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.19.conv3.norm.running_mean   loaded from res4_19_branch2c_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.19.conv3.norm.running_var    loaded from res4_19_branch2c_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.19.conv3.norm.weight         loaded from res4_19_branch2c_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.19.conv3.weight              loaded from res4_19_branch2c_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.2.conv1.norm.bias            loaded from res4_2_branch2a_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.2.conv1.norm.running_mean    loaded from res4_2_branch2a_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.2.conv1.norm.running_var     loaded from res4_2_branch2a_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.2.conv1.norm.weight          loaded from res4_2_branch2a_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.2.conv1.weight               loaded from res4_2_branch2a_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.2.conv2.norm.bias            loaded from res4_2_branch2b_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.2.conv2.norm.running_mean    loaded from res4_2_branch2b_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.2.conv2.norm.running_var     loaded from res4_2_branch2b_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.2.conv2.norm.weight          loaded from res4_2_branch2b_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.2.conv2.weight               loaded from res4_2_branch2b_w                 of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.2.conv3.norm.bias            loaded from res4_2_branch2c_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.2.conv3.norm.running_mean    loaded from res4_2_branch2c_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.2.conv3.norm.running_var     loaded from res4_2_branch2c_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.2.conv3.norm.weight          loaded from res4_2_branch2c_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.2.conv3.weight               loaded from res4_2_branch2c_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.20.conv1.norm.bias           loaded from res4_20_branch2a_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.20.conv1.norm.running_mean   loaded from res4_20_branch2a_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.20.conv1.norm.running_var    loaded from res4_20_branch2a_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.20.conv1.norm.weight         loaded from res4_20_branch2a_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.20.conv1.weight              loaded from res4_20_branch2a_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.20.conv2.norm.bias           loaded from res4_20_branch2b_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.20.conv2.norm.running_mean   loaded from res4_20_branch2b_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.20.conv2.norm.running_var    loaded from res4_20_branch2b_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.20.conv2.norm.weight         loaded from res4_20_branch2b_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.20.conv2.weight              loaded from res4_20_branch2b_w                of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.20.conv3.norm.bias           loaded from res4_20_branch2c_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.20.conv3.norm.running_mean   loaded from res4_20_branch2c_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.20.conv3.norm.running_var    loaded from res4_20_branch2c_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.20.conv3.norm.weight         loaded from res4_20_branch2c_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.20.conv3.weight              loaded from res4_20_branch2c_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.21.conv1.norm.bias           loaded from res4_21_branch2a_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.21.conv1.norm.running_mean   loaded from res4_21_branch2a_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.21.conv1.norm.running_var    loaded from res4_21_branch2a_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.21.conv1.norm.weight         loaded from res4_21_branch2a_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.21.conv1.weight              loaded from res4_21_branch2a_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.21.conv2.norm.bias           loaded from res4_21_branch2b_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.21.conv2.norm.running_mean   loaded from res4_21_branch2b_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.21.conv2.norm.running_var    loaded from res4_21_branch2b_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.21.conv2.norm.weight         loaded from res4_21_branch2b_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.21.conv2.weight              loaded from res4_21_branch2b_w                of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.21.conv3.norm.bias           loaded from res4_21_branch2c_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.21.conv3.norm.running_mean   loaded from res4_21_branch2c_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.21.conv3.norm.running_var    loaded from res4_21_branch2c_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.21.conv3.norm.weight         loaded from res4_21_branch2c_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.21.conv3.weight              loaded from res4_21_branch2c_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.22.conv1.norm.bias           loaded from res4_22_branch2a_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.22.conv1.norm.running_mean   loaded from res4_22_branch2a_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.22.conv1.norm.running_var    loaded from res4_22_branch2a_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.22.conv1.norm.weight         loaded from res4_22_branch2a_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.22.conv1.weight              loaded from res4_22_branch2a_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.22.conv2.norm.bias           loaded from res4_22_branch2b_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.22.conv2.norm.running_mean   loaded from res4_22_branch2b_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.22.conv2.norm.running_var    loaded from res4_22_branch2b_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.22.conv2.norm.weight         loaded from res4_22_branch2b_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.22.conv2.weight              loaded from res4_22_branch2b_w                of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.22.conv3.norm.bias           loaded from res4_22_branch2c_bn_b             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.22.conv3.norm.running_mean   loaded from res4_22_branch2c_bn_rm            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.22.conv3.norm.running_var    loaded from res4_22_branch2c_bn_riv           of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.22.conv3.norm.weight         loaded from res4_22_branch2c_bn_s             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.22.conv3.weight              loaded from res4_22_branch2c_w                of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.3.conv1.norm.bias            loaded from res4_3_branch2a_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.3.conv1.norm.running_mean    loaded from res4_3_branch2a_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.3.conv1.norm.running_var     loaded from res4_3_branch2a_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.3.conv1.norm.weight          loaded from res4_3_branch2a_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.3.conv1.weight               loaded from res4_3_branch2a_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.3.conv2.norm.bias            loaded from res4_3_branch2b_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.3.conv2.norm.running_mean    loaded from res4_3_branch2b_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.3.conv2.norm.running_var     loaded from res4_3_branch2b_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.3.conv2.norm.weight          loaded from res4_3_branch2b_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.3.conv2.weight               loaded from res4_3_branch2b_w                 of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.3.conv3.norm.bias            loaded from res4_3_branch2c_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.3.conv3.norm.running_mean    loaded from res4_3_branch2c_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.3.conv3.norm.running_var     loaded from res4_3_branch2c_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.3.conv3.norm.weight          loaded from res4_3_branch2c_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.3.conv3.weight               loaded from res4_3_branch2c_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.4.conv1.norm.bias            loaded from res4_4_branch2a_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.4.conv1.norm.running_mean    loaded from res4_4_branch2a_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.4.conv1.norm.running_var     loaded from res4_4_branch2a_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.4.conv1.norm.weight          loaded from res4_4_branch2a_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.4.conv1.weight               loaded from res4_4_branch2a_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.4.conv2.norm.bias            loaded from res4_4_branch2b_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.4.conv2.norm.running_mean    loaded from res4_4_branch2b_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.4.conv2.norm.running_var     loaded from res4_4_branch2b_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.4.conv2.norm.weight          loaded from res4_4_branch2b_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.4.conv2.weight               loaded from res4_4_branch2b_w                 of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.4.conv3.norm.bias            loaded from res4_4_branch2c_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.4.conv3.norm.running_mean    loaded from res4_4_branch2c_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.4.conv3.norm.running_var     loaded from res4_4_branch2c_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.4.conv3.norm.weight          loaded from res4_4_branch2c_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.4.conv3.weight               loaded from res4_4_branch2c_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.5.conv1.norm.bias            loaded from res4_5_branch2a_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.5.conv1.norm.running_mean    loaded from res4_5_branch2a_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.5.conv1.norm.running_var     loaded from res4_5_branch2a_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.5.conv1.norm.weight          loaded from res4_5_branch2a_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.5.conv1.weight               loaded from res4_5_branch2a_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.5.conv2.norm.bias            loaded from res4_5_branch2b_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.5.conv2.norm.running_mean    loaded from res4_5_branch2b_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.5.conv2.norm.running_var     loaded from res4_5_branch2b_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.5.conv2.norm.weight          loaded from res4_5_branch2b_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.5.conv2.weight               loaded from res4_5_branch2b_w                 of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.5.conv3.norm.bias            loaded from res4_5_branch2c_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.5.conv3.norm.running_mean    loaded from res4_5_branch2c_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.5.conv3.norm.running_var     loaded from res4_5_branch2c_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.5.conv3.norm.weight          loaded from res4_5_branch2c_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.5.conv3.weight               loaded from res4_5_branch2c_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.6.conv1.norm.bias            loaded from res4_6_branch2a_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.6.conv1.norm.running_mean    loaded from res4_6_branch2a_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.6.conv1.norm.running_var     loaded from res4_6_branch2a_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.6.conv1.norm.weight          loaded from res4_6_branch2a_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.6.conv1.weight               loaded from res4_6_branch2a_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.6.conv2.norm.bias            loaded from res4_6_branch2b_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.6.conv2.norm.running_mean    loaded from res4_6_branch2b_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.6.conv2.norm.running_var     loaded from res4_6_branch2b_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.6.conv2.norm.weight          loaded from res4_6_branch2b_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.6.conv2.weight               loaded from res4_6_branch2b_w                 of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.6.conv3.norm.bias            loaded from res4_6_branch2c_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.6.conv3.norm.running_mean    loaded from res4_6_branch2c_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.6.conv3.norm.running_var     loaded from res4_6_branch2c_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.6.conv3.norm.weight          loaded from res4_6_branch2c_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.6.conv3.weight               loaded from res4_6_branch2c_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.7.conv1.norm.bias            loaded from res4_7_branch2a_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.7.conv1.norm.running_mean    loaded from res4_7_branch2a_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.7.conv1.norm.running_var     loaded from res4_7_branch2a_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.7.conv1.norm.weight          loaded from res4_7_branch2a_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.7.conv1.weight               loaded from res4_7_branch2a_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.7.conv2.norm.bias            loaded from res4_7_branch2b_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.7.conv2.norm.running_mean    loaded from res4_7_branch2b_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.7.conv2.norm.running_var     loaded from res4_7_branch2b_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.7.conv2.norm.weight          loaded from res4_7_branch2b_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.7.conv2.weight               loaded from res4_7_branch2b_w                 of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.7.conv3.norm.bias            loaded from res4_7_branch2c_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.7.conv3.norm.running_mean    loaded from res4_7_branch2c_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.7.conv3.norm.running_var     loaded from res4_7_branch2c_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.7.conv3.norm.weight          loaded from res4_7_branch2c_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.7.conv3.weight               loaded from res4_7_branch2c_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.8.conv1.norm.bias            loaded from res4_8_branch2a_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.8.conv1.norm.running_mean    loaded from res4_8_branch2a_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.8.conv1.norm.running_var     loaded from res4_8_branch2a_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.8.conv1.norm.weight          loaded from res4_8_branch2a_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.8.conv1.weight               loaded from res4_8_branch2a_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.8.conv2.norm.bias            loaded from res4_8_branch2b_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.8.conv2.norm.running_mean    loaded from res4_8_branch2b_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.8.conv2.norm.running_var     loaded from res4_8_branch2b_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.8.conv2.norm.weight          loaded from res4_8_branch2b_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.8.conv2.weight               loaded from res4_8_branch2b_w                 of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.8.conv3.norm.bias            loaded from res4_8_branch2c_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.8.conv3.norm.running_mean    loaded from res4_8_branch2c_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.8.conv3.norm.running_var     loaded from res4_8_branch2c_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.8.conv3.norm.weight          loaded from res4_8_branch2c_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.8.conv3.weight               loaded from res4_8_branch2c_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.9.conv1.norm.bias            loaded from res4_9_branch2a_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.9.conv1.norm.running_mean    loaded from res4_9_branch2a_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.9.conv1.norm.running_var     loaded from res4_9_branch2a_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.9.conv1.norm.weight          loaded from res4_9_branch2a_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.9.conv1.weight               loaded from res4_9_branch2a_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.9.conv2.norm.bias            loaded from res4_9_branch2b_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.9.conv2.norm.running_mean    loaded from res4_9_branch2b_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.9.conv2.norm.running_var     loaded from res4_9_branch2b_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.9.conv2.norm.weight          loaded from res4_9_branch2b_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.9.conv2.weight               loaded from res4_9_branch2b_w                 of shape (1024, 32, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.9.conv3.norm.bias            loaded from res4_9_branch2c_bn_b              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.9.conv3.norm.running_mean    loaded from res4_9_branch2c_bn_rm             of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.9.conv3.norm.running_var     loaded from res4_9_branch2c_bn_riv            of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.9.conv3.norm.weight          loaded from res4_9_branch2c_bn_s              of shape (1024,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res4.9.conv3.weight               loaded from res4_9_branch2c_w                 of shape (1024, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.conv1.norm.bias            loaded from res5_0_branch2a_bn_b              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.conv1.norm.running_mean    loaded from res5_0_branch2a_bn_rm             of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.conv1.norm.running_var     loaded from res5_0_branch2a_bn_riv            of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.conv1.norm.weight          loaded from res5_0_branch2a_bn_s              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.conv1.weight               loaded from res5_0_branch2a_w                 of shape (2048, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.conv2.norm.bias            loaded from res5_0_branch2b_bn_b              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.conv2.norm.running_mean    loaded from res5_0_branch2b_bn_rm             of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.conv2.norm.running_var     loaded from res5_0_branch2b_bn_riv            of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.conv2.norm.weight          loaded from res5_0_branch2b_bn_s              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.conv2.weight               loaded from res5_0_branch2b_w                 of shape (2048, 64, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.conv3.norm.bias            loaded from res5_0_branch2c_bn_b              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.conv3.norm.running_mean    loaded from res5_0_branch2c_bn_rm             of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.conv3.norm.running_var     loaded from res5_0_branch2c_bn_riv            of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.conv3.norm.weight          loaded from res5_0_branch2c_bn_s              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.conv3.weight               loaded from res5_0_branch2c_w                 of shape (2048, 2048, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.shortcut.norm.bias         loaded from res5_0_branch1_bn_b               of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.shortcut.norm.running_mean loaded from res5_0_branch1_bn_rm              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.shortcut.norm.running_var  loaded from res5_0_branch1_bn_riv             of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.shortcut.norm.weight       loaded from res5_0_branch1_bn_s               of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.0.shortcut.weight            loaded from res5_0_branch1_w                  of shape (2048, 1024, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.1.conv1.norm.bias            loaded from res5_1_branch2a_bn_b              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.1.conv1.norm.running_mean    loaded from res5_1_branch2a_bn_rm             of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.1.conv1.norm.running_var     loaded from res5_1_branch2a_bn_riv            of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.1.conv1.norm.weight          loaded from res5_1_branch2a_bn_s              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.1.conv1.weight               loaded from res5_1_branch2a_w                 of shape (2048, 2048, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.1.conv2.norm.bias            loaded from res5_1_branch2b_bn_b              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.1.conv2.norm.running_mean    loaded from res5_1_branch2b_bn_rm             of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.1.conv2.norm.running_var     loaded from res5_1_branch2b_bn_riv            of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.1.conv2.norm.weight          loaded from res5_1_branch2b_bn_s              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.1.conv2.weight               loaded from res5_1_branch2b_w                 of shape (2048, 64, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.1.conv3.norm.bias            loaded from res5_1_branch2c_bn_b              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.1.conv3.norm.running_mean    loaded from res5_1_branch2c_bn_rm             of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.1.conv3.norm.running_var     loaded from res5_1_branch2c_bn_riv            of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.1.conv3.norm.weight          loaded from res5_1_branch2c_bn_s              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.1.conv3.weight               loaded from res5_1_branch2c_w                 of shape (2048, 2048, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.2.conv1.norm.bias            loaded from res5_2_branch2a_bn_b              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.2.conv1.norm.running_mean    loaded from res5_2_branch2a_bn_rm             of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.2.conv1.norm.running_var     loaded from res5_2_branch2a_bn_riv            of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.2.conv1.norm.weight          loaded from res5_2_branch2a_bn_s              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.2.conv1.weight               loaded from res5_2_branch2a_w                 of shape (2048, 2048, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.2.conv2.norm.bias            loaded from res5_2_branch2b_bn_b              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.2.conv2.norm.running_mean    loaded from res5_2_branch2b_bn_rm             of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.2.conv2.norm.running_var     loaded from res5_2_branch2b_bn_riv            of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.2.conv2.norm.weight          loaded from res5_2_branch2b_bn_s              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.2.conv2.weight               loaded from res5_2_branch2b_w                 of shape (2048, 64, 3, 3)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.2.conv3.norm.bias            loaded from res5_2_branch2c_bn_b              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.2.conv3.norm.running_mean    loaded from res5_2_branch2c_bn_rm             of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.2.conv3.norm.running_var     loaded from res5_2_branch2c_bn_riv            of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.2.conv3.norm.weight          loaded from res5_2_branch2c_bn_s              of shape (2048,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.res5.2.conv3.weight               loaded from res5_2_branch2c_w                 of shape (2048, 2048, 1, 1)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.stem.conv1.norm.bias              loaded from res_conv1_bn_b                    of shape (64,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.stem.conv1.norm.running_mean      loaded from res_conv1_bn_rm                   of shape (64,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.stem.conv1.norm.running_var       loaded from res_conv1_bn_riv                  of shape (64,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.stem.conv1.norm.weight            loaded from res_conv1_bn_s                    of shape (64,)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mbackbone.bottom_up.stem.conv1.weight                 loaded from conv1_w                           of shape (64, 3, 7, 7)
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mSome model parameters or buffers are not found in the checkpoint:
  [34mbackbone.fpn_lateral2.{bias, weight}[0m
  [34mbackbone.fpn_lateral3.{bias, weight}[0m
  [34mbackbone.fpn_lateral4.{bias, weight}[0m
  [34mbackbone.fpn_lateral5.{bias, weight}[0m
  [34mbackbone.fpn_output2.{bias, weight}[0m
  [34mbackbone.fpn_output3.{bias, weight}[0m
  [34mbackbone.fpn_output4.{bias, weight}[0m
  [34mbackbone.fpn_output5.{bias, weight}[0m
  [34mpixel_mean[0m
  [34mpixel_std[0m
  [34mroi_heads.box_head.fc1.{bias, weight}[0m
  [34mroi_heads.box_head.fc2.{bias, weight}[0m
  [34mroi_heads.box_predictor.bbox_pred.{bias, weight}[0m
  [34mroi_heads.box_predictor.cls_score.{bias, weight}[0m
[32m[12/18 23:28:36 d2.checkpoint.c2_model_loading]: [0mThe checkpoint state_dict contains keys that are not used by the model:
  [35mpred_b[0m
  [35mpred_w[0m
Start training
[32m[12/18 23:28:36 d2.engine.train_loop]: [0mStarting training from iteration 0
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
[32m[12/18 23:28:39 d2.data.common]: [0mSerializing 3618 elements to byte tensors and concatenating them all ...
[32m[12/18 23:28:40 d2.data.common]: [0mSerialized dataset takes 33.05 MiB
[32m[12/18 23:28:40 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/18 23:28:40 d2.evaluation.evaluator]: [0mStart inference on 3618 images
[32m[12/18 23:28:40 d2.evaluation.evaluator]: [0mInference done 11/3618. 0.0501 s / img. ETA=0:03:05
[32m[12/18 23:28:50 d2.evaluation.evaluator]: [0mInference done 202/3618. 0.0503 s / img. ETA=0:02:59
[32m[12/18 23:29:00 d2.evaluation.evaluator]: [0mInference done 391/3618. 0.0500 s / img. ETA=0:02:50
[32m[12/18 23:29:10 d2.evaluation.evaluator]: [0mInference done 580/3618. 0.0495 s / img. ETA=0:02:40
[32m[12/18 23:29:20 d2.evaluation.evaluator]: [0mInference done 764/3618. 0.0493 s / img. ETA=0:02:32
[32m[12/18 23:29:30 d2.evaluation.evaluator]: [0mInference done 944/3618. 0.0492 s / img. ETA=0:02:23
[32m[12/18 23:29:40 d2.evaluation.evaluator]: [0mInference done 1120/3618. 0.0491 s / img. ETA=0:02:15
[32m[12/18 23:29:50 d2.evaluation.evaluator]: [0mInference done 1250/3618. 0.0506 s / img. ETA=0:02:14
[32m[12/18 23:30:01 d2.evaluation.evaluator]: [0mInference done 1378/3618. 0.0518 s / img. ETA=0:02:11
[32m[12/18 23:30:11 d2.evaluation.evaluator]: [0mInference done 1505/3618. 0.0527 s / img. ETA=0:02:07
[32m[12/18 23:30:21 d2.evaluation.evaluator]: [0mInference done 1628/3618. 0.0535 s / img. ETA=0:02:03
[32m[12/18 23:30:31 d2.evaluation.evaluator]: [0mInference done 1749/3618. 0.0542 s / img. ETA=0:01:58
[32m[12/18 23:30:41 d2.evaluation.evaluator]: [0mInference done 1871/3618. 0.0547 s / img. ETA=0:01:52
[32m[12/18 23:30:51 d2.evaluation.evaluator]: [0mInference done 1979/3618. 0.0555 s / img. ETA=0:01:48
[32m[12/18 23:31:01 d2.evaluation.evaluator]: [0mInference done 2128/3618. 0.0552 s / img. ETA=0:01:38
[32m[12/18 23:31:11 d2.evaluation.evaluator]: [0mInference done 2282/3618. 0.0548 s / img. ETA=0:01:28
[32m[12/18 23:31:21 d2.evaluation.evaluator]: [0mInference done 2434/3618. 0.0544 s / img. ETA=0:01:18
[32m[12/18 23:31:31 d2.evaluation.evaluator]: [0mInference done 2583/3618. 0.0541 s / img. ETA=0:01:08
[32m[12/18 23:31:41 d2.evaluation.evaluator]: [0mInference done 2729/3618. 0.0538 s / img. ETA=0:00:58
[32m[12/18 23:31:51 d2.evaluation.evaluator]: [0mInference done 2873/3618. 0.0535 s / img. ETA=0:00:49
[32m[12/18 23:32:01 d2.evaluation.evaluator]: [0mInference done 3014/3618. 0.0533 s / img. ETA=0:00:40
[32m[12/18 23:32:11 d2.evaluation.evaluator]: [0mInference done 3152/3618. 0.0531 s / img. ETA=0:00:31
[32m[12/18 23:32:21 d2.evaluation.evaluator]: [0mInference done 3268/3618. 0.0532 s / img. ETA=0:00:23
[32m[12/18 23:32:31 d2.evaluation.evaluator]: [0mInference done 3403/3618. 0.0531 s / img. ETA=0:00:14
[32m[12/18 23:32:41 d2.evaluation.evaluator]: [0mInference done 3536/3618. 0.0529 s / img. ETA=0:00:05
[32m[12/18 23:32:48 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:08.280240 (0.068719 s / img per device, on 1 devices)
[32m[12/18 23:32:48 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:03:10 (0.052829 s / img per device, on 1 devices)
Evaluate predictions
We have 2442 single comp cutouts and 1176 multi
Plot predictions
Baseline assumption cat is 67.5% correct
train cat is 13.2% correct
-80.38% improvement
[32m[12/18 23:32:51 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/18 23:32:51 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/18 23:32:51 d2.evaluation.evaluator]: [0m0.00%
[32m[12/18 23:32:51 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/18 23:32:51 d2.evaluation.evaluator]: [0m0.00%
[32m[12/18 23:32:51 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/18 23:32:51 d2.evaluation.evaluator]: [0m87.31%
[32m[12/18 23:32:51 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/18 23:32:51 d2.evaluation.evaluator]: [0m85.63%
[32m[12/18 23:32:51 d2.evaluation.evaluator]: [0mCatalogue is 0.13239358761746822 correct.
[32m[12/18 23:32:51 d2.engine.defaults]: [0mEvaluation results for train in csv format:
[32m[12/18 23:32:51 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/18 23:32:51 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/18 23:32:51 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.8731,0.8563,0.1324
len dataset is: 1205 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_val.pkl
[32m[12/18 23:32:51 d2.data.build]: [0mDistribution of instances among all 1 categories:
[36m|   category   | #instances   |
|:------------:|:-------------|
| radio_source | 1205         |
|              |              |[0m
[32m[12/18 23:32:51 d2.data.common]: [0mSerializing 1205 elements to byte tensors and concatenating them all ...
[32m[12/18 23:32:51 d2.data.common]: [0mSerialized dataset takes 11.07 MiB
[32m[12/18 23:32:51 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/18 23:32:51 d2.evaluation.evaluator]: [0mStart inference on 1205 images
/home/mostert/miniconda3/envs/early_access/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
[32m[12/18 23:32:52 d2.evaluation.evaluator]: [0mInference done 11/1205. 0.0495 s / img. ETA=0:01:00
[32m[12/18 23:33:02 d2.evaluation.evaluator]: [0mInference done 207/1205. 0.0490 s / img. ETA=0:00:51
[32m[12/18 23:33:12 d2.evaluation.evaluator]: [0mInference done 398/1205. 0.0490 s / img. ETA=0:00:41
[32m[12/18 23:33:22 d2.evaluation.evaluator]: [0mInference done 554/1205. 0.0516 s / img. ETA=0:00:36
[32m[12/18 23:33:32 d2.evaluation.evaluator]: [0mInference done 694/1205. 0.0540 s / img. ETA=0:00:30
[32m[12/18 23:33:42 d2.evaluation.evaluator]: [0mInference done 829/1205. 0.0557 s / img. ETA=0:00:23
[32m[12/18 23:33:52 d2.evaluation.evaluator]: [0mInference done 984/1205. 0.0557 s / img. ETA=0:00:13
[32m[12/18 23:34:02 d2.evaluation.evaluator]: [0mInference done 1160/1205. 0.0546 s / img. ETA=0:00:02
[32m[12/18 23:34:05 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:13.647831 (0.061373 s / img per device, on 1 devices)
[32m[12/18 23:34:05 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:05 (0.054392 s / img per device, on 1 devices)
Evaluate predictions
We have 817 single comp cutouts and 388 multi
Plot predictions
Baseline assumption cat is 67.8% correct
val cat is 10.6% correct
-84.33% improvement
[32m[12/18 23:34:06 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/18 23:34:06 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/18 23:34:06 d2.evaluation.evaluator]: [0m0.00%
[32m[12/18 23:34:06 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/18 23:34:06 d2.evaluation.evaluator]: [0m0.00%
[32m[12/18 23:34:06 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/18 23:34:06 d2.evaluation.evaluator]: [0m89.35%
[32m[12/18 23:34:06 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/18 23:34:06 d2.evaluation.evaluator]: [0m89.43%
[32m[12/18 23:34:06 d2.evaluation.evaluator]: [0mCatalogue is 0.10622406639004149 correct.
[32m[12/18 23:34:06 d2.engine.defaults]: [0mEvaluation results for val in csv format:
[32m[12/18 23:34:06 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/18 23:34:06 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/18 23:34:06 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.8935,0.8943,0.1062
len dataset is: 1211 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_test.pkl
[32m[12/18 23:34:06 d2.data.build]: [0mDistribution of instances among all 1 categories:
[36m|   category   | #instances   |
|:------------:|:-------------|
| radio_source | 1211         |
|              |              |[0m
[32m[12/18 23:34:06 d2.data.common]: [0mSerializing 1211 elements to byte tensors and concatenating them all ...
[32m[12/18 23:34:06 d2.data.common]: [0mSerialized dataset takes 11.23 MiB
[32m[12/18 23:34:06 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/18 23:34:06 d2.evaluation.evaluator]: [0mStart inference on 1211 images
[32m[12/18 23:34:12 d2.evaluation.evaluator]: [0mInference done 112/1211. 0.0500 s / img. ETA=0:00:57
[32m[12/18 23:34:22 d2.evaluation.evaluator]: [0mInference done 301/1211. 0.0501 s / img. ETA=0:00:47
[32m[12/18 23:34:32 d2.evaluation.evaluator]: [0mInference done 486/1211. 0.0501 s / img. ETA=0:00:38
[32m[12/18 23:34:42 d2.evaluation.evaluator]: [0mInference done 667/1211. 0.0502 s / img. ETA=0:00:29
[32m[12/18 23:34:52 d2.evaluation.evaluator]: [0mInference done 844/1211. 0.0502 s / img. ETA=0:00:19
[32m[12/18 23:35:02 d2.evaluation.evaluator]: [0mInference done 1018/1211. 0.0502 s / img. ETA=0:00:10
[32m[12/18 23:35:12 d2.evaluation.evaluator]: [0mInference done 1189/1211. 0.0502 s / img. ETA=0:00:01
[32m[12/18 23:35:14 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:07.522723 (0.055989 s / img per device, on 1 devices)
[32m[12/18 23:35:14 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:00 (0.050174 s / img per device, on 1 devices)
Evaluate predictions
We have 819 single comp cutouts and 392 multi
Plot predictions
Baseline assumption cat is 67.6% correct
test cat is 12.9% correct
-80.95% improvement
[32m[12/18 23:35:15 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/18 23:35:15 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/18 23:35:15 d2.evaluation.evaluator]: [0m0.00%
[32m[12/18 23:35:15 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/18 23:35:15 d2.evaluation.evaluator]: [0m0.00%
[32m[12/18 23:35:15 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/18 23:35:15 d2.evaluation.evaluator]: [0m88.40%
[32m[12/18 23:35:15 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/18 23:35:15 d2.evaluation.evaluator]: [0m84.44%
[32m[12/18 23:35:15 d2.evaluation.evaluator]: [0mCatalogue is 0.12881915772089184 correct.
[32m[12/18 23:35:15 d2.engine.defaults]: [0mEvaluation results for test in csv format:
[32m[12/18 23:35:15 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/18 23:35:15 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/18 23:35:15 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.8840,0.8444,0.1288
[32m[12/18 23:35:19 d2.utils.events]: [0meta: 0:51:57  iter: 19  total_loss: 0.638  loss_cls: 0.638  loss_box_reg: 0.000  time: 0.2085  data_time: 0.0190  lr: 0.000006  max_mem: 1822M
[32m[12/18 23:35:23 d2.utils.events]: [0meta: 0:51:58  iter: 39  total_loss: 0.058  loss_cls: 0.058  loss_box_reg: 0.000  time: 0.2089  data_time: 0.0164  lr: 0.000012  max_mem: 1822M
[32m[12/18 23:35:27 d2.utils.events]: [0meta: 0:51:53  iter: 59  total_loss: 0.192  loss_cls: 0.192  loss_box_reg: 0.000  time: 0.2088  data_time: 0.0164  lr: 0.000018  max_mem: 1822M
[32m[12/18 23:35:32 d2.utils.events]: [0meta: 0:51:57  iter: 79  total_loss: 0.153  loss_cls: 0.153  loss_box_reg: 0.000  time: 0.2094  data_time: 0.0165  lr: 0.000024  max_mem: 1822M
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
[32m[12/18 23:35:37 d2.data.common]: [0mSerializing 3618 elements to byte tensors and concatenating them all ...
[32m[12/18 23:35:37 d2.data.common]: [0mSerialized dataset takes 33.05 MiB
[32m[12/18 23:35:37 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/18 23:35:37 d2.evaluation.evaluator]: [0mStart inference on 3618 images
[32m[12/18 23:35:38 d2.evaluation.evaluator]: [0mInference done 11/3618. 0.0507 s / img. ETA=0:03:07
[32m[12/18 23:35:48 d2.evaluation.evaluator]: [0mInference done 202/3618. 0.0503 s / img. ETA=0:02:59
[32m[12/18 23:35:58 d2.evaluation.evaluator]: [0mInference done 331/3618. 0.0592 s / img. ETA=0:03:25
[32m[12/18 23:36:08 d2.evaluation.evaluator]: [0mInference done 432/3618. 0.0669 s / img. ETA=0:03:47
[32m[12/18 23:36:18 d2.evaluation.evaluator]: [0mInference done 540/3618. 0.0704 s / img. ETA=0:03:53
[32m[12/18 23:36:28 d2.evaluation.evaluator]: [0mInference done 683/3618. 0.0688 s / img. ETA=0:03:38
[32m[12/18 23:36:38 d2.evaluation.evaluator]: [0mInference done 864/3618. 0.0646 s / img. ETA=0:03:14
[32m[12/18 23:36:48 d2.evaluation.evaluator]: [0mInference done 967/3618. 0.0666 s / img. ETA=0:03:14
[32m[12/18 23:36:58 d2.evaluation.evaluator]: [0mInference done 1099/3618. 0.0663 s / img. ETA=0:03:05
[32m[12/18 23:37:08 d2.evaluation.evaluator]: [0mInference done 1230/3618. 0.0660 s / img. ETA=0:02:56
[32m[12/18 23:37:18 d2.evaluation.evaluator]: [0mInference done 1359/3618. 0.0657 s / img. ETA=0:02:48
[32m[12/18 23:37:28 d2.evaluation.evaluator]: [0mInference done 1486/3618. 0.0655 s / img. ETA=0:02:39
[32m[12/18 23:37:38 d2.evaluation.evaluator]: [0mInference done 1611/3618. 0.0653 s / img. ETA=0:02:31
[32m[12/18 23:37:49 d2.evaluation.evaluator]: [0mInference done 1734/3618. 0.0651 s / img. ETA=0:02:22
[32m[12/18 23:37:59 d2.evaluation.evaluator]: [0mInference done 1879/3618. 0.0643 s / img. ETA=0:02:10
[32m[12/18 23:38:09 d2.evaluation.evaluator]: [0mInference done 2042/3618. 0.0630 s / img. ETA=0:01:56
[32m[12/18 23:38:19 d2.evaluation.evaluator]: [0mInference done 2130/3618. 0.0639 s / img. ETA=0:01:52
[32m[12/18 23:38:29 d2.evaluation.evaluator]: [0mInference done 2220/3618. 0.0646 s / img. ETA=0:01:47
[32m[12/18 23:38:39 d2.evaluation.evaluator]: [0mInference done 2322/3618. 0.0649 s / img. ETA=0:01:41
[32m[12/18 23:38:49 d2.evaluation.evaluator]: [0mInference done 2434/3618. 0.0649 s / img. ETA=0:01:33
[32m[12/18 23:38:59 d2.evaluation.evaluator]: [0mInference done 2547/3618. 0.0648 s / img. ETA=0:01:24
[32m[12/18 23:39:09 d2.evaluation.evaluator]: [0mInference done 2658/3618. 0.0647 s / img. ETA=0:01:16
[32m[12/18 23:39:19 d2.evaluation.evaluator]: [0mInference done 2767/3618. 0.0647 s / img. ETA=0:01:08
[32m[12/18 23:39:29 d2.evaluation.evaluator]: [0mInference done 2878/3618. 0.0646 s / img. ETA=0:00:59
[32m[12/18 23:39:39 d2.evaluation.evaluator]: [0mInference done 3020/3618. 0.0638 s / img. ETA=0:00:47
[32m[12/18 23:39:49 d2.evaluation.evaluator]: [0mInference done 3125/3618. 0.0638 s / img. ETA=0:00:39
[32m[12/18 23:39:59 d2.evaluation.evaluator]: [0mInference done 3264/3618. 0.0631 s / img. ETA=0:00:28
[32m[12/18 23:40:09 d2.evaluation.evaluator]: [0mInference done 3401/3618. 0.0626 s / img. ETA=0:00:17
[32m[12/18 23:40:19 d2.evaluation.evaluator]: [0mInference done 3536/3618. 0.0620 s / img. ETA=0:00:06
[32m[12/18 23:40:26 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:48.685103 (0.079902 s / img per device, on 1 devices)
[32m[12/18 23:40:26 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:03:43 (0.061735 s / img per device, on 1 devices)
Evaluate predictions
We have 2442 single comp cutouts and 1176 multi
Plot predictions
Baseline assumption cat is 67.5% correct
train cat is 88.0% correct
30.38% improvement
[32m[12/18 23:40:29 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/18 23:40:29 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/18 23:40:29 d2.evaluation.evaluator]: [0m0.00%
[32m[12/18 23:40:29 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/18 23:40:29 d2.evaluation.evaluator]: [0m0.00%
[32m[12/18 23:40:29 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/18 23:40:29 d2.evaluation.evaluator]: [0m7.13%
[32m[12/18 23:40:29 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/18 23:40:29 d2.evaluation.evaluator]: [0m22.11%
[32m[12/18 23:40:29 d2.evaluation.evaluator]: [0mCatalogue is 0.8800442233278054 correct.
[32m[12/18 23:40:29 d2.engine.defaults]: [0mEvaluation results for train in csv format:
[32m[12/18 23:40:29 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/18 23:40:29 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/18 23:40:29 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0713,0.2211,0.8800
len dataset is: 1205 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_val.pkl
[32m[12/18 23:40:29 d2.data.common]: [0mSerializing 1205 elements to byte tensors and concatenating them all ...
[32m[12/18 23:40:29 d2.data.common]: [0mSerialized dataset takes 11.07 MiB
[32m[12/18 23:40:29 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/18 23:40:29 d2.evaluation.evaluator]: [0mStart inference on 1205 images
[32m[12/18 23:40:30 d2.evaluation.evaluator]: [0mInference done 11/1205. 0.0508 s / img. ETA=0:01:02
[32m[12/18 23:40:40 d2.evaluation.evaluator]: [0mInference done 202/1205. 0.0502 s / img. ETA=0:00:52
[32m[12/18 23:40:50 d2.evaluation.evaluator]: [0mInference done 388/1205. 0.0503 s / img. ETA=0:00:43
[32m[12/18 23:41:00 d2.evaluation.evaluator]: [0mInference done 570/1205. 0.0503 s / img. ETA=0:00:34
[32m[12/18 23:41:10 d2.evaluation.evaluator]: [0mInference done 750/1205. 0.0502 s / img. ETA=0:00:24
[32m[12/18 23:41:20 d2.evaluation.evaluator]: [0mInference done 929/1205. 0.0500 s / img. ETA=0:00:15
[32m[12/18 23:41:30 d2.evaluation.evaluator]: [0mInference done 1104/1205. 0.0498 s / img. ETA=0:00:05
[32m[12/18 23:41:36 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:06.774622 (0.055646 s / img per device, on 1 devices)
[32m[12/18 23:41:36 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:59 (0.049721 s / img per device, on 1 devices)
Evaluate predictions
We have 817 single comp cutouts and 388 multi
Plot predictions
Baseline assumption cat is 67.8% correct
val cat is 87.6% correct
29.13% improvement
[32m[12/18 23:41:37 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/18 23:41:37 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/18 23:41:37 d2.evaluation.evaluator]: [0m0.00%
[32m[12/18 23:41:37 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/18 23:41:37 d2.evaluation.evaluator]: [0m0.00%
[32m[12/18 23:41:37 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/18 23:41:37 d2.evaluation.evaluator]: [0m6.98%
[32m[12/18 23:41:37 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/18 23:41:37 d2.evaluation.evaluator]: [0m23.97%
[32m[12/18 23:41:37 d2.evaluation.evaluator]: [0mCatalogue is 0.8755186721991701 correct.
[32m[12/18 23:41:37 d2.engine.defaults]: [0mEvaluation results for val in csv format:
[32m[12/18 23:41:37 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/18 23:41:37 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/18 23:41:37 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0698,0.2397,0.8755
len dataset is: 1211 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_test.pkl
[32m[12/18 23:41:37 d2.data.common]: [0mSerializing 1211 elements to byte tensors and concatenating them all ...
[32m[12/18 23:41:38 d2.data.common]: [0mSerialized dataset takes 11.23 MiB
[32m[12/18 23:41:38 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/18 23:41:38 d2.evaluation.evaluator]: [0mStart inference on 1211 images
[32m[12/18 23:41:40 d2.evaluation.evaluator]: [0mInference done 48/1211. 0.0503 s / img. ETA=0:01:00
[32m[12/18 23:41:50 d2.evaluation.evaluator]: [0mInference done 238/1211. 0.0502 s / img. ETA=0:00:51
[32m[12/18 23:42:00 d2.evaluation.evaluator]: [0mInference done 424/1211. 0.0501 s / img. ETA=0:00:41
[32m[12/18 23:42:10 d2.evaluation.evaluator]: [0mInference done 606/1211. 0.0501 s / img. ETA=0:00:32
[32m[12/18 23:42:20 d2.evaluation.evaluator]: [0mInference done 785/1211. 0.0501 s / img. ETA=0:00:23
[32m[12/18 23:42:30 d2.evaluation.evaluator]: [0mInference done 961/1211. 0.0500 s / img. ETA=0:00:13
[32m[12/18 23:42:40 d2.evaluation.evaluator]: [0mInference done 1137/1211. 0.0498 s / img. ETA=0:00:04
[32m[12/18 23:42:45 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:07.012315 (0.055566 s / img per device, on 1 devices)
[32m[12/18 23:42:45 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:59 (0.049739 s / img per device, on 1 devices)
Evaluate predictions
We have 819 single comp cutouts and 392 multi
Plot predictions
Baseline assumption cat is 67.6% correct
test cat is 88.9% correct
31.50% improvement
[32m[12/18 23:42:46 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/18 23:42:46 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/18 23:42:46 d2.evaluation.evaluator]: [0m0.00%
[32m[12/18 23:42:46 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/18 23:42:46 d2.evaluation.evaluator]: [0m0.00%
[32m[12/18 23:42:46 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/18 23:42:46 d2.evaluation.evaluator]: [0m6.84%
[32m[12/18 23:42:46 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/18 23:42:46 d2.evaluation.evaluator]: [0m19.90%
[32m[12/18 23:42:46 d2.evaluation.evaluator]: [0mCatalogue is 0.8893476465730801 correct.
[32m[12/18 23:42:46 d2.engine.defaults]: [0mEvaluation results for test in csv format:
[32m[12/18 23:42:46 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/18 23:42:46 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/18 23:42:46 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0684,0.1990,0.8893
[32m[12/18 23:42:46 d2.utils.events]: [0meta: 0:51:56  iter: 99  total_loss: 0.122  loss_cls: 0.122  loss_box_reg: 0.000  time: 0.2095  data_time: 0.0164  lr: 0.000030  max_mem: 1822M
[32m[12/18 23:42:50 d2.utils.events]: [0meta: 0:51:48  iter: 119  total_loss: 0.275  loss_cls: 0.275  loss_box_reg: 0.000  time: 0.2093  data_time: 0.0167  lr: 0.000036  max_mem: 1822M
[32m[12/18 23:42:54 d2.utils.events]: [0meta: 0:51:40  iter: 139  total_loss: 0.176  loss_cls: 0.176  loss_box_reg: 0.000  time: 0.2091  data_time: 0.0166  lr: 0.000042  max_mem: 1822M
[32m[12/18 23:42:58 d2.utils.events]: [0meta: 0:51:33  iter: 159  total_loss: 0.123  loss_cls: 0.123  loss_box_reg: 0.000  time: 0.2090  data_time: 0.0165  lr: 0.000048  max_mem: 1822M
[32m[12/18 23:43:03 d2.utils.events]: [0meta: 0:51:31  iter: 179  total_loss: 0.141  loss_cls: 0.141  loss_box_reg: 0.000  time: 0.2102  data_time: 0.0168  lr: 0.000054  max_mem: 1822M
[32m[12/18 23:43:07 d2.utils.events]: [0meta: 0:51:30  iter: 199  total_loss: 0.133  loss_cls: 0.133  loss_box_reg: 0.000  time: 0.2102  data_time: 0.0165  lr: 0.000060  max_mem: 1822M
[32m[12/18 23:43:11 d2.utils.events]: [0meta: 0:51:27  iter: 219  total_loss: 0.080  loss_cls: 0.080  loss_box_reg: 0.000  time: 0.2106  data_time: 0.0167  lr: 0.000066  max_mem: 1822M
[32m[12/18 23:43:16 d2.utils.events]: [0meta: 0:51:21  iter: 239  total_loss: 0.127  loss_cls: 0.127  loss_box_reg: 0.000  time: 0.2106  data_time: 0.0163  lr: 0.000072  max_mem: 1822M
[32m[12/18 23:43:20 d2.utils.events]: [0meta: 0:51:16  iter: 259  total_loss: 0.099  loss_cls: 0.099  loss_box_reg: 0.000  time: 0.2111  data_time: 0.0168  lr: 0.000078  max_mem: 1822M
[32m[12/18 23:43:24 d2.utils.events]: [0meta: 0:51:14  iter: 279  total_loss: 0.060  loss_cls: 0.060  loss_box_reg: 0.000  time: 0.2111  data_time: 0.0165  lr: 0.000084  max_mem: 1822M
[32m[12/18 23:43:28 d2.utils.events]: [0meta: 0:51:13  iter: 299  total_loss: 0.082  loss_cls: 0.082  loss_box_reg: 0.000  time: 0.2111  data_time: 0.0177  lr: 0.000090  max_mem: 1822M
[32m[12/18 23:43:33 d2.utils.events]: [0meta: 0:51:15  iter: 319  total_loss: 0.100  loss_cls: 0.100  loss_box_reg: 0.000  time: 0.2112  data_time: 0.0196  lr: 0.000096  max_mem: 1822M
[32m[12/18 23:43:37 d2.utils.events]: [0meta: 0:51:16  iter: 339  total_loss: 0.078  loss_cls: 0.078  loss_box_reg: 0.000  time: 0.2114  data_time: 0.0201  lr: 0.000102  max_mem: 1822M
[32m[12/18 23:43:41 d2.utils.events]: [0meta: 0:51:12  iter: 359  total_loss: 0.075  loss_cls: 0.075  loss_box_reg: 0.000  time: 0.2113  data_time: 0.0158  lr: 0.000108  max_mem: 1822M
[32m[12/18 23:43:46 d2.utils.events]: [0meta: 0:51:08  iter: 379  total_loss: 0.094  loss_cls: 0.094  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0159  lr: 0.000114  max_mem: 1822M
[32m[12/18 23:43:50 d2.utils.events]: [0meta: 0:51:02  iter: 399  total_loss: 0.085  loss_cls: 0.085  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0154  lr: 0.000120  max_mem: 1822M
[32m[12/18 23:43:54 d2.utils.events]: [0meta: 0:50:57  iter: 419  total_loss: 0.072  loss_cls: 0.072  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0156  lr: 0.000126  max_mem: 1822M
[32m[12/18 23:43:59 d2.utils.events]: [0meta: 0:50:52  iter: 439  total_loss: 0.084  loss_cls: 0.084  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0155  lr: 0.000132  max_mem: 1822M
[32m[12/18 23:44:03 d2.utils.events]: [0meta: 0:50:44  iter: 459  total_loss: 0.195  loss_cls: 0.195  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0154  lr: 0.000138  max_mem: 1822M
[32m[12/18 23:44:07 d2.utils.events]: [0meta: 0:50:40  iter: 479  total_loss: 0.068  loss_cls: 0.068  loss_box_reg: 0.000  time: 0.2116  data_time: 0.0176  lr: 0.000144  max_mem: 1822M
[32m[12/18 23:44:11 d2.utils.events]: [0meta: 0:50:38  iter: 499  total_loss: 0.249  loss_cls: 0.249  loss_box_reg: 0.000  time: 0.2116  data_time: 0.0193  lr: 0.000150  max_mem: 1822M
[32m[12/18 23:44:16 d2.utils.events]: [0meta: 0:50:37  iter: 519  total_loss: 0.068  loss_cls: 0.068  loss_box_reg: 0.000  time: 0.2116  data_time: 0.0195  lr: 0.000156  max_mem: 1822M
[32m[12/18 23:44:20 d2.utils.events]: [0meta: 0:50:34  iter: 539  total_loss: 0.044  loss_cls: 0.044  loss_box_reg: 0.000  time: 0.2116  data_time: 0.0194  lr: 0.000162  max_mem: 1822M
[32m[12/18 23:44:24 d2.utils.events]: [0meta: 0:50:30  iter: 559  total_loss: 0.130  loss_cls: 0.130  loss_box_reg: 0.000  time: 0.2116  data_time: 0.0188  lr: 0.000168  max_mem: 1822M
[32m[12/18 23:44:28 d2.utils.events]: [0meta: 0:50:26  iter: 579  total_loss: 0.108  loss_cls: 0.108  loss_box_reg: 0.000  time: 0.2115  data_time: 0.0182  lr: 0.000174  max_mem: 1822M
[32m[12/18 23:44:33 d2.utils.events]: [0meta: 0:50:21  iter: 599  total_loss: 0.103  loss_cls: 0.103  loss_box_reg: 0.000  time: 0.2115  data_time: 0.0182  lr: 0.000180  max_mem: 1822M
[32m[12/18 23:44:37 d2.utils.events]: [0meta: 0:50:16  iter: 619  total_loss: 0.032  loss_cls: 0.032  loss_box_reg: 0.000  time: 0.2114  data_time: 0.0154  lr: 0.000186  max_mem: 1822M
[32m[12/18 23:44:41 d2.utils.events]: [0meta: 0:50:10  iter: 639  total_loss: 0.092  loss_cls: 0.092  loss_box_reg: 0.000  time: 0.2113  data_time: 0.0154  lr: 0.000192  max_mem: 1822M
[32m[12/18 23:44:45 d2.utils.events]: [0meta: 0:50:05  iter: 659  total_loss: 0.081  loss_cls: 0.081  loss_box_reg: 0.000  time: 0.2112  data_time: 0.0151  lr: 0.000198  max_mem: 1822M
[32m[12/18 23:44:49 d2.utils.events]: [0meta: 0:49:58  iter: 679  total_loss: 0.092  loss_cls: 0.092  loss_box_reg: 0.000  time: 0.2110  data_time: 0.0152  lr: 0.000204  max_mem: 1822M
[32m[12/18 23:44:54 d2.utils.events]: [0meta: 0:49:53  iter: 699  total_loss: 0.114  loss_cls: 0.114  loss_box_reg: 0.000  time: 0.2109  data_time: 0.0154  lr: 0.000210  max_mem: 1822M
[32m[12/18 23:44:58 d2.utils.events]: [0meta: 0:49:47  iter: 719  total_loss: 0.055  loss_cls: 0.055  loss_box_reg: 0.000  time: 0.2108  data_time: 0.0153  lr: 0.000216  max_mem: 1822M
[32m[12/18 23:45:02 d2.utils.events]: [0meta: 0:49:41  iter: 739  total_loss: 0.087  loss_cls: 0.087  loss_box_reg: 0.000  time: 0.2108  data_time: 0.0159  lr: 0.000222  max_mem: 1822M
[32m[12/18 23:45:06 d2.utils.events]: [0meta: 0:49:37  iter: 759  total_loss: 0.158  loss_cls: 0.158  loss_box_reg: 0.000  time: 0.2107  data_time: 0.0157  lr: 0.000228  max_mem: 1822M
[32m[12/18 23:45:10 d2.utils.events]: [0meta: 0:49:32  iter: 779  total_loss: 0.082  loss_cls: 0.082  loss_box_reg: 0.000  time: 0.2107  data_time: 0.0160  lr: 0.000234  max_mem: 1822M
[32m[12/18 23:45:15 d2.utils.events]: [0meta: 0:49:28  iter: 799  total_loss: 0.056  loss_cls: 0.056  loss_box_reg: 0.000  time: 0.2106  data_time: 0.0157  lr: 0.000240  max_mem: 1822M
[32m[12/18 23:45:19 d2.utils.events]: [0meta: 0:49:23  iter: 819  total_loss: 0.154  loss_cls: 0.154  loss_box_reg: 0.000  time: 0.2106  data_time: 0.0170  lr: 0.000246  max_mem: 1822M
[32m[12/18 23:45:23 d2.utils.events]: [0meta: 0:49:20  iter: 839  total_loss: 0.064  loss_cls: 0.064  loss_box_reg: 0.000  time: 0.2107  data_time: 0.0186  lr: 0.000252  max_mem: 1822M
[32m[12/18 23:45:27 d2.utils.events]: [0meta: 0:49:17  iter: 859  total_loss: 0.055  loss_cls: 0.055  loss_box_reg: 0.000  time: 0.2107  data_time: 0.0163  lr: 0.000258  max_mem: 1822M
[32m[12/18 23:45:32 d2.utils.events]: [0meta: 0:49:15  iter: 879  total_loss: 0.126  loss_cls: 0.126  loss_box_reg: 0.000  time: 0.2108  data_time: 0.0165  lr: 0.000264  max_mem: 1822M
[32m[12/18 23:45:36 d2.utils.events]: [0meta: 0:49:11  iter: 899  total_loss: 0.065  loss_cls: 0.065  loss_box_reg: 0.000  time: 0.2109  data_time: 0.0164  lr: 0.000270  max_mem: 1822M
[32m[12/18 23:45:41 d2.utils.events]: [0meta: 0:49:08  iter: 919  total_loss: 0.048  loss_cls: 0.048  loss_box_reg: 0.000  time: 0.2111  data_time: 0.0166  lr: 0.000276  max_mem: 1822M
[32m[12/18 23:45:45 d2.utils.events]: [0meta: 0:49:04  iter: 939  total_loss: 0.072  loss_cls: 0.072  loss_box_reg: 0.000  time: 0.2113  data_time: 0.0164  lr: 0.000282  max_mem: 1822M
[32m[12/18 23:45:49 d2.utils.events]: [0meta: 0:48:59  iter: 959  total_loss: 0.050  loss_cls: 0.050  loss_box_reg: 0.000  time: 0.2113  data_time: 0.0171  lr: 0.000288  max_mem: 1822M
[32m[12/18 23:45:54 d2.utils.events]: [0meta: 0:48:56  iter: 979  total_loss: 0.067  loss_cls: 0.067  loss_box_reg: 0.000  time: 0.2115  data_time: 0.0169  lr: 0.000294  max_mem: 1822M
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
[32m[12/18 23:46:00 d2.data.common]: [0mSerializing 3618 elements to byte tensors and concatenating them all ...
[32m[12/18 23:46:00 d2.data.common]: [0mSerialized dataset takes 33.05 MiB
[32m[12/18 23:46:00 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/18 23:46:00 d2.evaluation.evaluator]: [0mStart inference on 3618 images
[32m[12/18 23:46:01 d2.evaluation.evaluator]: [0mInference done 11/3618. 0.0502 s / img. ETA=0:03:05
[32m[12/18 23:46:11 d2.evaluation.evaluator]: [0mInference done 202/3618. 0.0502 s / img. ETA=0:02:58
[32m[12/18 23:46:21 d2.evaluation.evaluator]: [0mInference done 382/3618. 0.0510 s / img. ETA=0:02:54
[32m[12/18 23:46:31 d2.evaluation.evaluator]: [0mInference done 570/3618. 0.0502 s / img. ETA=0:02:43
[32m[12/18 23:46:41 d2.evaluation.evaluator]: [0mInference done 754/3618. 0.0499 s / img. ETA=0:02:34
[32m[12/18 23:46:51 d2.evaluation.evaluator]: [0mInference done 934/3618. 0.0496 s / img. ETA=0:02:25
[32m[12/18 23:47:01 d2.evaluation.evaluator]: [0mInference done 1105/3618. 0.0498 s / img. ETA=0:02:17
[32m[12/18 23:47:11 d2.evaluation.evaluator]: [0mInference done 1250/3618. 0.0507 s / img. ETA=0:02:13
[32m[12/18 23:47:21 d2.evaluation.evaluator]: [0mInference done 1416/3618. 0.0507 s / img. ETA=0:02:05
[32m[12/18 23:47:31 d2.evaluation.evaluator]: [0mInference done 1580/3618. 0.0506 s / img. ETA=0:01:57
[32m[12/18 23:47:41 d2.evaluation.evaluator]: [0mInference done 1745/3618. 0.0504 s / img. ETA=0:01:48
[32m[12/18 23:47:51 d2.evaluation.evaluator]: [0mInference done 1907/3618. 0.0503 s / img. ETA=0:01:39
[32m[12/18 23:48:01 d2.evaluation.evaluator]: [0mInference done 2064/3618. 0.0502 s / img. ETA=0:01:30
[32m[12/18 23:48:11 d2.evaluation.evaluator]: [0mInference done 2220/3618. 0.0501 s / img. ETA=0:01:22
[32m[12/18 23:48:21 d2.evaluation.evaluator]: [0mInference done 2373/3618. 0.0500 s / img. ETA=0:01:13
[32m[12/18 23:48:31 d2.evaluation.evaluator]: [0mInference done 2524/3618. 0.0499 s / img. ETA=0:01:05
[32m[12/18 23:48:41 d2.evaluation.evaluator]: [0mInference done 2672/3618. 0.0499 s / img. ETA=0:00:57
[32m[12/18 23:48:51 d2.evaluation.evaluator]: [0mInference done 2817/3618. 0.0498 s / img. ETA=0:00:48
[32m[12/18 23:49:01 d2.evaluation.evaluator]: [0mInference done 2958/3618. 0.0498 s / img. ETA=0:00:40
[32m[12/18 23:49:11 d2.evaluation.evaluator]: [0mInference done 3098/3618. 0.0498 s / img. ETA=0:00:32
[32m[12/18 23:49:21 d2.evaluation.evaluator]: [0mInference done 3236/3618. 0.0497 s / img. ETA=0:00:23
[32m[12/18 23:49:31 d2.evaluation.evaluator]: [0mInference done 3373/3618. 0.0497 s / img. ETA=0:00:15
[32m[12/18 23:49:41 d2.evaluation.evaluator]: [0mInference done 3508/3618. 0.0497 s / img. ETA=0:00:06
[32m[12/18 23:49:51 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:50.455708 (0.063785 s / img per device, on 1 devices)
[32m[12/18 23:49:51 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:59 (0.049646 s / img per device, on 1 devices)
Evaluate predictions
We have 2442 single comp cutouts and 1176 multi
Plot predictions
Baseline assumption cat is 67.5% correct
train cat is 89.3% correct
32.31% improvement
[32m[12/18 23:49:53 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/18 23:49:53 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/18 23:49:53 d2.evaluation.evaluator]: [0m0.00%
[32m[12/18 23:49:53 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/18 23:49:53 d2.evaluation.evaluator]: [0m0.00%
[32m[12/18 23:49:53 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/18 23:49:53 d2.evaluation.evaluator]: [0m7.25%
[32m[12/18 23:49:53 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/18 23:49:53 d2.evaluation.evaluator]: [0m17.86%
[32m[12/18 23:49:53 d2.evaluation.evaluator]: [0mCatalogue is 0.8930348258706468 correct.
[32m[12/18 23:49:53 d2.engine.defaults]: [0mEvaluation results for train in csv format:
[32m[12/18 23:49:53 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/18 23:49:53 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/18 23:49:53 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0725,0.1786,0.8930
len dataset is: 1205 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_val.pkl
[32m[12/18 23:49:54 d2.data.common]: [0mSerializing 1205 elements to byte tensors and concatenating them all ...
[32m[12/18 23:49:54 d2.data.common]: [0mSerialized dataset takes 11.07 MiB
[32m[12/18 23:49:54 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/18 23:49:54 d2.evaluation.evaluator]: [0mStart inference on 1205 images
[32m[12/18 23:49:54 d2.evaluation.evaluator]: [0mInference done 11/1205. 0.0504 s / img. ETA=0:01:01
[32m[12/18 23:50:04 d2.evaluation.evaluator]: [0mInference done 202/1205. 0.0503 s / img. ETA=0:00:52
[32m[12/18 23:50:14 d2.evaluation.evaluator]: [0mInference done 388/1205. 0.0504 s / img. ETA=0:00:43
[32m[12/18 23:50:24 d2.evaluation.evaluator]: [0mInference done 570/1205. 0.0504 s / img. ETA=0:00:34
[32m[12/18 23:50:35 d2.evaluation.evaluator]: [0mInference done 685/1205. 0.0549 s / img. ETA=0:00:30
[32m[12/18 23:50:45 d2.evaluation.evaluator]: [0mInference done 777/1205. 0.0597 s / img. ETA=0:00:28
[32m[12/18 23:50:55 d2.evaluation.evaluator]: [0mInference done 861/1205. 0.0639 s / img. ETA=0:00:24
[32m[12/18 23:51:05 d2.evaluation.evaluator]: [0mInference done 993/1205. 0.0640 s / img. ETA=0:00:15
[32m[12/18 23:51:15 d2.evaluation.evaluator]: [0mInference done 1167/1205. 0.0618 s / img. ETA=0:00:02
[32m[12/18 23:51:17 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:23.295176 (0.069413 s / img per device, on 1 devices)
[32m[12/18 23:51:17 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:13 (0.061345 s / img per device, on 1 devices)
Evaluate predictions
We have 817 single comp cutouts and 388 multi
Plot predictions
Baseline assumption cat is 67.8% correct
val cat is 89.3% correct
31.70% improvement
[32m[12/18 23:51:18 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/18 23:51:18 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/18 23:51:18 d2.evaluation.evaluator]: [0m0.00%
[32m[12/18 23:51:18 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/18 23:51:18 d2.evaluation.evaluator]: [0m0.00%
[32m[12/18 23:51:18 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/18 23:51:18 d2.evaluation.evaluator]: [0m6.85%
[32m[12/18 23:51:18 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/18 23:51:18 d2.evaluation.evaluator]: [0m18.81%
[32m[12/18 23:51:18 d2.evaluation.evaluator]: [0mCatalogue is 0.8929460580912864 correct.
[32m[12/18 23:51:18 d2.engine.defaults]: [0mEvaluation results for val in csv format:
[32m[12/18 23:51:18 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/18 23:51:18 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/18 23:51:18 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0685,0.1881,0.8929
len dataset is: 1211 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_test.pkl
[32m[12/18 23:51:18 d2.data.common]: [0mSerializing 1211 elements to byte tensors and concatenating them all ...
[32m[12/18 23:51:18 d2.data.common]: [0mSerialized dataset takes 11.23 MiB
[32m[12/18 23:51:18 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/18 23:51:18 d2.evaluation.evaluator]: [0mStart inference on 1211 images
[32m[12/18 23:51:25 d2.evaluation.evaluator]: [0mInference done 111/1211. 0.0550 s / img. ETA=0:01:02
[32m[12/18 23:51:35 d2.evaluation.evaluator]: [0mInference done 248/1211. 0.0628 s / img. ETA=0:01:03
[32m[12/18 23:51:45 d2.evaluation.evaluator]: [0mInference done 363/1211. 0.0685 s / img. ETA=0:01:01
[32m[12/18 23:51:55 d2.evaluation.evaluator]: [0mInference done 493/1211. 0.0690 s / img. ETA=0:00:53
[32m[12/18 23:52:05 d2.evaluation.evaluator]: [0mInference done 664/1211. 0.0646 s / img. ETA=0:00:38
[32m[12/18 23:52:15 d2.evaluation.evaluator]: [0mInference done 809/1211. 0.0639 s / img. ETA=0:00:28
[32m[12/18 23:52:25 d2.evaluation.evaluator]: [0mInference done 961/1211. 0.0628 s / img. ETA=0:00:17
[32m[12/18 23:52:35 d2.evaluation.evaluator]: [0mInference done 1137/1211. 0.0606 s / img. ETA=0:00:04
[32m[12/18 23:52:40 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:20.986107 (0.067153 s / img per device, on 1 devices)
[32m[12/18 23:52:40 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:12 (0.059887 s / img per device, on 1 devices)
Evaluate predictions
We have 819 single comp cutouts and 392 multi
Plot predictions
Baseline assumption cat is 67.6% correct
test cat is 89.9% correct
32.97% improvement
[32m[12/18 23:52:41 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/18 23:52:41 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/18 23:52:41 d2.evaluation.evaluator]: [0m0.00%
[32m[12/18 23:52:41 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/18 23:52:41 d2.evaluation.evaluator]: [0m0.00%
[32m[12/18 23:52:41 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/18 23:52:41 d2.evaluation.evaluator]: [0m6.72%
[32m[12/18 23:52:41 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/18 23:52:41 d2.evaluation.evaluator]: [0m17.09%
[32m[12/18 23:52:41 d2.evaluation.evaluator]: [0mCatalogue is 0.8992568125516103 correct.
[32m[12/18 23:52:41 d2.engine.defaults]: [0mEvaluation results for test in csv format:
[32m[12/18 23:52:41 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/18 23:52:41 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/18 23:52:41 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0672,0.1709,0.8993
[32m[12/18 23:52:41 d2.utils.events]: [0meta: 0:48:53  iter: 999  total_loss: 0.068  loss_cls: 0.068  loss_box_reg: 0.000  time: 0.2115  data_time: 0.0179  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:52:45 d2.utils.events]: [0meta: 0:48:49  iter: 1019  total_loss: 0.070  loss_cls: 0.070  loss_box_reg: 0.000  time: 0.2115  data_time: 0.0163  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:52:49 d2.utils.events]: [0meta: 0:48:44  iter: 1039  total_loss: 0.060  loss_cls: 0.060  loss_box_reg: 0.000  time: 0.2114  data_time: 0.0161  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:52:53 d2.utils.events]: [0meta: 0:48:39  iter: 1059  total_loss: 0.111  loss_cls: 0.111  loss_box_reg: 0.000  time: 0.2114  data_time: 0.0168  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:52:57 d2.utils.events]: [0meta: 0:48:33  iter: 1079  total_loss: 0.043  loss_cls: 0.043  loss_box_reg: 0.000  time: 0.2113  data_time: 0.0160  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:53:02 d2.utils.events]: [0meta: 0:48:30  iter: 1099  total_loss: 0.060  loss_cls: 0.060  loss_box_reg: 0.000  time: 0.2115  data_time: 0.0173  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:53:06 d2.utils.events]: [0meta: 0:48:27  iter: 1119  total_loss: 0.102  loss_cls: 0.102  loss_box_reg: 0.000  time: 0.2115  data_time: 0.0197  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:53:10 d2.utils.events]: [0meta: 0:48:25  iter: 1139  total_loss: 0.066  loss_cls: 0.066  loss_box_reg: 0.000  time: 0.2115  data_time: 0.0184  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:53:15 d2.utils.events]: [0meta: 0:48:21  iter: 1159  total_loss: 0.092  loss_cls: 0.092  loss_box_reg: 0.000  time: 0.2115  data_time: 0.0160  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:53:19 d2.utils.events]: [0meta: 0:48:16  iter: 1179  total_loss: 0.060  loss_cls: 0.060  loss_box_reg: 0.000  time: 0.2114  data_time: 0.0157  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:53:23 d2.utils.events]: [0meta: 0:48:10  iter: 1199  total_loss: 0.059  loss_cls: 0.059  loss_box_reg: 0.000  time: 0.2114  data_time: 0.0157  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:53:28 d2.utils.events]: [0meta: 0:48:05  iter: 1219  total_loss: 0.054  loss_cls: 0.054  loss_box_reg: 0.000  time: 0.2115  data_time: 0.0166  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:53:32 d2.utils.events]: [0meta: 0:48:01  iter: 1239  total_loss: 0.044  loss_cls: 0.044  loss_box_reg: 0.000  time: 0.2114  data_time: 0.0155  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:53:36 d2.utils.events]: [0meta: 0:47:58  iter: 1259  total_loss: 0.093  loss_cls: 0.093  loss_box_reg: 0.000  time: 0.2116  data_time: 0.0167  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:53:40 d2.utils.events]: [0meta: 0:47:53  iter: 1279  total_loss: 0.059  loss_cls: 0.059  loss_box_reg: 0.000  time: 0.2115  data_time: 0.0174  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:53:45 d2.utils.events]: [0meta: 0:47:48  iter: 1299  total_loss: 0.063  loss_cls: 0.063  loss_box_reg: 0.000  time: 0.2115  data_time: 0.0176  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:53:49 d2.utils.events]: [0meta: 0:47:43  iter: 1319  total_loss: 0.036  loss_cls: 0.036  loss_box_reg: 0.000  time: 0.2115  data_time: 0.0164  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:53:53 d2.utils.events]: [0meta: 0:47:39  iter: 1339  total_loss: 0.072  loss_cls: 0.072  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0184  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:53:58 d2.utils.events]: [0meta: 0:47:35  iter: 1359  total_loss: 0.122  loss_cls: 0.122  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0220  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:54:02 d2.utils.events]: [0meta: 0:47:31  iter: 1379  total_loss: 0.056  loss_cls: 0.056  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0232  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:54:06 d2.utils.events]: [0meta: 0:47:28  iter: 1399  total_loss: 0.050  loss_cls: 0.050  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0218  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:54:11 d2.utils.events]: [0meta: 0:47:26  iter: 1419  total_loss: 0.128  loss_cls: 0.128  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0183  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:54:15 d2.utils.events]: [0meta: 0:47:21  iter: 1439  total_loss: 0.071  loss_cls: 0.071  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0179  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:54:19 d2.utils.events]: [0meta: 0:47:18  iter: 1459  total_loss: 0.065  loss_cls: 0.065  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0178  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:54:23 d2.utils.events]: [0meta: 0:47:14  iter: 1479  total_loss: 0.055  loss_cls: 0.055  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0187  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:54:28 d2.utils.events]: [0meta: 0:47:09  iter: 1499  total_loss: 0.036  loss_cls: 0.036  loss_box_reg: 0.000  time: 0.2116  data_time: 0.0182  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:54:32 d2.utils.events]: [0meta: 0:47:04  iter: 1519  total_loss: 0.052  loss_cls: 0.052  loss_box_reg: 0.000  time: 0.2116  data_time: 0.0186  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:54:36 d2.utils.events]: [0meta: 0:46:58  iter: 1539  total_loss: 0.072  loss_cls: 0.072  loss_box_reg: 0.000  time: 0.2116  data_time: 0.0198  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:54:40 d2.utils.events]: [0meta: 0:46:52  iter: 1559  total_loss: 0.074  loss_cls: 0.074  loss_box_reg: 0.000  time: 0.2116  data_time: 0.0161  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:54:45 d2.utils.events]: [0meta: 0:46:48  iter: 1579  total_loss: 0.055  loss_cls: 0.055  loss_box_reg: 0.000  time: 0.2116  data_time: 0.0165  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:54:49 d2.utils.events]: [0meta: 0:46:44  iter: 1599  total_loss: 0.080  loss_cls: 0.080  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0180  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:54:53 d2.utils.events]: [0meta: 0:46:41  iter: 1619  total_loss: 0.095  loss_cls: 0.095  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0175  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:54:58 d2.utils.events]: [0meta: 0:46:37  iter: 1639  total_loss: 0.088  loss_cls: 0.088  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0171  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:55:02 d2.utils.events]: [0meta: 0:46:32  iter: 1659  total_loss: 0.091  loss_cls: 0.091  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0170  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:55:06 d2.utils.events]: [0meta: 0:46:31  iter: 1679  total_loss: 0.088  loss_cls: 0.088  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0181  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:55:11 d2.utils.events]: [0meta: 0:46:28  iter: 1699  total_loss: 0.074  loss_cls: 0.074  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0210  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:55:15 d2.utils.events]: [0meta: 0:46:24  iter: 1719  total_loss: 0.051  loss_cls: 0.051  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0171  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:55:20 d2.utils.events]: [0meta: 0:46:21  iter: 1739  total_loss: 0.107  loss_cls: 0.107  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0171  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:55:24 d2.utils.events]: [0meta: 0:46:17  iter: 1759  total_loss: 0.237  loss_cls: 0.237  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0190  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:55:28 d2.utils.events]: [0meta: 0:46:14  iter: 1779  total_loss: 0.038  loss_cls: 0.038  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0190  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:55:33 d2.utils.events]: [0meta: 0:46:13  iter: 1799  total_loss: 0.039  loss_cls: 0.039  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0199  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:55:37 d2.utils.events]: [0meta: 0:46:11  iter: 1819  total_loss: 0.036  loss_cls: 0.036  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0197  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:55:41 d2.utils.events]: [0meta: 0:46:07  iter: 1839  total_loss: 0.086  loss_cls: 0.086  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0181  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:55:46 d2.utils.events]: [0meta: 0:46:02  iter: 1859  total_loss: 0.084  loss_cls: 0.084  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0179  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:55:50 d2.utils.events]: [0meta: 0:45:56  iter: 1879  total_loss: 0.077  loss_cls: 0.077  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0179  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:55:54 d2.utils.events]: [0meta: 0:45:50  iter: 1899  total_loss: 0.081  loss_cls: 0.081  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0181  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:55:58 d2.utils.events]: [0meta: 0:45:45  iter: 1919  total_loss: 0.070  loss_cls: 0.070  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0180  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:56:03 d2.utils.events]: [0meta: 0:45:40  iter: 1939  total_loss: 0.103  loss_cls: 0.103  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0181  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:56:07 d2.utils.events]: [0meta: 0:45:36  iter: 1959  total_loss: 0.118  loss_cls: 0.118  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0175  lr: 0.000300  max_mem: 1822M
[32m[12/18 23:56:11 d2.utils.events]: [0meta: 0:45:31  iter: 1979  total_loss: 0.040  loss_cls: 0.040  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0179  lr: 0.000300  max_mem: 1822M
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
[32m[12/18 23:56:17 d2.data.common]: [0mSerializing 3618 elements to byte tensors and concatenating them all ...
[32m[12/18 23:56:17 d2.data.common]: [0mSerialized dataset takes 33.05 MiB
[32m[12/18 23:56:17 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/18 23:56:17 d2.evaluation.evaluator]: [0mStart inference on 3618 images
[32m[12/18 23:56:18 d2.evaluation.evaluator]: [0mInference done 11/3618. 0.0503 s / img. ETA=0:03:06
[32m[12/18 23:56:28 d2.evaluation.evaluator]: [0mInference done 203/3618. 0.0501 s / img. ETA=0:02:58
[32m[12/18 23:56:38 d2.evaluation.evaluator]: [0mInference done 389/3618. 0.0503 s / img. ETA=0:02:51
[32m[12/18 23:56:48 d2.evaluation.evaluator]: [0mInference done 578/3618. 0.0498 s / img. ETA=0:02:41
[32m[12/18 23:56:58 d2.evaluation.evaluator]: [0mInference done 763/3618. 0.0495 s / img. ETA=0:02:32
[32m[12/18 23:57:08 d2.evaluation.evaluator]: [0mInference done 940/3618. 0.0495 s / img. ETA=0:02:24
[32m[12/18 23:57:18 d2.evaluation.evaluator]: [0mInference done 1116/3618. 0.0494 s / img. ETA=0:02:16
[32m[12/18 23:57:28 d2.evaluation.evaluator]: [0mInference done 1275/3618. 0.0499 s / img. ETA=0:02:10
[32m[12/18 23:57:38 d2.evaluation.evaluator]: [0mInference done 1410/3618. 0.0509 s / img. ETA=0:02:06
[32m[12/18 23:57:48 d2.evaluation.evaluator]: [0mInference done 1566/3618. 0.0510 s / img. ETA=0:01:59
[32m[12/18 23:57:58 d2.evaluation.evaluator]: [0mInference done 1717/3618. 0.0512 s / img. ETA=0:01:51
[32m[12/18 23:58:08 d2.evaluation.evaluator]: [0mInference done 1864/3618. 0.0514 s / img. ETA=0:01:44
[32m[12/18 23:58:18 d2.evaluation.evaluator]: [0mInference done 2023/3618. 0.0512 s / img. ETA=0:01:35
[32m[12/18 23:58:28 d2.evaluation.evaluator]: [0mInference done 2180/3618. 0.0511 s / img. ETA=0:01:26
[32m[12/18 23:58:38 d2.evaluation.evaluator]: [0mInference done 2335/3618. 0.0509 s / img. ETA=0:01:17
[32m[12/18 23:58:48 d2.evaluation.evaluator]: [0mInference done 2480/3618. 0.0509 s / img. ETA=0:01:09
[32m[12/18 23:58:58 d2.evaluation.evaluator]: [0mInference done 2630/3618. 0.0508 s / img. ETA=0:01:00
[32m[12/18 23:59:08 d2.evaluation.evaluator]: [0mInference done 2757/3618. 0.0510 s / img. ETA=0:00:53
[32m[12/18 23:59:18 d2.evaluation.evaluator]: [0mInference done 2851/3618. 0.0517 s / img. ETA=0:00:48
[32m[12/18 23:59:28 d2.evaluation.evaluator]: [0mInference done 2944/3618. 0.0523 s / img. ETA=0:00:43
[32m[12/18 23:59:38 d2.evaluation.evaluator]: [0mInference done 3038/3618. 0.0529 s / img. ETA=0:00:38
[32m[12/18 23:59:48 d2.evaluation.evaluator]: [0mInference done 3132/3618. 0.0534 s / img. ETA=0:00:32
[32m[12/18 23:59:59 d2.evaluation.evaluator]: [0mInference done 3225/3618. 0.0539 s / img. ETA=0:00:26
[32m[12/19 00:00:09 d2.evaluation.evaluator]: [0mInference done 3298/3618. 0.0546 s / img. ETA=0:00:22
[32m[12/19 00:00:19 d2.evaluation.evaluator]: [0mInference done 3372/3618. 0.0553 s / img. ETA=0:00:17
[32m[12/19 00:00:29 d2.evaluation.evaluator]: [0mInference done 3445/3618. 0.0559 s / img. ETA=0:00:12
[32m[12/19 00:00:39 d2.evaluation.evaluator]: [0mInference done 3515/3618. 0.0565 s / img. ETA=0:00:07
[32m[12/19 00:00:49 d2.evaluation.evaluator]: [0mInference done 3582/3618. 0.0572 s / img. ETA=0:00:02
[32m[12/19 00:00:53 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:35.358643 (0.076213 s / img per device, on 1 devices)
[32m[12/19 00:00:53 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:03:26 (0.057127 s / img per device, on 1 devices)
Evaluate predictions
We have 2442 single comp cutouts and 1176 multi
Plot predictions
Baseline assumption cat is 67.5% correct
train cat is 89.4% correct
32.47% improvement
[32m[12/19 00:00:57 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 00:00:57 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 00:00:57 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:00:57 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 00:00:57 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:00:57 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 00:00:57 d2.evaluation.evaluator]: [0m7.13%
[32m[12/19 00:00:57 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 00:00:57 d2.evaluation.evaluator]: [0m17.77%
[32m[12/19 00:00:57 d2.evaluation.evaluator]: [0mCatalogue is 0.8941404090657822 correct.
[32m[12/19 00:00:57 d2.engine.defaults]: [0mEvaluation results for train in csv format:
[32m[12/19 00:00:57 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 00:00:57 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 00:00:57 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0713,0.1777,0.8941
len dataset is: 1205 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_val.pkl
[32m[12/19 00:00:57 d2.data.common]: [0mSerializing 1205 elements to byte tensors and concatenating them all ...
[32m[12/19 00:00:57 d2.data.common]: [0mSerialized dataset takes 11.07 MiB
[32m[12/19 00:00:57 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 00:00:57 d2.evaluation.evaluator]: [0mStart inference on 1205 images
[32m[12/19 00:00:59 d2.evaluation.evaluator]: [0mInference done 26/1205. 0.0500 s / img. ETA=0:01:00
[32m[12/19 00:01:09 d2.evaluation.evaluator]: [0mInference done 217/1205. 0.0500 s / img. ETA=0:00:51
[32m[12/19 00:01:19 d2.evaluation.evaluator]: [0mInference done 404/1205. 0.0501 s / img. ETA=0:00:42
[32m[12/19 00:01:29 d2.evaluation.evaluator]: [0mInference done 586/1205. 0.0501 s / img. ETA=0:00:33
[32m[12/19 00:01:39 d2.evaluation.evaluator]: [0mInference done 764/1205. 0.0502 s / img. ETA=0:00:23
[32m[12/19 00:01:49 d2.evaluation.evaluator]: [0mInference done 939/1205. 0.0502 s / img. ETA=0:00:14
[32m[12/19 00:01:59 d2.evaluation.evaluator]: [0mInference done 1111/1205. 0.0502 s / img. ETA=0:00:05
[32m[12/19 00:02:05 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:07.265355 (0.056054 s / img per device, on 1 devices)
[32m[12/19 00:02:05 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:00 (0.050177 s / img per device, on 1 devices)
Evaluate predictions
We have 817 single comp cutouts and 388 multi
Plot predictions
Baseline assumption cat is 67.8% correct
val cat is 88.9% correct
31.09% improvement
[32m[12/19 00:02:06 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 00:02:06 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 00:02:06 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:02:06 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 00:02:06 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:02:06 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 00:02:06 d2.evaluation.evaluator]: [0m7.59%
[32m[12/19 00:02:06 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 00:02:06 d2.evaluation.evaluator]: [0m18.56%
[32m[12/19 00:02:06 d2.evaluation.evaluator]: [0mCatalogue is 0.8887966804979253 correct.
[32m[12/19 00:02:06 d2.engine.defaults]: [0mEvaluation results for val in csv format:
[32m[12/19 00:02:06 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 00:02:06 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 00:02:06 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0759,0.1856,0.8888
len dataset is: 1211 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_test.pkl
[32m[12/19 00:02:06 d2.data.common]: [0mSerializing 1211 elements to byte tensors and concatenating them all ...
[32m[12/19 00:02:07 d2.data.common]: [0mSerialized dataset takes 11.23 MiB
[32m[12/19 00:02:07 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 00:02:07 d2.evaluation.evaluator]: [0mStart inference on 1211 images
[32m[12/19 00:02:09 d2.evaluation.evaluator]: [0mInference done 46/1211. 0.0501 s / img. ETA=0:01:00
[32m[12/19 00:02:19 d2.evaluation.evaluator]: [0mInference done 237/1211. 0.0501 s / img. ETA=0:00:51
[32m[12/19 00:02:29 d2.evaluation.evaluator]: [0mInference done 423/1211. 0.0501 s / img. ETA=0:00:41
[32m[12/19 00:02:39 d2.evaluation.evaluator]: [0mInference done 606/1211. 0.0501 s / img. ETA=0:00:32
[32m[12/19 00:02:49 d2.evaluation.evaluator]: [0mInference done 785/1211. 0.0501 s / img. ETA=0:00:23
[32m[12/19 00:02:59 d2.evaluation.evaluator]: [0mInference done 960/1211. 0.0501 s / img. ETA=0:00:13
[32m[12/19 00:03:09 d2.evaluation.evaluator]: [0mInference done 1079/1211. 0.0524 s / img. ETA=0:00:07
[32m[12/19 00:03:19 d2.evaluation.evaluator]: [0mInference done 1138/1211. 0.0570 s / img. ETA=0:00:04
[32m[12/19 00:03:26 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:18.627348 (0.065197 s / img per device, on 1 devices)
[32m[12/19 00:03:26 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:09 (0.057415 s / img per device, on 1 devices)
Evaluate predictions
We have 819 single comp cutouts and 392 multi
Plot predictions
Baseline assumption cat is 67.6% correct
test cat is 90.2% correct
33.33% improvement
[32m[12/19 00:03:27 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 00:03:27 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 00:03:27 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:03:27 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 00:03:27 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:03:27 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 00:03:27 d2.evaluation.evaluator]: [0m6.23%
[32m[12/19 00:03:27 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 00:03:27 d2.evaluation.evaluator]: [0m17.35%
[32m[12/19 00:03:27 d2.evaluation.evaluator]: [0mCatalogue is 0.9017341040462428 correct.
[32m[12/19 00:03:27 d2.engine.defaults]: [0mEvaluation results for test in csv format:
[32m[12/19 00:03:27 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 00:03:27 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 00:03:27 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0623,0.1735,0.9017
[32m[12/19 00:03:27 d2.utils.events]: [0meta: 0:45:26  iter: 1999  total_loss: 0.070  loss_cls: 0.070  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0187  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:03:32 d2.utils.events]: [0meta: 0:45:23  iter: 2019  total_loss: 0.081  loss_cls: 0.081  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0186  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:03:36 d2.utils.events]: [0meta: 0:45:19  iter: 2039  total_loss: 0.065  loss_cls: 0.065  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0177  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:03:40 d2.utils.events]: [0meta: 0:45:16  iter: 2059  total_loss: 0.045  loss_cls: 0.045  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0180  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:03:44 d2.utils.events]: [0meta: 0:45:13  iter: 2079  total_loss: 0.064  loss_cls: 0.064  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0179  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:03:49 d2.utils.events]: [0meta: 0:45:07  iter: 2099  total_loss: 0.049  loss_cls: 0.049  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0179  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:03:53 d2.utils.events]: [0meta: 0:45:02  iter: 2119  total_loss: 0.066  loss_cls: 0.066  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0178  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:03:57 d2.utils.events]: [0meta: 0:44:56  iter: 2139  total_loss: 0.050  loss_cls: 0.050  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0178  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:04:01 d2.utils.events]: [0meta: 0:44:52  iter: 2159  total_loss: 0.040  loss_cls: 0.040  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0182  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:04:06 d2.utils.events]: [0meta: 0:44:48  iter: 2179  total_loss: 0.049  loss_cls: 0.049  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0186  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:04:10 d2.utils.events]: [0meta: 0:44:44  iter: 2199  total_loss: 0.050  loss_cls: 0.050  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0184  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:04:14 d2.utils.events]: [0meta: 0:44:40  iter: 2219  total_loss: 0.062  loss_cls: 0.062  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0181  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:04:18 d2.utils.events]: [0meta: 0:44:36  iter: 2239  total_loss: 0.036  loss_cls: 0.036  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0178  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:04:23 d2.utils.events]: [0meta: 0:44:31  iter: 2259  total_loss: 0.086  loss_cls: 0.086  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0177  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:04:27 d2.utils.events]: [0meta: 0:44:27  iter: 2279  total_loss: 0.095  loss_cls: 0.095  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0186  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:04:31 d2.utils.events]: [0meta: 0:44:23  iter: 2299  total_loss: 0.093  loss_cls: 0.093  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0184  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:04:35 d2.utils.events]: [0meta: 0:44:18  iter: 2319  total_loss: 0.071  loss_cls: 0.071  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0182  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:04:39 d2.utils.events]: [0meta: 0:44:14  iter: 2339  total_loss: 0.066  loss_cls: 0.066  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0182  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:04:44 d2.utils.events]: [0meta: 0:44:09  iter: 2359  total_loss: 0.034  loss_cls: 0.034  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0182  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:04:48 d2.utils.events]: [0meta: 0:44:04  iter: 2379  total_loss: 0.061  loss_cls: 0.061  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0197  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:04:52 d2.utils.events]: [0meta: 0:44:00  iter: 2399  total_loss: 0.078  loss_cls: 0.078  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0197  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:04:57 d2.utils.events]: [0meta: 0:43:55  iter: 2419  total_loss: 0.104  loss_cls: 0.104  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0182  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:05:01 d2.utils.events]: [0meta: 0:43:51  iter: 2439  total_loss: 0.062  loss_cls: 0.062  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0177  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:05:05 d2.utils.events]: [0meta: 0:43:47  iter: 2459  total_loss: 0.063  loss_cls: 0.063  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0178  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:05:09 d2.utils.events]: [0meta: 0:43:42  iter: 2479  total_loss: 0.057  loss_cls: 0.057  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0177  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:05:14 d2.utils.events]: [0meta: 0:43:39  iter: 2499  total_loss: 0.043  loss_cls: 0.043  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0183  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:05:19 d2.utils.events]: [0meta: 0:43:34  iter: 2519  total_loss: 0.057  loss_cls: 0.057  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0187  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:05:23 d2.utils.events]: [0meta: 0:43:30  iter: 2539  total_loss: 0.088  loss_cls: 0.088  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0193  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:05:27 d2.utils.events]: [0meta: 0:43:27  iter: 2559  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0176  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:05:32 d2.utils.events]: [0meta: 0:43:22  iter: 2579  total_loss: 0.058  loss_cls: 0.058  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0175  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:05:36 d2.utils.events]: [0meta: 0:43:18  iter: 2599  total_loss: 0.038  loss_cls: 0.038  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0191  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:05:40 d2.utils.events]: [0meta: 0:43:14  iter: 2619  total_loss: 0.032  loss_cls: 0.032  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0182  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:05:45 d2.utils.events]: [0meta: 0:43:10  iter: 2639  total_loss: 0.065  loss_cls: 0.065  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0207  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:05:49 d2.utils.events]: [0meta: 0:43:07  iter: 2659  total_loss: 0.138  loss_cls: 0.138  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0211  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:05:53 d2.utils.events]: [0meta: 0:43:03  iter: 2679  total_loss: 0.043  loss_cls: 0.043  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0175  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:05:58 d2.utils.events]: [0meta: 0:42:58  iter: 2699  total_loss: 0.046  loss_cls: 0.046  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0175  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:06:02 d2.utils.events]: [0meta: 0:42:53  iter: 2719  total_loss: 0.027  loss_cls: 0.027  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0175  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:06:06 d2.utils.events]: [0meta: 0:42:49  iter: 2739  total_loss: 0.034  loss_cls: 0.034  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0174  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:06:11 d2.utils.events]: [0meta: 0:42:45  iter: 2759  total_loss: 0.073  loss_cls: 0.073  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0187  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:06:15 d2.utils.events]: [0meta: 0:42:40  iter: 2779  total_loss: 0.097  loss_cls: 0.097  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0191  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:06:19 d2.utils.events]: [0meta: 0:42:35  iter: 2799  total_loss: 0.045  loss_cls: 0.045  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0174  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:06:24 d2.utils.events]: [0meta: 0:42:30  iter: 2819  total_loss: 0.071  loss_cls: 0.071  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0161  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:06:28 d2.utils.events]: [0meta: 0:42:26  iter: 2839  total_loss: 0.046  loss_cls: 0.046  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0168  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:06:32 d2.utils.events]: [0meta: 0:42:21  iter: 2859  total_loss: 0.028  loss_cls: 0.028  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0165  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:06:36 d2.utils.events]: [0meta: 0:42:17  iter: 2879  total_loss: 0.020  loss_cls: 0.020  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0164  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:06:41 d2.utils.events]: [0meta: 0:42:13  iter: 2899  total_loss: 0.076  loss_cls: 0.076  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0162  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:06:45 d2.utils.events]: [0meta: 0:42:09  iter: 2919  total_loss: 0.042  loss_cls: 0.042  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0159  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:06:49 d2.utils.events]: [0meta: 0:42:04  iter: 2939  total_loss: 0.044  loss_cls: 0.044  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0163  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:06:53 d2.utils.events]: [0meta: 0:42:00  iter: 2959  total_loss: 0.032  loss_cls: 0.032  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0171  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:06:58 d2.utils.events]: [0meta: 0:41:56  iter: 2979  total_loss: 0.048  loss_cls: 0.048  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0170  lr: 0.000300  max_mem: 1822M
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
[32m[12/19 00:07:03 d2.data.common]: [0mSerializing 3618 elements to byte tensors and concatenating them all ...
[32m[12/19 00:07:04 d2.data.common]: [0mSerialized dataset takes 33.05 MiB
[32m[12/19 00:07:04 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 00:07:04 d2.evaluation.evaluator]: [0mStart inference on 3618 images
[32m[12/19 00:07:04 d2.evaluation.evaluator]: [0mInference done 11/3618. 0.0501 s / img. ETA=0:03:05
[32m[12/19 00:07:14 d2.evaluation.evaluator]: [0mInference done 203/3618. 0.0500 s / img. ETA=0:02:58
[32m[12/19 00:07:24 d2.evaluation.evaluator]: [0mInference done 391/3618. 0.0500 s / img. ETA=0:02:50
[32m[12/19 00:07:34 d2.evaluation.evaluator]: [0mInference done 572/3618. 0.0500 s / img. ETA=0:02:43
[32m[12/19 00:07:44 d2.evaluation.evaluator]: [0mInference done 753/3618. 0.0500 s / img. ETA=0:02:34
[32m[12/19 00:07:54 d2.evaluation.evaluator]: [0mInference done 929/3618. 0.0501 s / img. ETA=0:02:26
[32m[12/19 00:08:04 d2.evaluation.evaluator]: [0mInference done 1089/3618. 0.0506 s / img. ETA=0:02:21
[32m[12/19 00:08:14 d2.evaluation.evaluator]: [0mInference done 1263/3618. 0.0503 s / img. ETA=0:02:11
[32m[12/19 00:08:24 d2.evaluation.evaluator]: [0mInference done 1406/3618. 0.0511 s / img. ETA=0:02:07
[32m[12/19 00:08:35 d2.evaluation.evaluator]: [0mInference done 1572/3618. 0.0509 s / img. ETA=0:01:58
[32m[12/19 00:08:45 d2.evaluation.evaluator]: [0mInference done 1736/3618. 0.0507 s / img. ETA=0:01:49
[32m[12/19 00:08:55 d2.evaluation.evaluator]: [0mInference done 1895/3618. 0.0507 s / img. ETA=0:01:40
[32m[12/19 00:09:05 d2.evaluation.evaluator]: [0mInference done 2052/3618. 0.0506 s / img. ETA=0:01:32
[32m[12/19 00:09:15 d2.evaluation.evaluator]: [0mInference done 2207/3618. 0.0505 s / img. ETA=0:01:23
[32m[12/19 00:09:25 d2.evaluation.evaluator]: [0mInference done 2363/3618. 0.0504 s / img. ETA=0:01:14
[32m[12/19 00:09:35 d2.evaluation.evaluator]: [0mInference done 2518/3618. 0.0502 s / img. ETA=0:01:05
[32m[12/19 00:09:45 d2.evaluation.evaluator]: [0mInference done 2670/3618. 0.0501 s / img. ETA=0:00:57
[32m[12/19 00:09:55 d2.evaluation.evaluator]: [0mInference done 2818/3618. 0.0501 s / img. ETA=0:00:48
[32m[12/19 00:10:05 d2.evaluation.evaluator]: [0mInference done 2964/3618. 0.0500 s / img. ETA=0:00:39
[32m[12/19 00:10:15 d2.evaluation.evaluator]: [0mInference done 3106/3618. 0.0499 s / img. ETA=0:00:31
[32m[12/19 00:10:25 d2.evaluation.evaluator]: [0mInference done 3238/3618. 0.0500 s / img. ETA=0:00:23
[32m[12/19 00:10:35 d2.evaluation.evaluator]: [0mInference done 3377/3618. 0.0499 s / img. ETA=0:00:15
[32m[12/19 00:10:45 d2.evaluation.evaluator]: [0mInference done 3515/3618. 0.0499 s / img. ETA=0:00:06
[32m[12/19 00:10:53 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:49.384132 (0.063489 s / img per device, on 1 devices)
[32m[12/19 00:10:53 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:03:00 (0.049826 s / img per device, on 1 devices)
Evaluate predictions
We have 2442 single comp cutouts and 1176 multi
Plot predictions
Baseline assumption cat is 67.5% correct
train cat is 91.3% correct
35.34% improvement
[32m[12/19 00:10:56 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 00:10:56 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 00:10:56 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:10:56 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 00:10:56 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:10:56 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 00:10:56 d2.evaluation.evaluator]: [0m5.28%
[32m[12/19 00:10:56 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 00:10:56 d2.evaluation.evaluator]: [0m15.65%
[32m[12/19 00:10:56 d2.evaluation.evaluator]: [0mCatalogue is 0.9134881149806523 correct.
[32m[12/19 00:10:56 d2.engine.defaults]: [0mEvaluation results for train in csv format:
[32m[12/19 00:10:56 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 00:10:56 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 00:10:56 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0528,0.1565,0.9135
len dataset is: 1205 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_val.pkl
[32m[12/19 00:10:56 d2.data.common]: [0mSerializing 1205 elements to byte tensors and concatenating them all ...
[32m[12/19 00:10:56 d2.data.common]: [0mSerialized dataset takes 11.07 MiB
[32m[12/19 00:10:56 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 00:10:56 d2.evaluation.evaluator]: [0mStart inference on 1205 images
[32m[12/19 00:10:57 d2.evaluation.evaluator]: [0mInference done 11/1205. 0.0501 s / img. ETA=0:01:01
[32m[12/19 00:11:07 d2.evaluation.evaluator]: [0mInference done 203/1205. 0.0500 s / img. ETA=0:00:52
[32m[12/19 00:11:17 d2.evaluation.evaluator]: [0mInference done 390/1205. 0.0500 s / img. ETA=0:00:43
[32m[12/19 00:11:27 d2.evaluation.evaluator]: [0mInference done 577/1205. 0.0497 s / img. ETA=0:00:33
[32m[12/19 00:11:37 d2.evaluation.evaluator]: [0mInference done 745/1205. 0.0505 s / img. ETA=0:00:25
[32m[12/19 00:11:47 d2.evaluation.evaluator]: [0mInference done 868/1205. 0.0534 s / img. ETA=0:00:19
[32m[12/19 00:11:57 d2.evaluation.evaluator]: [0mInference done 989/1205. 0.0555 s / img. ETA=0:00:13
[32m[12/19 00:12:07 d2.evaluation.evaluator]: [0mInference done 1094/1205. 0.0579 s / img. ETA=0:00:07
[32m[12/19 00:12:17 d2.evaluation.evaluator]: [0mInference done 1188/1205. 0.0603 s / img. ETA=0:00:01
[32m[12/19 00:12:20 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:23.368613 (0.069474 s / img per device, on 1 devices)
[32m[12/19 00:12:20 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:12 (0.060670 s / img per device, on 1 devices)
Evaluate predictions
We have 817 single comp cutouts and 388 multi
Plot predictions
Baseline assumption cat is 67.8% correct
val cat is 90.1% correct
32.93% improvement
[32m[12/19 00:12:21 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 00:12:21 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 00:12:21 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:12:21 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 00:12:21 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:12:21 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 00:12:21 d2.evaluation.evaluator]: [0m5.88%
[32m[12/19 00:12:21 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 00:12:21 d2.evaluation.evaluator]: [0m18.30%
[32m[12/19 00:12:21 d2.evaluation.evaluator]: [0mCatalogue is 0.9012448132780083 correct.
[32m[12/19 00:12:21 d2.engine.defaults]: [0mEvaluation results for val in csv format:
[32m[12/19 00:12:21 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 00:12:21 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 00:12:21 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0588,0.1830,0.9012
len dataset is: 1211 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_test.pkl
[32m[12/19 00:12:21 d2.data.common]: [0mSerializing 1211 elements to byte tensors and concatenating them all ...
[32m[12/19 00:12:22 d2.data.common]: [0mSerialized dataset takes 11.23 MiB
[32m[12/19 00:12:22 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 00:12:22 d2.evaluation.evaluator]: [0mStart inference on 1211 images
[32m[12/19 00:12:27 d2.evaluation.evaluator]: [0mInference done 110/1211. 0.0500 s / img. ETA=0:00:57
[32m[12/19 00:12:37 d2.evaluation.evaluator]: [0mInference done 300/1211. 0.0500 s / img. ETA=0:00:47
[32m[12/19 00:12:47 d2.evaluation.evaluator]: [0mInference done 486/1211. 0.0500 s / img. ETA=0:00:38
[32m[12/19 00:12:57 d2.evaluation.evaluator]: [0mInference done 668/1211. 0.0500 s / img. ETA=0:00:29
[32m[12/19 00:13:07 d2.evaluation.evaluator]: [0mInference done 846/1211. 0.0500 s / img. ETA=0:00:19
[32m[12/19 00:13:17 d2.evaluation.evaluator]: [0mInference done 1020/1211. 0.0500 s / img. ETA=0:00:10
[32m[12/19 00:13:27 d2.evaluation.evaluator]: [0mInference done 1191/1211. 0.0500 s / img. ETA=0:00:01
[32m[12/19 00:13:29 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:07.234927 (0.055750 s / img per device, on 1 devices)
[32m[12/19 00:13:29 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:00 (0.050017 s / img per device, on 1 devices)
Evaluate predictions
We have 819 single comp cutouts and 392 multi
Plot predictions
Baseline assumption cat is 67.6% correct
test cat is 91.1% correct
34.68% improvement
[32m[12/19 00:13:30 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 00:13:30 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 00:13:30 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:13:30 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 00:13:30 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:13:30 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 00:13:30 d2.evaluation.evaluator]: [0m5.25%
[32m[12/19 00:13:30 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 00:13:30 d2.evaluation.evaluator]: [0m16.58%
[32m[12/19 00:13:30 d2.evaluation.evaluator]: [0mCatalogue is 0.9108175061932288 correct.
[32m[12/19 00:13:30 d2.engine.defaults]: [0mEvaluation results for test in csv format:
[32m[12/19 00:13:30 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 00:13:30 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 00:13:30 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0525,0.1658,0.9108
[32m[12/19 00:13:30 d2.utils.events]: [0meta: 0:41:52  iter: 2999  total_loss: 0.052  loss_cls: 0.052  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0203  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:13:34 d2.utils.events]: [0meta: 0:41:47  iter: 3019  total_loss: 0.061  loss_cls: 0.061  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0206  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:13:39 d2.utils.events]: [0meta: 0:41:43  iter: 3039  total_loss: 0.023  loss_cls: 0.023  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0175  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:13:43 d2.utils.events]: [0meta: 0:41:39  iter: 3059  total_loss: 0.066  loss_cls: 0.066  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0180  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:13:47 d2.utils.events]: [0meta: 0:41:35  iter: 3079  total_loss: 0.036  loss_cls: 0.036  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0158  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:13:51 d2.utils.events]: [0meta: 0:41:30  iter: 3099  total_loss: 0.060  loss_cls: 0.060  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0158  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:13:55 d2.utils.events]: [0meta: 0:41:26  iter: 3119  total_loss: 0.033  loss_cls: 0.033  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0158  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:14:00 d2.utils.events]: [0meta: 0:41:22  iter: 3139  total_loss: 0.056  loss_cls: 0.056  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0158  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:14:04 d2.utils.events]: [0meta: 0:41:18  iter: 3159  total_loss: 0.061  loss_cls: 0.061  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0197  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:14:08 d2.utils.events]: [0meta: 0:41:15  iter: 3179  total_loss: 0.133  loss_cls: 0.133  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0228  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:14:12 d2.utils.events]: [0meta: 0:41:11  iter: 3199  total_loss: 0.064  loss_cls: 0.064  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0231  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:14:17 d2.utils.events]: [0meta: 0:41:07  iter: 3219  total_loss: 0.053  loss_cls: 0.053  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0205  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:14:21 d2.utils.events]: [0meta: 0:41:04  iter: 3239  total_loss: 0.030  loss_cls: 0.030  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0196  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:14:25 d2.utils.events]: [0meta: 0:41:01  iter: 3259  total_loss: 0.054  loss_cls: 0.054  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0197  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:14:30 d2.utils.events]: [0meta: 0:40:57  iter: 3279  total_loss: 0.029  loss_cls: 0.029  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0188  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:14:34 d2.utils.events]: [0meta: 0:40:53  iter: 3299  total_loss: 0.036  loss_cls: 0.036  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0157  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:14:38 d2.utils.events]: [0meta: 0:40:48  iter: 3319  total_loss: 0.066  loss_cls: 0.066  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0158  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:14:43 d2.utils.events]: [0meta: 0:40:44  iter: 3339  total_loss: 0.034  loss_cls: 0.034  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0160  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:14:47 d2.utils.events]: [0meta: 0:40:39  iter: 3359  total_loss: 0.055  loss_cls: 0.055  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0159  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:14:52 d2.utils.events]: [0meta: 0:40:35  iter: 3379  total_loss: 0.029  loss_cls: 0.029  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0188  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:14:56 d2.utils.events]: [0meta: 0:40:31  iter: 3399  total_loss: 0.047  loss_cls: 0.047  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0186  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:15:01 d2.utils.events]: [0meta: 0:40:26  iter: 3419  total_loss: 0.055  loss_cls: 0.055  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0169  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:15:05 d2.utils.events]: [0meta: 0:40:23  iter: 3439  total_loss: 0.044  loss_cls: 0.044  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0169  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:15:09 d2.utils.events]: [0meta: 0:40:19  iter: 3459  total_loss: 0.030  loss_cls: 0.030  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0168  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:15:13 d2.utils.events]: [0meta: 0:40:15  iter: 3479  total_loss: 0.037  loss_cls: 0.037  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0197  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:15:18 d2.utils.events]: [0meta: 0:40:11  iter: 3499  total_loss: 0.059  loss_cls: 0.059  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0201  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:15:22 d2.utils.events]: [0meta: 0:40:07  iter: 3519  total_loss: 0.022  loss_cls: 0.022  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0201  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:15:26 d2.utils.events]: [0meta: 0:40:04  iter: 3539  total_loss: 0.023  loss_cls: 0.023  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0205  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:15:30 d2.utils.events]: [0meta: 0:40:00  iter: 3559  total_loss: 0.059  loss_cls: 0.059  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0201  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:15:35 d2.utils.events]: [0meta: 0:39:57  iter: 3579  total_loss: 0.068  loss_cls: 0.068  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0181  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:15:39 d2.utils.events]: [0meta: 0:39:52  iter: 3599  total_loss: 0.099  loss_cls: 0.099  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0164  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:15:43 d2.utils.events]: [0meta: 0:39:48  iter: 3619  total_loss: 0.053  loss_cls: 0.053  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0177  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:15:47 d2.utils.events]: [0meta: 0:39:44  iter: 3639  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0200  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:15:52 d2.utils.events]: [0meta: 0:39:38  iter: 3659  total_loss: 0.049  loss_cls: 0.049  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0168  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:15:56 d2.utils.events]: [0meta: 0:39:35  iter: 3679  total_loss: 0.047  loss_cls: 0.047  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0171  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:16:00 d2.utils.events]: [0meta: 0:39:31  iter: 3699  total_loss: 0.053  loss_cls: 0.053  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0192  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:16:04 d2.utils.events]: [0meta: 0:39:27  iter: 3719  total_loss: 0.022  loss_cls: 0.022  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0175  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:16:09 d2.utils.events]: [0meta: 0:39:22  iter: 3739  total_loss: 0.026  loss_cls: 0.026  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0169  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:16:13 d2.utils.events]: [0meta: 0:39:19  iter: 3759  total_loss: 0.045  loss_cls: 0.045  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0198  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:16:17 d2.utils.events]: [0meta: 0:39:15  iter: 3779  total_loss: 0.040  loss_cls: 0.040  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0208  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:16:21 d2.utils.events]: [0meta: 0:39:10  iter: 3799  total_loss: 0.048  loss_cls: 0.048  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0170  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:16:26 d2.utils.events]: [0meta: 0:39:06  iter: 3819  total_loss: 0.060  loss_cls: 0.060  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0171  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:16:30 d2.utils.events]: [0meta: 0:39:01  iter: 3839  total_loss: 0.052  loss_cls: 0.052  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0172  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:16:34 d2.utils.events]: [0meta: 0:38:57  iter: 3859  total_loss: 0.051  loss_cls: 0.051  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0174  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:16:38 d2.utils.events]: [0meta: 0:38:53  iter: 3879  total_loss: 0.055  loss_cls: 0.055  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0172  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:16:42 d2.utils.events]: [0meta: 0:38:48  iter: 3899  total_loss: 0.021  loss_cls: 0.021  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0176  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:16:47 d2.utils.events]: [0meta: 0:38:44  iter: 3919  total_loss: 0.042  loss_cls: 0.042  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0185  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:16:51 d2.utils.events]: [0meta: 0:38:40  iter: 3939  total_loss: 0.050  loss_cls: 0.050  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0171  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:16:55 d2.utils.events]: [0meta: 0:38:36  iter: 3959  total_loss: 0.047  loss_cls: 0.047  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0174  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:16:59 d2.utils.events]: [0meta: 0:38:31  iter: 3979  total_loss: 0.032  loss_cls: 0.032  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0171  lr: 0.000300  max_mem: 1822M
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
[32m[12/19 00:17:06 d2.data.common]: [0mSerializing 3618 elements to byte tensors and concatenating them all ...
[32m[12/19 00:17:06 d2.data.common]: [0mSerialized dataset takes 33.05 MiB
[32m[12/19 00:17:06 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 00:17:06 d2.evaluation.evaluator]: [0mStart inference on 3618 images
[32m[12/19 00:17:07 d2.evaluation.evaluator]: [0mInference done 11/3618. 0.0501 s / img. ETA=0:03:05
[32m[12/19 00:17:17 d2.evaluation.evaluator]: [0mInference done 203/3618. 0.0500 s / img. ETA=0:02:58
[32m[12/19 00:17:27 d2.evaluation.evaluator]: [0mInference done 391/3618. 0.0500 s / img. ETA=0:02:50
[32m[12/19 00:17:37 d2.evaluation.evaluator]: [0mInference done 572/3618. 0.0500 s / img. ETA=0:02:43
[32m[12/19 00:17:47 d2.evaluation.evaluator]: [0mInference done 752/3618. 0.0500 s / img. ETA=0:02:34
[32m[12/19 00:17:57 d2.evaluation.evaluator]: [0mInference done 929/3618. 0.0500 s / img. ETA=0:02:26
[32m[12/19 00:18:07 d2.evaluation.evaluator]: [0mInference done 1100/3618. 0.0500 s / img. ETA=0:02:19
[32m[12/19 00:18:17 d2.evaluation.evaluator]: [0mInference done 1271/3618. 0.0500 s / img. ETA=0:02:10
[32m[12/19 00:18:27 d2.evaluation.evaluator]: [0mInference done 1439/3618. 0.0500 s / img. ETA=0:02:02
[32m[12/19 00:18:37 d2.evaluation.evaluator]: [0mInference done 1604/3618. 0.0500 s / img. ETA=0:01:54
[32m[12/19 00:18:47 d2.evaluation.evaluator]: [0mInference done 1765/3618. 0.0500 s / img. ETA=0:01:45
[32m[12/19 00:18:57 d2.evaluation.evaluator]: [0mInference done 1924/3618. 0.0500 s / img. ETA=0:01:37
[32m[12/19 00:19:07 d2.evaluation.evaluator]: [0mInference done 2080/3618. 0.0500 s / img. ETA=0:01:29
[32m[12/19 00:19:17 d2.evaluation.evaluator]: [0mInference done 2234/3618. 0.0500 s / img. ETA=0:01:21
[32m[12/19 00:19:27 d2.evaluation.evaluator]: [0mInference done 2385/3618. 0.0500 s / img. ETA=0:01:12
[32m[12/19 00:19:37 d2.evaluation.evaluator]: [0mInference done 2534/3618. 0.0500 s / img. ETA=0:01:04
[32m[12/19 00:19:47 d2.evaluation.evaluator]: [0mInference done 2683/3618. 0.0500 s / img. ETA=0:00:56
[32m[12/19 00:19:57 d2.evaluation.evaluator]: [0mInference done 2826/3618. 0.0500 s / img. ETA=0:00:47
[32m[12/19 00:20:07 d2.evaluation.evaluator]: [0mInference done 2959/3618. 0.0501 s / img. ETA=0:00:40
[32m[12/19 00:20:17 d2.evaluation.evaluator]: [0mInference done 3063/3618. 0.0506 s / img. ETA=0:00:34
[32m[12/19 00:20:27 d2.evaluation.evaluator]: [0mInference done 3180/3618. 0.0508 s / img. ETA=0:00:27
[32m[12/19 00:20:37 d2.evaluation.evaluator]: [0mInference done 3274/3618. 0.0513 s / img. ETA=0:00:22
[32m[12/19 00:20:47 d2.evaluation.evaluator]: [0mInference done 3380/3618. 0.0516 s / img. ETA=0:00:15
[32m[12/19 00:20:57 d2.evaluation.evaluator]: [0mInference done 3473/3618. 0.0521 s / img. ETA=0:00:09
[32m[12/19 00:21:07 d2.evaluation.evaluator]: [0mInference done 3595/3618. 0.0521 s / img. ETA=0:00:01
[32m[12/19 00:21:10 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:03.749659 (0.067465 s / img per device, on 1 devices)
[32m[12/19 00:21:10 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:03:08 (0.052086 s / img per device, on 1 devices)
Evaluate predictions
We have 2442 single comp cutouts and 1176 multi
Plot predictions
Baseline assumption cat is 67.5% correct
train cat is 88.6% correct
31.20% improvement
[32m[12/19 00:21:13 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 00:21:13 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 00:21:13 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:21:13 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 00:21:13 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:21:13 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 00:21:13 d2.evaluation.evaluator]: [0m6.72%
[32m[12/19 00:21:13 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 00:21:13 d2.evaluation.evaluator]: [0m21.26%
[32m[12/19 00:21:13 d2.evaluation.evaluator]: [0mCatalogue is 0.8855721393034826 correct.
[32m[12/19 00:21:13 d2.engine.defaults]: [0mEvaluation results for train in csv format:
[32m[12/19 00:21:13 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 00:21:13 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 00:21:13 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0672,0.2126,0.8856
len dataset is: 1205 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_val.pkl
[32m[12/19 00:21:13 d2.data.common]: [0mSerializing 1205 elements to byte tensors and concatenating them all ...
[32m[12/19 00:21:13 d2.data.common]: [0mSerialized dataset takes 11.07 MiB
[32m[12/19 00:21:13 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 00:21:13 d2.evaluation.evaluator]: [0mStart inference on 1205 images
[32m[12/19 00:21:17 d2.evaluation.evaluator]: [0mInference done 88/1205. 0.0493 s / img. ETA=0:00:57
[32m[12/19 00:21:27 d2.evaluation.evaluator]: [0mInference done 284/1205. 0.0487 s / img. ETA=0:00:47
[32m[12/19 00:21:37 d2.evaluation.evaluator]: [0mInference done 475/1205. 0.0486 s / img. ETA=0:00:37
[32m[12/19 00:21:47 d2.evaluation.evaluator]: [0mInference done 638/1205. 0.0503 s / img. ETA=0:00:30
[32m[12/19 00:21:58 d2.evaluation.evaluator]: [0mInference done 748/1205. 0.0547 s / img. ETA=0:00:27
[32m[12/19 00:22:08 d2.evaluation.evaluator]: [0mInference done 872/1205. 0.0569 s / img. ETA=0:00:20
[32m[12/19 00:22:18 d2.evaluation.evaluator]: [0mInference done 1021/1205. 0.0570 s / img. ETA=0:00:11
[32m[12/19 00:22:28 d2.evaluation.evaluator]: [0mInference done 1167/1205. 0.0571 s / img. ETA=0:00:02
[32m[12/19 00:22:30 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:17.060390 (0.064217 s / img per device, on 1 devices)
[32m[12/19 00:22:30 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:08 (0.056855 s / img per device, on 1 devices)
Evaluate predictions
We have 817 single comp cutouts and 388 multi
Plot predictions
Baseline assumption cat is 67.8% correct
val cat is 87.2% correct
28.64% improvement
[32m[12/19 00:22:31 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 00:22:31 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 00:22:31 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:22:31 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 00:22:31 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:22:31 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 00:22:31 d2.evaluation.evaluator]: [0m7.59%
[32m[12/19 00:22:31 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 00:22:31 d2.evaluation.evaluator]: [0m23.71%
[32m[12/19 00:22:31 d2.evaluation.evaluator]: [0mCatalogue is 0.8721991701244813 correct.
[32m[12/19 00:22:31 d2.engine.defaults]: [0mEvaluation results for val in csv format:
[32m[12/19 00:22:31 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 00:22:31 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 00:22:31 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0759,0.2371,0.8722
len dataset is: 1211 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_test.pkl
[32m[12/19 00:22:31 d2.data.common]: [0mSerializing 1211 elements to byte tensors and concatenating them all ...
[32m[12/19 00:22:31 d2.data.common]: [0mSerialized dataset takes 11.23 MiB
[32m[12/19 00:22:31 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 00:22:31 d2.evaluation.evaluator]: [0mStart inference on 1211 images
[32m[12/19 00:22:38 d2.evaluation.evaluator]: [0mInference done 119/1211. 0.0499 s / img. ETA=0:00:56
[32m[12/19 00:22:48 d2.evaluation.evaluator]: [0mInference done 309/1211. 0.0499 s / img. ETA=0:00:47
[32m[12/19 00:22:58 d2.evaluation.evaluator]: [0mInference done 494/1211. 0.0500 s / img. ETA=0:00:38
[32m[12/19 00:23:08 d2.evaluation.evaluator]: [0mInference done 676/1211. 0.0500 s / img. ETA=0:00:28
[32m[12/19 00:23:18 d2.evaluation.evaluator]: [0mInference done 854/1211. 0.0500 s / img. ETA=0:00:19
[32m[12/19 00:23:28 d2.evaluation.evaluator]: [0mInference done 1008/1211. 0.0509 s / img. ETA=0:00:11
[32m[12/19 00:23:38 d2.evaluation.evaluator]: [0mInference done 1180/1211. 0.0508 s / img. ETA=0:00:01
[32m[12/19 00:23:40 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:08.328494 (0.056657 s / img per device, on 1 devices)
[32m[12/19 00:23:40 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:01 (0.050763 s / img per device, on 1 devices)
Evaluate predictions
We have 819 single comp cutouts and 392 multi
Plot predictions
Baseline assumption cat is 67.6% correct
test cat is 88.0% correct
30.16% improvement
[32m[12/19 00:23:41 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 00:23:41 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 00:23:41 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:23:41 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 00:23:41 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:23:41 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 00:23:41 d2.evaluation.evaluator]: [0m6.47%
[32m[12/19 00:23:41 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 00:23:41 d2.evaluation.evaluator]: [0m23.47%
[32m[12/19 00:23:41 d2.evaluation.evaluator]: [0mCatalogue is 0.8802642444260942 correct.
[32m[12/19 00:23:41 d2.engine.defaults]: [0mEvaluation results for test in csv format:
[32m[12/19 00:23:41 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 00:23:41 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 00:23:41 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0647,0.2347,0.8803
[32m[12/19 00:23:41 d2.utils.events]: [0meta: 0:38:27  iter: 3999  total_loss: 0.025  loss_cls: 0.025  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0191  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:23:45 d2.utils.events]: [0meta: 0:38:23  iter: 4019  total_loss: 0.020  loss_cls: 0.020  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0214  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:23:50 d2.utils.events]: [0meta: 0:38:20  iter: 4039  total_loss: 0.030  loss_cls: 0.030  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0214  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:23:54 d2.utils.events]: [0meta: 0:38:17  iter: 4059  total_loss: 0.052  loss_cls: 0.052  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0212  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:23:58 d2.utils.events]: [0meta: 0:38:14  iter: 4079  total_loss: 0.071  loss_cls: 0.071  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0213  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:24:02 d2.utils.events]: [0meta: 0:38:10  iter: 4099  total_loss: 0.023  loss_cls: 0.023  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0213  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:24:07 d2.utils.events]: [0meta: 0:38:07  iter: 4119  total_loss: 0.031  loss_cls: 0.031  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0213  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:24:11 d2.utils.events]: [0meta: 0:38:04  iter: 4139  total_loss: 0.046  loss_cls: 0.046  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0212  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:24:15 d2.utils.events]: [0meta: 0:38:00  iter: 4159  total_loss: 0.036  loss_cls: 0.036  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0212  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:24:19 d2.utils.events]: [0meta: 0:37:56  iter: 4179  total_loss: 0.052  loss_cls: 0.052  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0212  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:24:24 d2.utils.events]: [0meta: 0:37:51  iter: 4199  total_loss: 0.031  loss_cls: 0.031  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0212  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:24:28 d2.utils.events]: [0meta: 0:37:48  iter: 4219  total_loss: 0.035  loss_cls: 0.035  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0212  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:24:32 d2.utils.events]: [0meta: 0:37:43  iter: 4239  total_loss: 0.027  loss_cls: 0.027  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0176  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:24:36 d2.utils.events]: [0meta: 0:37:39  iter: 4259  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0174  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:24:41 d2.utils.events]: [0meta: 0:37:34  iter: 4279  total_loss: 0.038  loss_cls: 0.038  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0190  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:24:45 d2.utils.events]: [0meta: 0:37:31  iter: 4299  total_loss: 0.028  loss_cls: 0.028  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0183  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:24:50 d2.utils.events]: [0meta: 0:37:28  iter: 4319  total_loss: 0.048  loss_cls: 0.048  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0183  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:24:54 d2.utils.events]: [0meta: 0:37:25  iter: 4339  total_loss: 0.048  loss_cls: 0.048  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0188  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:24:59 d2.utils.events]: [0meta: 0:37:22  iter: 4359  total_loss: 0.046  loss_cls: 0.046  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0212  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:25:03 d2.utils.events]: [0meta: 0:37:17  iter: 4379  total_loss: 0.067  loss_cls: 0.067  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0171  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:25:07 d2.utils.events]: [0meta: 0:37:12  iter: 4399  total_loss: 0.029  loss_cls: 0.029  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0173  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:25:11 d2.utils.events]: [0meta: 0:37:08  iter: 4419  total_loss: 0.048  loss_cls: 0.048  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0186  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:25:16 d2.utils.events]: [0meta: 0:37:03  iter: 4439  total_loss: 0.020  loss_cls: 0.020  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0169  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:25:20 d2.utils.events]: [0meta: 0:36:58  iter: 4459  total_loss: 0.066  loss_cls: 0.066  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0169  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:25:24 d2.utils.events]: [0meta: 0:36:53  iter: 4479  total_loss: 0.037  loss_cls: 0.037  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0170  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:25:28 d2.utils.events]: [0meta: 0:36:49  iter: 4499  total_loss: 0.047  loss_cls: 0.047  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0200  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:25:32 d2.utils.events]: [0meta: 0:36:45  iter: 4519  total_loss: 0.030  loss_cls: 0.030  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0186  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:25:37 d2.utils.events]: [0meta: 0:36:40  iter: 4539  total_loss: 0.041  loss_cls: 0.041  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0176  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:25:41 d2.utils.events]: [0meta: 0:36:35  iter: 4559  total_loss: 0.015  loss_cls: 0.015  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0192  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:25:45 d2.utils.events]: [0meta: 0:36:30  iter: 4579  total_loss: 0.044  loss_cls: 0.044  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0164  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:25:50 d2.utils.events]: [0meta: 0:36:26  iter: 4599  total_loss: 0.049  loss_cls: 0.049  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0161  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:25:54 d2.utils.events]: [0meta: 0:36:21  iter: 4619  total_loss: 0.045  loss_cls: 0.045  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0166  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:25:58 d2.utils.events]: [0meta: 0:36:15  iter: 4639  total_loss: 0.045  loss_cls: 0.045  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0162  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:26:02 d2.utils.events]: [0meta: 0:36:12  iter: 4659  total_loss: 0.026  loss_cls: 0.026  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0161  lr: 0.000300  max_mem: 1822M
[32m[12/19 00:26:07 d2.utils.events]: [0meta: 0:36:07  iter: 4679  total_loss: 0.065  loss_cls: 0.065  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0162  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:26:11 d2.utils.events]: [0meta: 0:36:01  iter: 4699  total_loss: 0.034  loss_cls: 0.034  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0163  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:26:15 d2.utils.events]: [0meta: 0:35:57  iter: 4719  total_loss: 0.048  loss_cls: 0.048  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0178  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:26:19 d2.utils.events]: [0meta: 0:35:52  iter: 4739  total_loss: 0.037  loss_cls: 0.037  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0162  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:26:23 d2.utils.events]: [0meta: 0:35:47  iter: 4759  total_loss: 0.026  loss_cls: 0.026  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0173  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:26:28 d2.utils.events]: [0meta: 0:35:41  iter: 4779  total_loss: 0.024  loss_cls: 0.024  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0161  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:26:32 d2.utils.events]: [0meta: 0:35:37  iter: 4799  total_loss: 0.036  loss_cls: 0.036  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0167  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:26:36 d2.utils.events]: [0meta: 0:35:33  iter: 4819  total_loss: 0.032  loss_cls: 0.032  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0162  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:26:40 d2.utils.events]: [0meta: 0:35:28  iter: 4839  total_loss: 0.033  loss_cls: 0.033  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0160  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:26:44 d2.utils.events]: [0meta: 0:35:23  iter: 4859  total_loss: 0.058  loss_cls: 0.058  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0162  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:26:49 d2.utils.events]: [0meta: 0:35:19  iter: 4879  total_loss: 0.044  loss_cls: 0.044  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0170  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:26:53 d2.utils.events]: [0meta: 0:35:14  iter: 4899  total_loss: 0.037  loss_cls: 0.037  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0161  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:26:57 d2.utils.events]: [0meta: 0:35:10  iter: 4919  total_loss: 0.063  loss_cls: 0.063  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0161  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:27:01 d2.utils.events]: [0meta: 0:35:05  iter: 4939  total_loss: 0.023  loss_cls: 0.023  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0162  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:27:05 d2.utils.events]: [0meta: 0:35:00  iter: 4959  total_loss: 0.031  loss_cls: 0.031  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0162  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:27:10 d2.utils.events]: [0meta: 0:34:56  iter: 4979  total_loss: 0.028  loss_cls: 0.028  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0161  lr: 0.000300  max_mem: 1825M
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
[32m[12/19 00:27:15 d2.data.common]: [0mSerializing 3618 elements to byte tensors and concatenating them all ...
[32m[12/19 00:27:16 d2.data.common]: [0mSerialized dataset takes 33.05 MiB
[32m[12/19 00:27:16 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 00:27:16 d2.evaluation.evaluator]: [0mStart inference on 3618 images
[32m[12/19 00:27:16 d2.evaluation.evaluator]: [0mInference done 11/3618. 0.0486 s / img. ETA=0:02:59
[32m[12/19 00:27:26 d2.evaluation.evaluator]: [0mInference done 198/3618. 0.0512 s / img. ETA=0:03:02
[32m[12/19 00:27:36 d2.evaluation.evaluator]: [0mInference done 391/3618. 0.0499 s / img. ETA=0:02:49
[32m[12/19 00:27:46 d2.evaluation.evaluator]: [0mInference done 580/3618. 0.0494 s / img. ETA=0:02:40
[32m[12/19 00:27:56 d2.evaluation.evaluator]: [0mInference done 765/3618. 0.0492 s / img. ETA=0:02:31
[32m[12/19 00:28:06 d2.evaluation.evaluator]: [0mInference done 947/3618. 0.0491 s / img. ETA=0:02:22
[32m[12/19 00:28:16 d2.evaluation.evaluator]: [0mInference done 1125/3618. 0.0490 s / img. ETA=0:02:14
[32m[12/19 00:28:26 d2.evaluation.evaluator]: [0mInference done 1300/3618. 0.0490 s / img. ETA=0:02:06
[32m[12/19 00:28:36 d2.evaluation.evaluator]: [0mInference done 1472/3618. 0.0489 s / img. ETA=0:01:57
[32m[12/19 00:28:46 d2.evaluation.evaluator]: [0mInference done 1641/3618. 0.0489 s / img. ETA=0:01:49
[32m[12/19 00:28:56 d2.evaluation.evaluator]: [0mInference done 1807/3618. 0.0488 s / img. ETA=0:01:41
[32m[12/19 00:29:06 d2.evaluation.evaluator]: [0mInference done 1970/3618. 0.0488 s / img. ETA=0:01:32
[32m[12/19 00:29:17 d2.evaluation.evaluator]: [0mInference done 2130/3618. 0.0488 s / img. ETA=0:01:24
[32m[12/19 00:29:27 d2.evaluation.evaluator]: [0mInference done 2287/3618. 0.0488 s / img. ETA=0:01:16
[32m[12/19 00:29:37 d2.evaluation.evaluator]: [0mInference done 2442/3618. 0.0488 s / img. ETA=0:01:07
[32m[12/19 00:29:47 d2.evaluation.evaluator]: [0mInference done 2595/3618. 0.0487 s / img. ETA=0:00:59
[32m[12/19 00:29:57 d2.evaluation.evaluator]: [0mInference done 2745/3618. 0.0487 s / img. ETA=0:00:51
[32m[12/19 00:30:07 d2.evaluation.evaluator]: [0mInference done 2892/3618. 0.0487 s / img. ETA=0:00:42
[32m[12/19 00:30:17 d2.evaluation.evaluator]: [0mInference done 3037/3618. 0.0487 s / img. ETA=0:00:34
[32m[12/19 00:30:27 d2.evaluation.evaluator]: [0mInference done 3176/3618. 0.0488 s / img. ETA=0:00:26
[32m[12/19 00:30:37 d2.evaluation.evaluator]: [0mInference done 3296/3618. 0.0490 s / img. ETA=0:00:19
[32m[12/19 00:30:47 d2.evaluation.evaluator]: [0mInference done 3409/3618. 0.0493 s / img. ETA=0:00:12
[32m[12/19 00:30:57 d2.evaluation.evaluator]: [0mInference done 3509/3618. 0.0498 s / img. ETA=0:00:06
[32m[12/19 00:31:07 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:50.719181 (0.063858 s / img per device, on 1 devices)
[32m[12/19 00:31:07 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:59 (0.049818 s / img per device, on 1 devices)
Evaluate predictions
We have 2442 single comp cutouts and 1176 multi
Plot predictions
Baseline assumption cat is 67.5% correct
train cat is 91.0% correct
34.77% improvement
[32m[12/19 00:31:09 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 00:31:09 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 00:31:09 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:31:09 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 00:31:09 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:31:09 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 00:31:09 d2.evaluation.evaluator]: [0m5.61%
[32m[12/19 00:31:09 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 00:31:09 d2.evaluation.evaluator]: [0m16.16%
[32m[12/19 00:31:09 d2.evaluation.evaluator]: [0mCatalogue is 0.9096185737976783 correct.
[32m[12/19 00:31:09 d2.engine.defaults]: [0mEvaluation results for train in csv format:
[32m[12/19 00:31:09 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 00:31:09 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 00:31:09 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0561,0.1616,0.9096
len dataset is: 1205 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_val.pkl
[32m[12/19 00:31:09 d2.data.common]: [0mSerializing 1205 elements to byte tensors and concatenating them all ...
[32m[12/19 00:31:09 d2.data.common]: [0mSerialized dataset takes 11.07 MiB
[32m[12/19 00:31:09 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 00:31:09 d2.evaluation.evaluator]: [0mStart inference on 1205 images
[32m[12/19 00:31:10 d2.evaluation.evaluator]: [0mInference done 11/1205. 0.0498 s / img. ETA=0:01:01
[32m[12/19 00:31:20 d2.evaluation.evaluator]: [0mInference done 203/1205. 0.0499 s / img. ETA=0:00:52
[32m[12/19 00:31:30 d2.evaluation.evaluator]: [0mInference done 369/1205. 0.0529 s / img. ETA=0:00:46
[32m[12/19 00:31:40 d2.evaluation.evaluator]: [0mInference done 527/1205. 0.0545 s / img. ETA=0:00:39
[32m[12/19 00:31:50 d2.evaluation.evaluator]: [0mInference done 683/1205. 0.0551 s / img. ETA=0:00:31
[32m[12/19 00:32:00 d2.evaluation.evaluator]: [0mInference done 851/1205. 0.0547 s / img. ETA=0:00:21
[32m[12/19 00:32:10 d2.evaluation.evaluator]: [0mInference done 1010/1205. 0.0547 s / img. ETA=0:00:11
[32m[12/19 00:32:20 d2.evaluation.evaluator]: [0mInference done 1155/1205. 0.0551 s / img. ETA=0:00:03
[32m[12/19 00:32:24 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:13.954946 (0.061629 s / img per device, on 1 devices)
[32m[12/19 00:32:24 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:05 (0.054903 s / img per device, on 1 devices)
Evaluate predictions
We have 817 single comp cutouts and 388 multi
Plot predictions
Baseline assumption cat is 67.8% correct
val cat is 90.0% correct
32.80% improvement
[32m[12/19 00:32:25 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 00:32:25 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 00:32:25 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:32:25 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 00:32:25 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:32:25 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 00:32:25 d2.evaluation.evaluator]: [0m6.24%
[32m[12/19 00:32:25 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 00:32:25 d2.evaluation.evaluator]: [0m17.78%
[32m[12/19 00:32:25 d2.evaluation.evaluator]: [0mCatalogue is 0.9004149377593361 correct.
[32m[12/19 00:32:25 d2.engine.defaults]: [0mEvaluation results for val in csv format:
[32m[12/19 00:32:25 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 00:32:25 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 00:32:25 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0624,0.1778,0.9004
len dataset is: 1211 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_test.pkl
[32m[12/19 00:32:25 d2.data.common]: [0mSerializing 1211 elements to byte tensors and concatenating them all ...
[32m[12/19 00:32:25 d2.data.common]: [0mSerialized dataset takes 11.23 MiB
[32m[12/19 00:32:25 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 00:32:25 d2.evaluation.evaluator]: [0mStart inference on 1211 images
[32m[12/19 00:32:30 d2.evaluation.evaluator]: [0mInference done 101/1211. 0.0529 s / img. ETA=0:01:00
[32m[12/19 00:32:40 d2.evaluation.evaluator]: [0mInference done 287/1211. 0.0518 s / img. ETA=0:00:50
[32m[12/19 00:32:50 d2.evaluation.evaluator]: [0mInference done 464/1211. 0.0521 s / img. ETA=0:00:41
[32m[12/19 00:33:00 d2.evaluation.evaluator]: [0mInference done 652/1211. 0.0510 s / img. ETA=0:00:30
[32m[12/19 00:33:10 d2.evaluation.evaluator]: [0mInference done 836/1211. 0.0504 s / img. ETA=0:00:20
[32m[12/19 00:33:21 d2.evaluation.evaluator]: [0mInference done 1014/1211. 0.0502 s / img. ETA=0:00:10
[32m[12/19 00:33:31 d2.evaluation.evaluator]: [0mInference done 1191/1211. 0.0500 s / img. ETA=0:00:01
[32m[12/19 00:33:32 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:07.106767 (0.055644 s / img per device, on 1 devices)
[32m[12/19 00:33:32 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:00 (0.049949 s / img per device, on 1 devices)
Evaluate predictions
We have 819 single comp cutouts and 392 multi
Plot predictions
Baseline assumption cat is 67.6% correct
test cat is 90.3% correct
33.58% improvement
[32m[12/19 00:33:33 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 00:33:33 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 00:33:33 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:33:33 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 00:33:33 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:33:33 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 00:33:33 d2.evaluation.evaluator]: [0m6.23%
[32m[12/19 00:33:33 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 00:33:33 d2.evaluation.evaluator]: [0m16.84%
[32m[12/19 00:33:33 d2.evaluation.evaluator]: [0mCatalogue is 0.9033856317093312 correct.
[32m[12/19 00:33:33 d2.engine.defaults]: [0mEvaluation results for test in csv format:
[32m[12/19 00:33:33 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 00:33:33 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 00:33:33 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0623,0.1684,0.9034
[32m[12/19 00:33:33 d2.utils.events]: [0meta: 0:34:51  iter: 4999  total_loss: 0.021  loss_cls: 0.021  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0172  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:33:37 d2.utils.events]: [0meta: 0:34:47  iter: 5019  total_loss: 0.041  loss_cls: 0.041  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0210  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:33:42 d2.utils.events]: [0meta: 0:34:42  iter: 5039  total_loss: 0.049  loss_cls: 0.049  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0210  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:33:46 d2.utils.events]: [0meta: 0:34:38  iter: 5059  total_loss: 0.032  loss_cls: 0.032  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0205  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:33:50 d2.utils.events]: [0meta: 0:34:34  iter: 5079  total_loss: 0.023  loss_cls: 0.023  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0206  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:33:54 d2.utils.events]: [0meta: 0:34:29  iter: 5099  total_loss: 0.059  loss_cls: 0.059  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0180  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:33:59 d2.utils.events]: [0meta: 0:34:24  iter: 5119  total_loss: 0.019  loss_cls: 0.019  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0168  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:34:03 d2.utils.events]: [0meta: 0:34:18  iter: 5139  total_loss: 0.041  loss_cls: 0.041  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0169  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:34:07 d2.utils.events]: [0meta: 0:34:12  iter: 5159  total_loss: 0.047  loss_cls: 0.047  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0171  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:34:11 d2.utils.events]: [0meta: 0:34:07  iter: 5179  total_loss: 0.030  loss_cls: 0.030  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0172  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:34:16 d2.utils.events]: [0meta: 0:34:01  iter: 5199  total_loss: 0.052  loss_cls: 0.052  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0177  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:34:20 d2.utils.events]: [0meta: 0:33:57  iter: 5219  total_loss: 0.067  loss_cls: 0.067  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0183  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:34:24 d2.utils.events]: [0meta: 0:33:53  iter: 5239  total_loss: 0.044  loss_cls: 0.044  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0176  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:34:28 d2.utils.events]: [0meta: 0:33:48  iter: 5259  total_loss: 0.035  loss_cls: 0.035  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0176  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:34:32 d2.utils.events]: [0meta: 0:33:44  iter: 5279  total_loss: 0.030  loss_cls: 0.030  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0176  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:34:37 d2.utils.events]: [0meta: 0:33:40  iter: 5299  total_loss: 0.062  loss_cls: 0.062  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0176  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:34:41 d2.utils.events]: [0meta: 0:33:35  iter: 5319  total_loss: 0.027  loss_cls: 0.027  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0173  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:34:45 d2.utils.events]: [0meta: 0:33:31  iter: 5339  total_loss: 0.046  loss_cls: 0.046  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0176  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:34:49 d2.utils.events]: [0meta: 0:33:25  iter: 5359  total_loss: 0.040  loss_cls: 0.040  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0171  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:34:53 d2.utils.events]: [0meta: 0:33:22  iter: 5379  total_loss: 0.031  loss_cls: 0.031  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0195  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:34:58 d2.utils.events]: [0meta: 0:33:18  iter: 5399  total_loss: 0.036  loss_cls: 0.036  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0195  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:35:02 d2.utils.events]: [0meta: 0:33:14  iter: 5419  total_loss: 0.022  loss_cls: 0.022  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0168  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:35:06 d2.utils.events]: [0meta: 0:33:10  iter: 5439  total_loss: 0.032  loss_cls: 0.032  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0155  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:35:10 d2.utils.events]: [0meta: 0:33:06  iter: 5459  total_loss: 0.015  loss_cls: 0.015  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0156  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:35:15 d2.utils.events]: [0meta: 0:33:02  iter: 5479  total_loss: 0.053  loss_cls: 0.053  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0156  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:35:19 d2.utils.events]: [0meta: 0:32:57  iter: 5499  total_loss: 0.024  loss_cls: 0.024  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0152  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:35:23 d2.utils.events]: [0meta: 0:32:52  iter: 5519  total_loss: 0.022  loss_cls: 0.022  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0153  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:35:27 d2.utils.events]: [0meta: 0:32:48  iter: 5539  total_loss: 0.038  loss_cls: 0.038  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0151  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:35:32 d2.utils.events]: [0meta: 0:32:43  iter: 5559  total_loss: 0.026  loss_cls: 0.026  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0151  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:35:37 d2.utils.events]: [0meta: 0:32:39  iter: 5579  total_loss: 0.039  loss_cls: 0.039  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0193  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:35:42 d2.utils.events]: [0meta: 0:32:37  iter: 5599  total_loss: 0.019  loss_cls: 0.019  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0187  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:35:46 d2.utils.events]: [0meta: 0:32:33  iter: 5619  total_loss: 0.015  loss_cls: 0.015  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0172  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:35:51 d2.utils.events]: [0meta: 0:32:28  iter: 5639  total_loss: 0.029  loss_cls: 0.029  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0167  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:35:55 d2.utils.events]: [0meta: 0:32:24  iter: 5659  total_loss: 0.028  loss_cls: 0.028  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0167  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:35:59 d2.utils.events]: [0meta: 0:32:20  iter: 5679  total_loss: 0.031  loss_cls: 0.031  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0164  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:36:03 d2.utils.events]: [0meta: 0:32:16  iter: 5699  total_loss: 0.045  loss_cls: 0.045  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0163  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:36:07 d2.utils.events]: [0meta: 0:32:11  iter: 5719  total_loss: 0.035  loss_cls: 0.035  loss_box_reg: 0.000  time: 0.2122  data_time: 0.0165  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:36:11 d2.utils.events]: [0meta: 0:32:07  iter: 5739  total_loss: 0.042  loss_cls: 0.042  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0166  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:36:16 d2.utils.events]: [0meta: 0:32:02  iter: 5759  total_loss: 0.030  loss_cls: 0.030  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0164  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:36:20 d2.utils.events]: [0meta: 0:31:58  iter: 5779  total_loss: 0.032  loss_cls: 0.032  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0163  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:36:24 d2.utils.events]: [0meta: 0:31:54  iter: 5799  total_loss: 0.035  loss_cls: 0.035  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0163  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:36:28 d2.utils.events]: [0meta: 0:31:50  iter: 5819  total_loss: 0.033  loss_cls: 0.033  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0163  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:36:32 d2.utils.events]: [0meta: 0:31:46  iter: 5839  total_loss: 0.033  loss_cls: 0.033  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0163  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:36:37 d2.utils.events]: [0meta: 0:31:42  iter: 5859  total_loss: 0.020  loss_cls: 0.020  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0166  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:36:41 d2.utils.events]: [0meta: 0:31:38  iter: 5879  total_loss: 0.019  loss_cls: 0.019  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0175  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:36:45 d2.utils.events]: [0meta: 0:31:34  iter: 5899  total_loss: 0.035  loss_cls: 0.035  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0165  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:36:49 d2.utils.events]: [0meta: 0:31:29  iter: 5919  total_loss: 0.023  loss_cls: 0.023  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0165  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:36:53 d2.utils.events]: [0meta: 0:31:25  iter: 5939  total_loss: 0.042  loss_cls: 0.042  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0164  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:36:58 d2.utils.events]: [0meta: 0:31:21  iter: 5959  total_loss: 0.023  loss_cls: 0.023  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0164  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:37:02 d2.utils.events]: [0meta: 0:31:17  iter: 5979  total_loss: 0.026  loss_cls: 0.026  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0165  lr: 0.000300  max_mem: 1825M
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
[32m[12/19 00:37:08 d2.data.common]: [0mSerializing 3618 elements to byte tensors and concatenating them all ...
[32m[12/19 00:37:08 d2.data.common]: [0mSerialized dataset takes 33.05 MiB
[32m[12/19 00:37:08 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 00:37:08 d2.evaluation.evaluator]: [0mStart inference on 3618 images
[32m[12/19 00:37:09 d2.evaluation.evaluator]: [0mInference done 11/3618. 0.0501 s / img. ETA=0:03:05
[32m[12/19 00:37:19 d2.evaluation.evaluator]: [0mInference done 203/3618. 0.0500 s / img. ETA=0:02:58
[32m[12/19 00:37:29 d2.evaluation.evaluator]: [0mInference done 391/3618. 0.0500 s / img. ETA=0:02:50
[32m[12/19 00:37:39 d2.evaluation.evaluator]: [0mInference done 575/3618. 0.0500 s / img. ETA=0:02:42
[32m[12/19 00:37:49 d2.evaluation.evaluator]: [0mInference done 755/3618. 0.0501 s / img. ETA=0:02:34
[32m[12/19 00:37:59 d2.evaluation.evaluator]: [0mInference done 931/3618. 0.0501 s / img. ETA=0:02:26
[32m[12/19 00:38:09 d2.evaluation.evaluator]: [0mInference done 1108/3618. 0.0499 s / img. ETA=0:02:17
[32m[12/19 00:38:19 d2.evaluation.evaluator]: [0mInference done 1282/3618. 0.0497 s / img. ETA=0:02:08
[32m[12/19 00:38:29 d2.evaluation.evaluator]: [0mInference done 1449/3618. 0.0497 s / img. ETA=0:02:01
[32m[12/19 00:38:39 d2.evaluation.evaluator]: [0mInference done 1603/3618. 0.0500 s / img. ETA=0:01:54
[32m[12/19 00:38:49 d2.evaluation.evaluator]: [0mInference done 1758/3618. 0.0501 s / img. ETA=0:01:46
[32m[12/19 00:38:59 d2.evaluation.evaluator]: [0mInference done 1903/3618. 0.0504 s / img. ETA=0:01:40
[32m[12/19 00:39:09 d2.evaluation.evaluator]: [0mInference done 2053/3618. 0.0505 s / img. ETA=0:01:32
[32m[12/19 00:39:19 d2.evaluation.evaluator]: [0mInference done 2206/3618. 0.0505 s / img. ETA=0:01:23
[32m[12/19 00:39:29 d2.evaluation.evaluator]: [0mInference done 2347/3618. 0.0506 s / img. ETA=0:01:16
[32m[12/19 00:39:40 d2.evaluation.evaluator]: [0mInference done 2495/3618. 0.0506 s / img. ETA=0:01:08
[32m[12/19 00:39:50 d2.evaluation.evaluator]: [0mInference done 2643/3618. 0.0505 s / img. ETA=0:00:59
[32m[12/19 00:40:00 d2.evaluation.evaluator]: [0mInference done 2771/3618. 0.0507 s / img. ETA=0:00:52
[32m[12/19 00:40:10 d2.evaluation.evaluator]: [0mInference done 2897/3618. 0.0509 s / img. ETA=0:00:45
[32m[12/19 00:40:20 d2.evaluation.evaluator]: [0mInference done 3021/3618. 0.0510 s / img. ETA=0:00:37
[32m[12/19 00:40:30 d2.evaluation.evaluator]: [0mInference done 3144/3618. 0.0512 s / img. ETA=0:00:30
[32m[12/19 00:40:40 d2.evaluation.evaluator]: [0mInference done 3265/3618. 0.0513 s / img. ETA=0:00:22
[32m[12/19 00:40:50 d2.evaluation.evaluator]: [0mInference done 3382/3618. 0.0515 s / img. ETA=0:00:15
[32m[12/19 00:41:00 d2.evaluation.evaluator]: [0mInference done 3461/3618. 0.0522 s / img. ETA=0:00:10
[32m[12/19 00:41:10 d2.evaluation.evaluator]: [0mInference done 3530/3618. 0.0529 s / img. ETA=0:00:06
[32m[12/19 00:41:20 d2.evaluation.evaluator]: [0mInference done 3595/3618. 0.0537 s / img. ETA=0:00:01
[32m[12/19 00:41:27 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:17.898640 (0.071381 s / img per device, on 1 devices)
[32m[12/19 00:41:27 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:03:15 (0.054034 s / img per device, on 1 devices)
Evaluate predictions
We have 2442 single comp cutouts and 1176 multi
Plot predictions
Baseline assumption cat is 67.5% correct
train cat is 90.9% correct
34.68% improvement
[32m[12/19 00:41:30 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 00:41:30 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 00:41:30 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:41:30 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 00:41:30 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:41:30 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 00:41:30 d2.evaluation.evaluator]: [0m4.87%
[32m[12/19 00:41:30 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 00:41:30 d2.evaluation.evaluator]: [0m17.86%
[32m[12/19 00:41:30 d2.evaluation.evaluator]: [0mCatalogue is 0.9090657822001106 correct.
[32m[12/19 00:41:30 d2.engine.defaults]: [0mEvaluation results for train in csv format:
[32m[12/19 00:41:30 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 00:41:30 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 00:41:30 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0487,0.1786,0.9091
len dataset is: 1205 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_val.pkl
[32m[12/19 00:41:30 d2.data.common]: [0mSerializing 1205 elements to byte tensors and concatenating them all ...
[32m[12/19 00:41:30 d2.data.common]: [0mSerialized dataset takes 11.07 MiB
[32m[12/19 00:41:30 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 00:41:30 d2.evaluation.evaluator]: [0mStart inference on 1205 images
[32m[12/19 00:41:30 d2.evaluation.evaluator]: [0mInference done 11/1205. 0.0502 s / img. ETA=0:01:01
[32m[12/19 00:41:40 d2.evaluation.evaluator]: [0mInference done 197/1205. 0.0517 s / img. ETA=0:00:54
[32m[12/19 00:41:50 d2.evaluation.evaluator]: [0mInference done 390/1205. 0.0501 s / img. ETA=0:00:43
[32m[12/19 00:42:01 d2.evaluation.evaluator]: [0mInference done 579/1205. 0.0496 s / img. ETA=0:00:33
[32m[12/19 00:42:11 d2.evaluation.evaluator]: [0mInference done 763/1205. 0.0493 s / img. ETA=0:00:23
[32m[12/19 00:42:21 d2.evaluation.evaluator]: [0mInference done 943/1205. 0.0492 s / img. ETA=0:00:14
[32m[12/19 00:42:31 d2.evaluation.evaluator]: [0mInference done 1117/1205. 0.0492 s / img. ETA=0:00:04
[32m[12/19 00:42:36 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:05.948556 (0.054957 s / img per device, on 1 devices)
[32m[12/19 00:42:36 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:58 (0.049161 s / img per device, on 1 devices)
Evaluate predictions
We have 817 single comp cutouts and 388 multi
Plot predictions
Baseline assumption cat is 67.8% correct
val cat is 89.9% correct
32.56% improvement
[32m[12/19 00:42:37 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 00:42:37 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 00:42:37 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:42:37 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 00:42:37 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:42:37 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 00:42:37 d2.evaluation.evaluator]: [0m5.26%
[32m[12/19 00:42:37 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 00:42:37 d2.evaluation.evaluator]: [0m20.36%
[32m[12/19 00:42:37 d2.evaluation.evaluator]: [0mCatalogue is 0.8987551867219917 correct.
[32m[12/19 00:42:37 d2.engine.defaults]: [0mEvaluation results for val in csv format:
[32m[12/19 00:42:37 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 00:42:37 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 00:42:37 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0526,0.2036,0.8988
len dataset is: 1211 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_test.pkl
[32m[12/19 00:42:37 d2.data.common]: [0mSerializing 1211 elements to byte tensors and concatenating them all ...
[32m[12/19 00:42:37 d2.data.common]: [0mSerialized dataset takes 11.23 MiB
[32m[12/19 00:42:37 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 00:42:37 d2.evaluation.evaluator]: [0mStart inference on 1211 images
[32m[12/19 00:42:41 d2.evaluation.evaluator]: [0mInference done 55/1211. 0.0600 s / img. ETA=0:01:11
[32m[12/19 00:42:51 d2.evaluation.evaluator]: [0mInference done 217/1211. 0.0593 s / img. ETA=0:01:01
[32m[12/19 00:43:01 d2.evaluation.evaluator]: [0mInference done 397/1211. 0.0559 s / img. ETA=0:00:48
[32m[12/19 00:43:11 d2.evaluation.evaluator]: [0mInference done 575/1211. 0.0545 s / img. ETA=0:00:37
[32m[12/19 00:43:21 d2.evaluation.evaluator]: [0mInference done 723/1211. 0.0556 s / img. ETA=0:00:29
[32m[12/19 00:43:31 d2.evaluation.evaluator]: [0mInference done 905/1211. 0.0542 s / img. ETA=0:00:18
[32m[12/19 00:43:41 d2.evaluation.evaluator]: [0mInference done 1083/1211. 0.0533 s / img. ETA=0:00:07
[32m[12/19 00:43:48 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:11.012737 (0.058883 s / img per device, on 1 devices)
[32m[12/19 00:43:48 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:03 (0.052754 s / img per device, on 1 devices)
Evaluate predictions
We have 819 single comp cutouts and 392 multi
Plot predictions
Baseline assumption cat is 67.6% correct
test cat is 89.7% correct
32.60% improvement
[32m[12/19 00:43:49 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 00:43:49 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 00:43:49 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:43:49 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 00:43:49 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:43:49 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 00:43:49 d2.evaluation.evaluator]: [0m6.11%
[32m[12/19 00:43:49 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 00:43:49 d2.evaluation.evaluator]: [0m19.13%
[32m[12/19 00:43:49 d2.evaluation.evaluator]: [0mCatalogue is 0.8967795210569777 correct.
[32m[12/19 00:43:49 d2.engine.defaults]: [0mEvaluation results for test in csv format:
[32m[12/19 00:43:49 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 00:43:49 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 00:43:49 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0611,0.1913,0.8968
[32m[12/19 00:43:49 d2.utils.events]: [0meta: 0:31:12  iter: 5999  total_loss: 0.030  loss_cls: 0.030  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0165  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:43:54 d2.utils.events]: [0meta: 0:31:08  iter: 6019  total_loss: 0.033  loss_cls: 0.033  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0191  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:43:58 d2.utils.events]: [0meta: 0:31:03  iter: 6039  total_loss: 0.027  loss_cls: 0.027  loss_box_reg: 0.000  time: 0.2120  data_time: 0.0165  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:44:02 d2.utils.events]: [0meta: 0:30:58  iter: 6059  total_loss: 0.037  loss_cls: 0.037  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0178  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:44:06 d2.utils.events]: [0meta: 0:30:54  iter: 6079  total_loss: 0.026  loss_cls: 0.026  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0192  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:44:10 d2.utils.events]: [0meta: 0:30:50  iter: 6099  total_loss: 0.050  loss_cls: 0.050  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0163  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:44:15 d2.utils.events]: [0meta: 0:30:46  iter: 6119  total_loss: 0.038  loss_cls: 0.038  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0161  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:44:19 d2.utils.events]: [0meta: 0:30:42  iter: 6139  total_loss: 0.015  loss_cls: 0.015  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0160  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:44:23 d2.utils.events]: [0meta: 0:30:37  iter: 6159  total_loss: 0.046  loss_cls: 0.046  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0160  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:44:27 d2.utils.events]: [0meta: 0:30:33  iter: 6179  total_loss: 0.045  loss_cls: 0.045  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0160  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:44:31 d2.utils.events]: [0meta: 0:30:29  iter: 6199  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0160  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:44:36 d2.utils.events]: [0meta: 0:30:25  iter: 6219  total_loss: 0.047  loss_cls: 0.047  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0167  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:44:40 d2.utils.events]: [0meta: 0:30:21  iter: 6239  total_loss: 0.049  loss_cls: 0.049  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0156  lr: 0.000300  max_mem: 1825M
[32m[12/19 00:44:44 d2.utils.events]: [0meta: 0:30:17  iter: 6259  total_loss: 0.039  loss_cls: 0.039  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:44:48 d2.utils.events]: [0meta: 0:30:12  iter: 6279  total_loss: 0.029  loss_cls: 0.029  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:44:52 d2.utils.events]: [0meta: 0:30:08  iter: 6299  total_loss: 0.025  loss_cls: 0.025  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0154  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:44:57 d2.utils.events]: [0meta: 0:30:03  iter: 6319  total_loss: 0.041  loss_cls: 0.041  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0152  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:45:01 d2.utils.events]: [0meta: 0:29:59  iter: 6339  total_loss: 0.032  loss_cls: 0.032  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0152  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:45:05 d2.utils.events]: [0meta: 0:29:55  iter: 6359  total_loss: 0.027  loss_cls: 0.027  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0154  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:45:09 d2.utils.events]: [0meta: 0:29:50  iter: 6379  total_loss: 0.025  loss_cls: 0.025  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:45:13 d2.utils.events]: [0meta: 0:29:46  iter: 6399  total_loss: 0.030  loss_cls: 0.030  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0194  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:45:18 d2.utils.events]: [0meta: 0:29:42  iter: 6419  total_loss: 0.034  loss_cls: 0.034  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0188  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:45:22 d2.utils.events]: [0meta: 0:29:38  iter: 6439  total_loss: 0.032  loss_cls: 0.032  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0175  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:45:26 d2.utils.events]: [0meta: 0:29:35  iter: 6459  total_loss: 0.045  loss_cls: 0.045  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0194  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:45:32 d2.utils.events]: [0meta: 0:29:31  iter: 6479  total_loss: 0.023  loss_cls: 0.023  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0158  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:45:36 d2.utils.events]: [0meta: 0:29:27  iter: 6499  total_loss: 0.023  loss_cls: 0.023  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0160  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:45:40 d2.utils.events]: [0meta: 0:29:22  iter: 6519  total_loss: 0.029  loss_cls: 0.029  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0154  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:45:45 d2.utils.events]: [0meta: 0:29:19  iter: 6539  total_loss: 0.045  loss_cls: 0.045  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0159  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:45:49 d2.utils.events]: [0meta: 0:29:14  iter: 6559  total_loss: 0.031  loss_cls: 0.031  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0159  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:45:53 d2.utils.events]: [0meta: 0:29:10  iter: 6579  total_loss: 0.017  loss_cls: 0.017  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0197  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:45:57 d2.utils.events]: [0meta: 0:29:06  iter: 6599  total_loss: 0.025  loss_cls: 0.025  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0201  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:46:02 d2.utils.events]: [0meta: 0:29:02  iter: 6619  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0176  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:46:06 d2.utils.events]: [0meta: 0:28:58  iter: 6639  total_loss: 0.027  loss_cls: 0.027  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0186  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:46:10 d2.utils.events]: [0meta: 0:28:54  iter: 6659  total_loss: 0.027  loss_cls: 0.027  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0205  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:46:15 d2.utils.events]: [0meta: 0:28:50  iter: 6679  total_loss: 0.026  loss_cls: 0.026  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0201  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:46:19 d2.utils.events]: [0meta: 0:28:47  iter: 6699  total_loss: 0.022  loss_cls: 0.022  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0201  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:46:23 d2.utils.events]: [0meta: 0:28:44  iter: 6719  total_loss: 0.023  loss_cls: 0.023  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0200  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:46:27 d2.utils.events]: [0meta: 0:28:41  iter: 6739  total_loss: 0.019  loss_cls: 0.019  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0201  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:46:32 d2.utils.events]: [0meta: 0:28:37  iter: 6759  total_loss: 0.027  loss_cls: 0.027  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0194  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:46:36 d2.utils.events]: [0meta: 0:28:34  iter: 6779  total_loss: 0.069  loss_cls: 0.069  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0196  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:46:40 d2.utils.events]: [0meta: 0:28:30  iter: 6799  total_loss: 0.023  loss_cls: 0.023  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0196  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:46:44 d2.utils.events]: [0meta: 0:28:27  iter: 6819  total_loss: 0.024  loss_cls: 0.024  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0197  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:46:49 d2.utils.events]: [0meta: 0:28:23  iter: 6839  total_loss: 0.037  loss_cls: 0.037  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0200  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:46:53 d2.utils.events]: [0meta: 0:28:20  iter: 6859  total_loss: 0.024  loss_cls: 0.024  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0199  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:46:57 d2.utils.events]: [0meta: 0:28:17  iter: 6879  total_loss: 0.019  loss_cls: 0.019  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0199  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:47:01 d2.utils.events]: [0meta: 0:28:13  iter: 6899  total_loss: 0.017  loss_cls: 0.017  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0199  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:47:06 d2.utils.events]: [0meta: 0:28:09  iter: 6919  total_loss: 0.015  loss_cls: 0.015  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0201  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:47:10 d2.utils.events]: [0meta: 0:28:06  iter: 6939  total_loss: 0.033  loss_cls: 0.033  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0201  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:47:14 d2.utils.events]: [0meta: 0:28:03  iter: 6959  total_loss: 0.021  loss_cls: 0.021  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0201  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:47:18 d2.utils.events]: [0meta: 0:28:00  iter: 6979  total_loss: 0.017  loss_cls: 0.017  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0202  lr: 0.000300  max_mem: 1829M
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
[32m[12/19 00:47:24 d2.data.common]: [0mSerializing 3618 elements to byte tensors and concatenating them all ...
[32m[12/19 00:47:24 d2.data.common]: [0mSerialized dataset takes 33.05 MiB
[32m[12/19 00:47:24 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 00:47:24 d2.evaluation.evaluator]: [0mStart inference on 3618 images
[32m[12/19 00:47:25 d2.evaluation.evaluator]: [0mInference done 11/3618. 0.0501 s / img. ETA=0:03:05
[32m[12/19 00:47:35 d2.evaluation.evaluator]: [0mInference done 204/3618. 0.0499 s / img. ETA=0:02:57
[32m[12/19 00:47:45 d2.evaluation.evaluator]: [0mInference done 395/3618. 0.0496 s / img. ETA=0:02:48
[32m[12/19 00:47:55 d2.evaluation.evaluator]: [0mInference done 551/3618. 0.0523 s / img. ETA=0:02:51
[32m[12/19 00:48:05 d2.evaluation.evaluator]: [0mInference done 684/3618. 0.0552 s / img. ETA=0:02:55
[32m[12/19 00:48:15 d2.evaluation.evaluator]: [0mInference done 838/3618. 0.0557 s / img. ETA=0:02:48
[32m[12/19 00:48:25 d2.evaluation.evaluator]: [0mInference done 1018/3618. 0.0544 s / img. ETA=0:02:35
[32m[12/19 00:48:35 d2.evaluation.evaluator]: [0mInference done 1195/3618. 0.0535 s / img. ETA=0:02:23
[32m[12/19 00:48:45 d2.evaluation.evaluator]: [0mInference done 1368/3618. 0.0529 s / img. ETA=0:02:13
[32m[12/19 00:48:55 d2.evaluation.evaluator]: [0mInference done 1538/3618. 0.0524 s / img. ETA=0:02:02
[32m[12/19 00:49:05 d2.evaluation.evaluator]: [0mInference done 1705/3618. 0.0520 s / img. ETA=0:01:53
[32m[12/19 00:49:15 d2.evaluation.evaluator]: [0mInference done 1869/3618. 0.0517 s / img. ETA=0:01:43
[32m[12/19 00:49:25 d2.evaluation.evaluator]: [0mInference done 2030/3618. 0.0515 s / img. ETA=0:01:34
[32m[12/19 00:49:35 d2.evaluation.evaluator]: [0mInference done 2188/3618. 0.0513 s / img. ETA=0:01:25
[32m[12/19 00:49:45 d2.evaluation.evaluator]: [0mInference done 2344/3618. 0.0511 s / img. ETA=0:01:16
[32m[12/19 00:49:55 d2.evaluation.evaluator]: [0mInference done 2498/3618. 0.0509 s / img. ETA=0:01:07
[32m[12/19 00:50:05 d2.evaluation.evaluator]: [0mInference done 2649/3618. 0.0508 s / img. ETA=0:00:58
[32m[12/19 00:50:15 d2.evaluation.evaluator]: [0mInference done 2797/3618. 0.0507 s / img. ETA=0:00:50
[32m[12/19 00:50:26 d2.evaluation.evaluator]: [0mInference done 2943/3618. 0.0506 s / img. ETA=0:00:41
[32m[12/19 00:50:36 d2.evaluation.evaluator]: [0mInference done 3088/3618. 0.0505 s / img. ETA=0:00:32
[32m[12/19 00:50:46 d2.evaluation.evaluator]: [0mInference done 3228/3618. 0.0504 s / img. ETA=0:00:24
[32m[12/19 00:50:56 d2.evaluation.evaluator]: [0mInference done 3307/3618. 0.0511 s / img. ETA=0:00:19
[32m[12/19 00:51:06 d2.evaluation.evaluator]: [0mInference done 3391/3618. 0.0517 s / img. ETA=0:00:14
[32m[12/19 00:51:16 d2.evaluation.evaluator]: [0mInference done 3452/3618. 0.0527 s / img. ETA=0:00:11
[32m[12/19 00:51:26 d2.evaluation.evaluator]: [0mInference done 3535/3618. 0.0532 s / img. ETA=0:00:05
[32m[12/19 00:51:36 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:11.457837 (0.069598 s / img per device, on 1 devices)
[32m[12/19 00:51:36 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:03:13 (0.053570 s / img per device, on 1 devices)
Evaluate predictions
We have 2442 single comp cutouts and 1176 multi
Plot predictions
Baseline assumption cat is 67.5% correct
train cat is 92.2% correct
36.53% improvement
[32m[12/19 00:51:39 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 00:51:39 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 00:51:39 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:51:39 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 00:51:39 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:51:39 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 00:51:39 d2.evaluation.evaluator]: [0m4.26%
[32m[12/19 00:51:39 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 00:51:39 d2.evaluation.evaluator]: [0m15.31%
[32m[12/19 00:51:39 d2.evaluation.evaluator]: [0mCatalogue is 0.9215035931453842 correct.
[32m[12/19 00:51:39 d2.engine.defaults]: [0mEvaluation results for train in csv format:
[32m[12/19 00:51:39 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 00:51:39 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 00:51:39 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0426,0.1531,0.9215
len dataset is: 1205 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_val.pkl
[32m[12/19 00:51:39 d2.data.common]: [0mSerializing 1205 elements to byte tensors and concatenating them all ...
[32m[12/19 00:51:39 d2.data.common]: [0mSerialized dataset takes 11.07 MiB
[32m[12/19 00:51:39 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 00:51:39 d2.evaluation.evaluator]: [0mStart inference on 1205 images
[32m[12/19 00:51:40 d2.evaluation.evaluator]: [0mInference done 11/1205. 0.0513 s / img. ETA=0:01:02
[32m[12/19 00:51:50 d2.evaluation.evaluator]: [0mInference done 198/1205. 0.0513 s / img. ETA=0:00:53
[32m[12/19 00:52:00 d2.evaluation.evaluator]: [0mInference done 382/1205. 0.0513 s / img. ETA=0:00:44
[32m[12/19 00:52:10 d2.evaluation.evaluator]: [0mInference done 562/1205. 0.0512 s / img. ETA=0:00:35
[32m[12/19 00:52:20 d2.evaluation.evaluator]: [0mInference done 738/1205. 0.0512 s / img. ETA=0:00:25
[32m[12/19 00:52:30 d2.evaluation.evaluator]: [0mInference done 911/1205. 0.0512 s / img. ETA=0:00:16
[32m[12/19 00:52:40 d2.evaluation.evaluator]: [0mInference done 1080/1205. 0.0512 s / img. ETA=0:00:07
[32m[12/19 00:52:48 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:08.255416 (0.056880 s / img per device, on 1 devices)
[32m[12/19 00:52:48 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:01 (0.051048 s / img per device, on 1 devices)
Evaluate predictions
We have 817 single comp cutouts and 388 multi
Plot predictions
Baseline assumption cat is 67.8% correct
val cat is 89.5% correct
31.95% improvement
[32m[12/19 00:52:48 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 00:52:48 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 00:52:48 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:52:48 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 00:52:48 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:52:48 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 00:52:48 d2.evaluation.evaluator]: [0m5.75%
[32m[12/19 00:52:48 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 00:52:48 d2.evaluation.evaluator]: [0m20.62%
[32m[12/19 00:52:48 d2.evaluation.evaluator]: [0mCatalogue is 0.8946058091286307 correct.
[32m[12/19 00:52:48 d2.engine.defaults]: [0mEvaluation results for val in csv format:
[32m[12/19 00:52:48 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 00:52:48 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 00:52:48 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0575,0.2062,0.8946
len dataset is: 1211 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_test.pkl
[32m[12/19 00:52:49 d2.data.common]: [0mSerializing 1211 elements to byte tensors and concatenating them all ...
[32m[12/19 00:52:49 d2.data.common]: [0mSerialized dataset takes 11.23 MiB
[32m[12/19 00:52:49 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 00:52:49 d2.evaluation.evaluator]: [0mStart inference on 1211 images
[32m[12/19 00:52:50 d2.evaluation.evaluator]: [0mInference done 21/1211. 0.0500 s / img. ETA=0:01:01
[32m[12/19 00:53:00 d2.evaluation.evaluator]: [0mInference done 213/1211. 0.0500 s / img. ETA=0:00:52
[32m[12/19 00:53:10 d2.evaluation.evaluator]: [0mInference done 378/1211. 0.0530 s / img. ETA=0:00:46
[32m[12/19 00:53:20 d2.evaluation.evaluator]: [0mInference done 536/1211. 0.0545 s / img. ETA=0:00:39
[32m[12/19 00:53:30 d2.evaluation.evaluator]: [0mInference done 677/1211. 0.0564 s / img. ETA=0:00:32
[32m[12/19 00:53:40 d2.evaluation.evaluator]: [0mInference done 826/1211. 0.0569 s / img. ETA=0:00:23
[32m[12/19 00:53:50 d2.evaluation.evaluator]: [0mInference done 984/1211. 0.0566 s / img. ETA=0:00:14
[32m[12/19 00:54:00 d2.evaluation.evaluator]: [0mInference done 1140/1211. 0.0564 s / img. ETA=0:00:04
[32m[12/19 00:54:06 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:16.676110 (0.063579 s / img per device, on 1 devices)
[32m[12/19 00:54:06 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:08 (0.056493 s / img per device, on 1 devices)
Evaluate predictions
We have 819 single comp cutouts and 392 multi
Plot predictions
Baseline assumption cat is 67.6% correct
test cat is 90.3% correct
33.58% improvement
[32m[12/19 00:54:07 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 00:54:07 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 00:54:07 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:54:07 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 00:54:07 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 00:54:07 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 00:54:07 d2.evaluation.evaluator]: [0m5.13%
[32m[12/19 00:54:07 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 00:54:07 d2.evaluation.evaluator]: [0m19.13%
[32m[12/19 00:54:07 d2.evaluation.evaluator]: [0mCatalogue is 0.9033856317093312 correct.
[32m[12/19 00:54:07 d2.engine.defaults]: [0mEvaluation results for test in csv format:
[32m[12/19 00:54:07 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 00:54:07 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 00:54:07 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0513,0.1913,0.9034
[32m[12/19 00:54:07 d2.utils.events]: [0meta: 0:27:56  iter: 6999  total_loss: 0.030  loss_cls: 0.030  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0201  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:54:11 d2.utils.events]: [0meta: 0:27:52  iter: 7019  total_loss: 0.028  loss_cls: 0.028  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0158  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:54:15 d2.utils.events]: [0meta: 0:27:48  iter: 7039  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0169  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:54:19 d2.utils.events]: [0meta: 0:27:44  iter: 7059  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0188  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:54:24 d2.utils.events]: [0meta: 0:27:40  iter: 7079  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0174  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:54:28 d2.utils.events]: [0meta: 0:27:36  iter: 7099  total_loss: 0.024  loss_cls: 0.024  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0177  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:54:32 d2.utils.events]: [0meta: 0:27:32  iter: 7119  total_loss: 0.041  loss_cls: 0.041  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0204  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:54:36 d2.utils.events]: [0meta: 0:27:28  iter: 7139  total_loss: 0.028  loss_cls: 0.028  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0160  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:54:40 d2.utils.events]: [0meta: 0:27:23  iter: 7159  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0159  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:54:45 d2.utils.events]: [0meta: 0:27:19  iter: 7179  total_loss: 0.033  loss_cls: 0.033  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0159  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:54:49 d2.utils.events]: [0meta: 0:27:16  iter: 7199  total_loss: 0.033  loss_cls: 0.033  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0182  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:54:53 d2.utils.events]: [0meta: 0:27:12  iter: 7219  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0175  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:54:58 d2.utils.events]: [0meta: 0:27:08  iter: 7239  total_loss: 0.027  loss_cls: 0.027  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0167  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:55:02 d2.utils.events]: [0meta: 0:27:04  iter: 7259  total_loss: 0.015  loss_cls: 0.015  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0168  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:55:06 d2.utils.events]: [0meta: 0:27:00  iter: 7279  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0161  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:55:10 d2.utils.events]: [0meta: 0:26:56  iter: 7299  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0160  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:55:15 d2.utils.events]: [0meta: 0:26:52  iter: 7319  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0163  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:55:19 d2.utils.events]: [0meta: 0:26:48  iter: 7339  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0161  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:55:23 d2.utils.events]: [0meta: 0:26:43  iter: 7359  total_loss: 0.019  loss_cls: 0.019  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0161  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:55:27 d2.utils.events]: [0meta: 0:26:39  iter: 7379  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0166  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:55:31 d2.utils.events]: [0meta: 0:26:35  iter: 7399  total_loss: 0.035  loss_cls: 0.035  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0171  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:55:36 d2.utils.events]: [0meta: 0:26:30  iter: 7419  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0161  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:55:40 d2.utils.events]: [0meta: 0:26:26  iter: 7439  total_loss: 0.026  loss_cls: 0.026  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0167  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:55:44 d2.utils.events]: [0meta: 0:26:22  iter: 7459  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0187  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:55:49 d2.utils.events]: [0meta: 0:26:18  iter: 7479  total_loss: 0.044  loss_cls: 0.044  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0174  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:55:53 d2.utils.events]: [0meta: 0:26:14  iter: 7499  total_loss: 0.021  loss_cls: 0.021  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0160  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:55:57 d2.utils.events]: [0meta: 0:26:10  iter: 7519  total_loss: 0.021  loss_cls: 0.021  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0161  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:56:01 d2.utils.events]: [0meta: 0:26:05  iter: 7539  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0157  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:56:05 d2.utils.events]: [0meta: 0:26:01  iter: 7559  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:56:10 d2.utils.events]: [0meta: 0:25:56  iter: 7579  total_loss: 0.019  loss_cls: 0.019  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0161  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:56:14 d2.utils.events]: [0meta: 0:25:52  iter: 7599  total_loss: 0.029  loss_cls: 0.029  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0160  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:56:18 d2.utils.events]: [0meta: 0:25:47  iter: 7619  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0157  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:56:22 d2.utils.events]: [0meta: 0:25:43  iter: 7639  total_loss: 0.027  loss_cls: 0.027  loss_box_reg: 0.000  time: 0.2117  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:56:27 d2.utils.events]: [0meta: 0:25:39  iter: 7659  total_loss: 0.015  loss_cls: 0.015  loss_box_reg: 0.000  time: 0.2118  data_time: 0.0169  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:56:33 d2.utils.events]: [0meta: 0:25:35  iter: 7679  total_loss: 0.023  loss_cls: 0.023  loss_box_reg: 0.000  time: 0.2119  data_time: 0.0171  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:56:39 d2.utils.events]: [0meta: 0:25:31  iter: 7699  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2121  data_time: 0.0194  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:56:44 d2.utils.events]: [0meta: 0:25:27  iter: 7719  total_loss: 0.030  loss_cls: 0.030  loss_box_reg: 0.000  time: 0.2123  data_time: 0.0197  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:56:49 d2.utils.events]: [0meta: 0:25:23  iter: 7739  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0179  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:56:54 d2.utils.events]: [0meta: 0:25:19  iter: 7759  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0183  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:56:58 d2.utils.events]: [0meta: 0:25:15  iter: 7779  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0179  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:57:03 d2.utils.events]: [0meta: 0:25:11  iter: 7799  total_loss: 0.020  loss_cls: 0.020  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0165  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:57:07 d2.utils.events]: [0meta: 0:25:07  iter: 7819  total_loss: 0.019  loss_cls: 0.019  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0170  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:57:12 d2.utils.events]: [0meta: 0:25:01  iter: 7839  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0162  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:57:16 d2.utils.events]: [0meta: 0:24:58  iter: 7859  total_loss: 0.026  loss_cls: 0.026  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0171  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:57:20 d2.utils.events]: [0meta: 0:24:53  iter: 7879  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0162  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:57:24 d2.utils.events]: [0meta: 0:24:47  iter: 7899  total_loss: 0.017  loss_cls: 0.017  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0163  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:57:29 d2.utils.events]: [0meta: 0:24:43  iter: 7919  total_loss: 0.025  loss_cls: 0.025  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0166  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:57:33 d2.utils.events]: [0meta: 0:24:37  iter: 7939  total_loss: 0.020  loss_cls: 0.020  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0162  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:57:37 d2.utils.events]: [0meta: 0:24:32  iter: 7959  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0162  lr: 0.000300  max_mem: 1829M
[32m[12/19 00:57:41 d2.utils.events]: [0meta: 0:24:27  iter: 7979  total_loss: 0.032  loss_cls: 0.032  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0175  lr: 0.000300  max_mem: 1829M
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
[32m[12/19 00:57:47 d2.data.common]: [0mSerializing 3618 elements to byte tensors and concatenating them all ...
[32m[12/19 00:57:47 d2.data.common]: [0mSerialized dataset takes 33.05 MiB
[32m[12/19 00:57:47 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 00:57:47 d2.evaluation.evaluator]: [0mStart inference on 3618 images
[32m[12/19 00:57:48 d2.evaluation.evaluator]: [0mInference done 11/3618. 0.0501 s / img. ETA=0:03:05
[32m[12/19 00:57:58 d2.evaluation.evaluator]: [0mInference done 204/3618. 0.0498 s / img. ETA=0:02:57
[32m[12/19 00:58:08 d2.evaluation.evaluator]: [0mInference done 397/3618. 0.0492 s / img. ETA=0:02:47
[32m[12/19 00:58:18 d2.evaluation.evaluator]: [0mInference done 570/3618. 0.0503 s / img. ETA=0:02:43
[32m[12/19 00:58:28 d2.evaluation.evaluator]: [0mInference done 754/3618. 0.0499 s / img. ETA=0:02:34
[32m[12/19 00:58:38 d2.evaluation.evaluator]: [0mInference done 934/3618. 0.0497 s / img. ETA=0:02:25
[32m[12/19 00:58:48 d2.evaluation.evaluator]: [0mInference done 1111/3618. 0.0496 s / img. ETA=0:02:17
[32m[12/19 00:58:58 d2.evaluation.evaluator]: [0mInference done 1285/3618. 0.0494 s / img. ETA=0:02:08
[32m[12/19 00:59:08 d2.evaluation.evaluator]: [0mInference done 1456/3618. 0.0493 s / img. ETA=0:01:59
[32m[12/19 00:59:18 d2.evaluation.evaluator]: [0mInference done 1624/3618. 0.0493 s / img. ETA=0:01:51
[32m[12/19 00:59:28 d2.evaluation.evaluator]: [0mInference done 1769/3618. 0.0497 s / img. ETA=0:01:45
[32m[12/19 00:59:38 d2.evaluation.evaluator]: [0mInference done 1911/3618. 0.0501 s / img. ETA=0:01:39
[32m[12/19 00:59:48 d2.evaluation.evaluator]: [0mInference done 2052/3618. 0.0504 s / img. ETA=0:01:32
[32m[12/19 00:59:58 d2.evaluation.evaluator]: [0mInference done 2190/3618. 0.0506 s / img. ETA=0:01:25
[32m[12/19 01:00:08 d2.evaluation.evaluator]: [0mInference done 2327/3618. 0.0508 s / img. ETA=0:01:18
[32m[12/19 01:00:18 d2.evaluation.evaluator]: [0mInference done 2477/3618. 0.0507 s / img. ETA=0:01:09
[32m[12/19 01:00:28 d2.evaluation.evaluator]: [0mInference done 2625/3618. 0.0507 s / img. ETA=0:01:00
[32m[12/19 01:00:38 d2.evaluation.evaluator]: [0mInference done 2741/3618. 0.0511 s / img. ETA=0:00:54
[32m[12/19 01:00:48 d2.evaluation.evaluator]: [0mInference done 2808/3618. 0.0522 s / img. ETA=0:00:52
[32m[12/19 01:00:58 d2.evaluation.evaluator]: [0mInference done 2895/3618. 0.0529 s / img. ETA=0:00:47
[32m[12/19 01:01:08 d2.evaluation.evaluator]: [0mInference done 3032/3618. 0.0528 s / img. ETA=0:00:38
[32m[12/19 01:01:19 d2.evaluation.evaluator]: [0mInference done 3155/3618. 0.0529 s / img. ETA=0:00:31
[32m[12/19 01:01:29 d2.evaluation.evaluator]: [0mInference done 3280/3618. 0.0529 s / img. ETA=0:00:22
[32m[12/19 01:01:39 d2.evaluation.evaluator]: [0mInference done 3418/3618. 0.0528 s / img. ETA=0:00:13
[32m[12/19 01:01:49 d2.evaluation.evaluator]: [0mInference done 3554/3618. 0.0526 s / img. ETA=0:00:04
[32m[12/19 01:01:54 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:06.880522 (0.068331 s / img per device, on 1 devices)
[32m[12/19 01:01:54 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:03:09 (0.052532 s / img per device, on 1 devices)
Evaluate predictions
We have 2442 single comp cutouts and 1176 multi
Plot predictions
Baseline assumption cat is 67.5% correct
train cat is 89.6% correct
32.76% improvement
[32m[12/19 01:01:57 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 01:01:57 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 01:01:57 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:01:57 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 01:01:57 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:01:57 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 01:01:57 d2.evaluation.evaluator]: [0m5.86%
[32m[12/19 01:01:57 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 01:01:57 d2.evaluation.evaluator]: [0m19.81%
[32m[12/19 01:01:57 d2.evaluation.evaluator]: [0mCatalogue is 0.8960751796572692 correct.
[32m[12/19 01:01:57 d2.engine.defaults]: [0mEvaluation results for train in csv format:
[32m[12/19 01:01:57 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 01:01:57 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 01:01:57 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0586,0.1981,0.8961
len dataset is: 1205 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_val.pkl
[32m[12/19 01:01:57 d2.data.common]: [0mSerializing 1205 elements to byte tensors and concatenating them all ...
[32m[12/19 01:01:57 d2.data.common]: [0mSerialized dataset takes 11.07 MiB
[32m[12/19 01:01:57 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 01:01:57 d2.evaluation.evaluator]: [0mStart inference on 1205 images
[32m[12/19 01:01:59 d2.evaluation.evaluator]: [0mInference done 27/1205. 0.0499 s / img. ETA=0:01:00
[32m[12/19 01:02:09 d2.evaluation.evaluator]: [0mInference done 220/1205. 0.0496 s / img. ETA=0:00:51
[32m[12/19 01:02:19 d2.evaluation.evaluator]: [0mInference done 412/1205. 0.0491 s / img. ETA=0:00:41
[32m[12/19 01:02:29 d2.evaluation.evaluator]: [0mInference done 577/1205. 0.0509 s / img. ETA=0:00:34
[32m[12/19 01:02:39 d2.evaluation.evaluator]: [0mInference done 757/1205. 0.0506 s / img. ETA=0:00:24
[32m[12/19 01:02:49 d2.evaluation.evaluator]: [0mInference done 931/1205. 0.0505 s / img. ETA=0:00:15
[32m[12/19 01:02:59 d2.evaluation.evaluator]: [0mInference done 1041/1205. 0.0534 s / img. ETA=0:00:09
[32m[12/19 01:03:09 d2.evaluation.evaluator]: [0mInference done 1187/1205. 0.0539 s / img. ETA=0:00:01
[32m[12/19 01:03:10 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:12.854097 (0.060712 s / img per device, on 1 devices)
[32m[12/19 01:03:10 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:04 (0.053834 s / img per device, on 1 devices)
Evaluate predictions
We have 817 single comp cutouts and 388 multi
Plot predictions
Baseline assumption cat is 67.8% correct
val cat is 88.1% correct
29.99% improvement
[32m[12/19 01:03:11 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 01:03:11 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 01:03:11 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:03:11 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 01:03:11 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:03:11 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 01:03:11 d2.evaluation.evaluator]: [0m6.49%
[32m[12/19 01:03:11 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 01:03:11 d2.evaluation.evaluator]: [0m23.20%
[32m[12/19 01:03:11 d2.evaluation.evaluator]: [0mCatalogue is 0.8813278008298755 correct.
[32m[12/19 01:03:11 d2.engine.defaults]: [0mEvaluation results for val in csv format:
[32m[12/19 01:03:11 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 01:03:11 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 01:03:11 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0649,0.2320,0.8813
len dataset is: 1211 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_test.pkl
[32m[12/19 01:03:11 d2.data.common]: [0mSerializing 1211 elements to byte tensors and concatenating them all ...
[32m[12/19 01:03:11 d2.data.common]: [0mSerialized dataset takes 11.23 MiB
[32m[12/19 01:03:11 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 01:03:11 d2.evaluation.evaluator]: [0mStart inference on 1211 images
[32m[12/19 01:03:19 d2.evaluation.evaluator]: [0mInference done 126/1211. 0.0567 s / img. ETA=0:01:03
[32m[12/19 01:03:29 d2.evaluation.evaluator]: [0mInference done 290/1211. 0.0574 s / img. ETA=0:00:55
[32m[12/19 01:03:39 d2.evaluation.evaluator]: [0mInference done 455/1211. 0.0569 s / img. ETA=0:00:45
[32m[12/19 01:03:49 d2.evaluation.evaluator]: [0mInference done 616/1211. 0.0568 s / img. ETA=0:00:36
[32m[12/19 01:03:59 d2.evaluation.evaluator]: [0mInference done 775/1211. 0.0567 s / img. ETA=0:00:26
[32m[12/19 01:04:09 d2.evaluation.evaluator]: [0mInference done 946/1211. 0.0556 s / img. ETA=0:00:16
[32m[12/19 01:04:19 d2.evaluation.evaluator]: [0mInference done 1115/1211. 0.0549 s / img. ETA=0:00:05
[32m[12/19 01:04:25 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:13.253523 (0.060741 s / img per device, on 1 devices)
[32m[12/19 01:04:25 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:05 (0.054395 s / img per device, on 1 devices)
Evaluate predictions
We have 819 single comp cutouts and 392 multi
Plot predictions
Baseline assumption cat is 67.6% correct
test cat is 87.8% correct
29.79% improvement
[32m[12/19 01:04:26 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 01:04:26 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 01:04:26 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:04:26 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 01:04:26 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:04:26 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 01:04:26 d2.evaluation.evaluator]: [0m6.96%
[32m[12/19 01:04:26 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 01:04:26 d2.evaluation.evaluator]: [0m23.21%
[32m[12/19 01:04:26 d2.evaluation.evaluator]: [0mCatalogue is 0.8777869529314616 correct.
[32m[12/19 01:04:26 d2.engine.defaults]: [0mEvaluation results for test in csv format:
[32m[12/19 01:04:26 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 01:04:26 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 01:04:26 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0696,0.2321,0.8778
[32m[12/19 01:04:26 d2.utils.events]: [0meta: 0:24:22  iter: 7999  total_loss: 0.034  loss_cls: 0.034  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0169  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:04:30 d2.utils.events]: [0meta: 0:24:18  iter: 8019  total_loss: 0.021  loss_cls: 0.021  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0181  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:04:34 d2.utils.events]: [0meta: 0:24:15  iter: 8039  total_loss: 0.025  loss_cls: 0.025  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0183  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:04:39 d2.utils.events]: [0meta: 0:24:11  iter: 8059  total_loss: 0.035  loss_cls: 0.035  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0174  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:04:43 d2.utils.events]: [0meta: 0:24:05  iter: 8079  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0151  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:04:47 d2.utils.events]: [0meta: 0:24:01  iter: 8099  total_loss: 0.034  loss_cls: 0.034  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0151  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:04:51 d2.utils.events]: [0meta: 0:23:56  iter: 8119  total_loss: 0.046  loss_cls: 0.046  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0153  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:04:55 d2.utils.events]: [0meta: 0:23:52  iter: 8139  total_loss: 0.017  loss_cls: 0.017  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0185  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:05:00 d2.utils.events]: [0meta: 0:23:48  iter: 8159  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0172  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:05:04 d2.utils.events]: [0meta: 0:23:45  iter: 8179  total_loss: 0.017  loss_cls: 0.017  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0185  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:05:08 d2.utils.events]: [0meta: 0:23:41  iter: 8199  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0157  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:05:13 d2.utils.events]: [0meta: 0:23:36  iter: 8219  total_loss: 0.027  loss_cls: 0.027  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0171  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:05:17 d2.utils.events]: [0meta: 0:23:32  iter: 8239  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0198  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:05:21 d2.utils.events]: [0meta: 0:23:27  iter: 8259  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0197  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:05:25 d2.utils.events]: [0meta: 0:23:24  iter: 8279  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0216  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:05:30 d2.utils.events]: [0meta: 0:23:21  iter: 8299  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0216  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:05:34 d2.utils.events]: [0meta: 0:23:18  iter: 8319  total_loss: 0.017  loss_cls: 0.017  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0199  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:05:38 d2.utils.events]: [0meta: 0:23:15  iter: 8339  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0207  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:05:42 d2.utils.events]: [0meta: 0:23:11  iter: 8359  total_loss: 0.026  loss_cls: 0.026  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0168  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:05:47 d2.utils.events]: [0meta: 0:23:08  iter: 8379  total_loss: 0.015  loss_cls: 0.015  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0197  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:05:51 d2.utils.events]: [0meta: 0:23:04  iter: 8399  total_loss: 0.017  loss_cls: 0.017  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0183  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:05:55 d2.utils.events]: [0meta: 0:23:01  iter: 8419  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0196  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:05:59 d2.utils.events]: [0meta: 0:22:56  iter: 8439  total_loss: 0.020  loss_cls: 0.020  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0177  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:06:04 d2.utils.events]: [0meta: 0:22:51  iter: 8459  total_loss: 0.019  loss_cls: 0.019  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0191  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:06:08 d2.utils.events]: [0meta: 0:22:47  iter: 8479  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0204  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:06:12 d2.utils.events]: [0meta: 0:22:43  iter: 8499  total_loss: 0.017  loss_cls: 0.017  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0202  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:06:16 d2.utils.events]: [0meta: 0:22:39  iter: 8519  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0178  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:06:21 d2.utils.events]: [0meta: 0:22:36  iter: 8539  total_loss: 0.021  loss_cls: 0.021  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0193  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:06:25 d2.utils.events]: [0meta: 0:22:32  iter: 8559  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0192  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:06:30 d2.utils.events]: [0meta: 0:22:28  iter: 8579  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0209  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:06:34 d2.utils.events]: [0meta: 0:22:25  iter: 8599  total_loss: 0.020  loss_cls: 0.020  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0201  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:06:38 d2.utils.events]: [0meta: 0:22:21  iter: 8619  total_loss: 0.036  loss_cls: 0.036  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0199  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:06:43 d2.utils.events]: [0meta: 0:22:17  iter: 8639  total_loss: 0.015  loss_cls: 0.015  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0191  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:06:47 d2.utils.events]: [0meta: 0:22:12  iter: 8659  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0206  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:06:51 d2.utils.events]: [0meta: 0:22:08  iter: 8679  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0207  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:06:56 d2.utils.events]: [0meta: 0:22:03  iter: 8699  total_loss: 0.039  loss_cls: 0.039  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0204  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:07:00 d2.utils.events]: [0meta: 0:21:59  iter: 8719  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0219  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:07:04 d2.utils.events]: [0meta: 0:21:55  iter: 8739  total_loss: 0.030  loss_cls: 0.030  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0216  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:07:08 d2.utils.events]: [0meta: 0:21:51  iter: 8759  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0223  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:07:13 d2.utils.events]: [0meta: 0:21:47  iter: 8779  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0171  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:07:17 d2.utils.events]: [0meta: 0:21:42  iter: 8799  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0169  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:07:21 d2.utils.events]: [0meta: 0:21:38  iter: 8819  total_loss: 0.022  loss_cls: 0.022  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0172  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:07:25 d2.utils.events]: [0meta: 0:21:34  iter: 8839  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0193  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:07:30 d2.utils.events]: [0meta: 0:21:30  iter: 8859  total_loss: 0.019  loss_cls: 0.019  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0204  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:07:35 d2.utils.events]: [0meta: 0:21:26  iter: 8879  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0187  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:07:42 d2.utils.events]: [0meta: 0:21:22  iter: 8899  total_loss: 0.024  loss_cls: 0.024  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0189  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:07:46 d2.utils.events]: [0meta: 0:21:18  iter: 8919  total_loss: 0.017  loss_cls: 0.017  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0202  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:07:51 d2.utils.events]: [0meta: 0:21:14  iter: 8939  total_loss: 0.030  loss_cls: 0.030  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0196  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:07:55 d2.utils.events]: [0meta: 0:21:10  iter: 8959  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0205  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:07:59 d2.utils.events]: [0meta: 0:21:07  iter: 8979  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0208  lr: 0.000300  max_mem: 1829M
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
[32m[12/19 01:08:05 d2.data.common]: [0mSerializing 3618 elements to byte tensors and concatenating them all ...
[32m[12/19 01:08:05 d2.data.common]: [0mSerialized dataset takes 33.05 MiB
[32m[12/19 01:08:05 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 01:08:05 d2.evaluation.evaluator]: [0mStart inference on 3618 images
[32m[12/19 01:08:06 d2.evaluation.evaluator]: [0mInference done 11/3618. 0.0501 s / img. ETA=0:03:05
[32m[12/19 01:08:16 d2.evaluation.evaluator]: [0mInference done 201/3618. 0.0506 s / img. ETA=0:03:00
[32m[12/19 01:08:26 d2.evaluation.evaluator]: [0mInference done 389/3618. 0.0503 s / img. ETA=0:02:51
[32m[12/19 01:08:36 d2.evaluation.evaluator]: [0mInference done 573/3618. 0.0502 s / img. ETA=0:02:42
[32m[12/19 01:08:46 d2.evaluation.evaluator]: [0mInference done 754/3618. 0.0501 s / img. ETA=0:02:34
[32m[12/19 01:08:56 d2.evaluation.evaluator]: [0mInference done 931/3618. 0.0501 s / img. ETA=0:02:26
[32m[12/19 01:09:06 d2.evaluation.evaluator]: [0mInference done 1105/3618. 0.0501 s / img. ETA=0:02:18
[32m[12/19 01:09:16 d2.evaluation.evaluator]: [0mInference done 1276/3618. 0.0500 s / img. ETA=0:02:09
[32m[12/19 01:09:26 d2.evaluation.evaluator]: [0mInference done 1444/3618. 0.0500 s / img. ETA=0:02:01
[32m[12/19 01:09:36 d2.evaluation.evaluator]: [0mInference done 1579/3618. 0.0509 s / img. ETA=0:01:57
[32m[12/19 01:09:46 d2.evaluation.evaluator]: [0mInference done 1705/3618. 0.0517 s / img. ETA=0:01:53
[32m[12/19 01:09:56 d2.evaluation.evaluator]: [0mInference done 1852/3618. 0.0519 s / img. ETA=0:01:45
[32m[12/19 01:10:06 d2.evaluation.evaluator]: [0mInference done 1969/3618. 0.0527 s / img. ETA=0:01:41
[32m[12/19 01:10:16 d2.evaluation.evaluator]: [0mInference done 2085/3618. 0.0534 s / img. ETA=0:01:36
[32m[12/19 01:10:26 d2.evaluation.evaluator]: [0mInference done 2199/3618. 0.0540 s / img. ETA=0:01:31
[32m[12/19 01:10:36 d2.evaluation.evaluator]: [0mInference done 2312/3618. 0.0545 s / img. ETA=0:01:25
[32m[12/19 01:10:46 d2.evaluation.evaluator]: [0mInference done 2423/3618. 0.0550 s / img. ETA=0:01:19
[32m[12/19 01:10:56 d2.evaluation.evaluator]: [0mInference done 2554/3618. 0.0550 s / img. ETA=0:01:11
[32m[12/19 01:11:06 d2.evaluation.evaluator]: [0mInference done 2700/3618. 0.0547 s / img. ETA=0:01:01
[32m[12/19 01:11:16 d2.evaluation.evaluator]: [0mInference done 2843/3618. 0.0545 s / img. ETA=0:00:52
[32m[12/19 01:11:26 d2.evaluation.evaluator]: [0mInference done 2985/3618. 0.0543 s / img. ETA=0:00:42
[32m[12/19 01:11:37 d2.evaluation.evaluator]: [0mInference done 3125/3618. 0.0541 s / img. ETA=0:00:33
[32m[12/19 01:11:47 d2.evaluation.evaluator]: [0mInference done 3263/3618. 0.0539 s / img. ETA=0:00:24
[32m[12/19 01:11:57 d2.evaluation.evaluator]: [0mInference done 3399/3618. 0.0538 s / img. ETA=0:00:14
[32m[12/19 01:12:07 d2.evaluation.evaluator]: [0mInference done 3534/3618. 0.0536 s / img. ETA=0:00:05
[32m[12/19 01:12:14 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:08.649990 (0.068821 s / img per device, on 1 devices)
[32m[12/19 01:12:14 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:03:13 (0.053528 s / img per device, on 1 devices)
Evaluate predictions
We have 2442 single comp cutouts and 1176 multi
Plot predictions
Baseline assumption cat is 67.5% correct
train cat is 91.2% correct
35.18% improvement
[32m[12/19 01:12:17 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 01:12:17 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 01:12:17 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:12:17 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 01:12:17 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:12:17 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 01:12:17 d2.evaluation.evaluator]: [0m4.05%
[32m[12/19 01:12:17 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 01:12:17 d2.evaluation.evaluator]: [0m18.54%
[32m[12/19 01:12:17 d2.evaluation.evaluator]: [0mCatalogue is 0.9123825317855169 correct.
[32m[12/19 01:12:17 d2.engine.defaults]: [0mEvaluation results for train in csv format:
[32m[12/19 01:12:17 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 01:12:17 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 01:12:17 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0405,0.1854,0.9124
len dataset is: 1205 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_val.pkl
[32m[12/19 01:12:17 d2.data.common]: [0mSerializing 1205 elements to byte tensors and concatenating them all ...
[32m[12/19 01:12:17 d2.data.common]: [0mSerialized dataset takes 11.07 MiB
[32m[12/19 01:12:17 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 01:12:17 d2.evaluation.evaluator]: [0mStart inference on 1205 images
[32m[12/19 01:12:18 d2.evaluation.evaluator]: [0mInference done 11/1205. 0.0500 s / img. ETA=0:01:01
[32m[12/19 01:12:28 d2.evaluation.evaluator]: [0mInference done 203/1205. 0.0499 s / img. ETA=0:00:52
[32m[12/19 01:12:38 d2.evaluation.evaluator]: [0mInference done 391/1205. 0.0499 s / img. ETA=0:00:42
[32m[12/19 01:12:48 d2.evaluation.evaluator]: [0mInference done 575/1205. 0.0499 s / img. ETA=0:00:33
[32m[12/19 01:12:58 d2.evaluation.evaluator]: [0mInference done 755/1205. 0.0499 s / img. ETA=0:00:24
[32m[12/19 01:13:08 d2.evaluation.evaluator]: [0mInference done 932/1205. 0.0499 s / img. ETA=0:00:14
[32m[12/19 01:13:18 d2.evaluation.evaluator]: [0mInference done 1105/1205. 0.0499 s / img. ETA=0:00:05
[32m[12/19 01:13:24 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:06.715385 (0.055596 s / img per device, on 1 devices)
[32m[12/19 01:13:24 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:59 (0.049932 s / img per device, on 1 devices)
Evaluate predictions
We have 817 single comp cutouts and 388 multi
Plot predictions
Baseline assumption cat is 67.8% correct
val cat is 89.4% correct
31.82% improvement
[32m[12/19 01:13:25 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 01:13:25 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 01:13:25 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:13:25 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 01:13:25 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:13:25 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 01:13:25 d2.evaluation.evaluator]: [0m5.75%
[32m[12/19 01:13:25 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 01:13:25 d2.evaluation.evaluator]: [0m20.88%
[32m[12/19 01:13:25 d2.evaluation.evaluator]: [0mCatalogue is 0.8937759336099586 correct.
[32m[12/19 01:13:25 d2.engine.defaults]: [0mEvaluation results for val in csv format:
[32m[12/19 01:13:25 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 01:13:25 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 01:13:25 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0575,0.2088,0.8938
len dataset is: 1211 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_test.pkl
[32m[12/19 01:13:25 d2.data.common]: [0mSerializing 1211 elements to byte tensors and concatenating them all ...
[32m[12/19 01:13:25 d2.data.common]: [0mSerialized dataset takes 11.23 MiB
[32m[12/19 01:13:25 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 01:13:25 d2.evaluation.evaluator]: [0mStart inference on 1211 images
[32m[12/19 01:13:28 d2.evaluation.evaluator]: [0mInference done 50/1211. 0.0499 s / img. ETA=0:00:59
[32m[12/19 01:13:38 d2.evaluation.evaluator]: [0mInference done 239/1211. 0.0504 s / img. ETA=0:00:51
[32m[12/19 01:13:48 d2.evaluation.evaluator]: [0mInference done 427/1211. 0.0502 s / img. ETA=0:00:41
[32m[12/19 01:13:58 d2.evaluation.evaluator]: [0mInference done 568/1211. 0.0537 s / img. ETA=0:00:36
[32m[12/19 01:14:08 d2.evaluation.evaluator]: [0mInference done 718/1211. 0.0550 s / img. ETA=0:00:29
[32m[12/19 01:14:18 d2.evaluation.evaluator]: [0mInference done 896/1211. 0.0539 s / img. ETA=0:00:18
[32m[12/19 01:14:28 d2.evaluation.evaluator]: [0mInference done 1070/1211. 0.0533 s / img. ETA=0:00:08
[32m[12/19 01:14:36 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:11.083095 (0.058941 s / img per device, on 1 devices)
[32m[12/19 01:14:36 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:03 (0.052851 s / img per device, on 1 devices)
Evaluate predictions
We have 819 single comp cutouts and 392 multi
Plot predictions
Baseline assumption cat is 67.6% correct
test cat is 90.1% correct
33.21% improvement
[32m[12/19 01:14:38 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 01:14:38 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 01:14:38 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:14:38 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 01:14:38 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:14:38 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 01:14:38 d2.evaluation.evaluator]: [0m5.13%
[32m[12/19 01:14:38 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 01:14:38 d2.evaluation.evaluator]: [0m19.90%
[32m[12/19 01:14:38 d2.evaluation.evaluator]: [0mCatalogue is 0.9009083402146986 correct.
[32m[12/19 01:14:38 d2.engine.defaults]: [0mEvaluation results for test in csv format:
[32m[12/19 01:14:38 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 01:14:38 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 01:14:38 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0513,0.1990,0.9009
[32m[12/19 01:14:38 d2.utils.events]: [0meta: 0:21:03  iter: 8999  total_loss: 0.021  loss_cls: 0.021  loss_box_reg: 0.000  time: 0.2124  data_time: 0.0192  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:14:43 d2.utils.events]: [0meta: 0:20:59  iter: 9019  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2125  data_time: 0.0222  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:14:48 d2.utils.events]: [0meta: 0:20:55  iter: 9039  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2126  data_time: 0.0218  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:14:53 d2.utils.events]: [0meta: 0:20:51  iter: 9059  total_loss: 0.015  loss_cls: 0.015  loss_box_reg: 0.000  time: 0.2126  data_time: 0.0206  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:14:57 d2.utils.events]: [0meta: 0:20:47  iter: 9079  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2126  data_time: 0.0205  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:15:01 d2.utils.events]: [0meta: 0:20:43  iter: 9099  total_loss: 0.027  loss_cls: 0.027  loss_box_reg: 0.000  time: 0.2126  data_time: 0.0207  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:15:06 d2.utils.events]: [0meta: 0:20:39  iter: 9119  total_loss: 0.023  loss_cls: 0.023  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0225  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:15:10 d2.utils.events]: [0meta: 0:20:36  iter: 9139  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0204  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:15:15 d2.utils.events]: [0meta: 0:20:31  iter: 9159  total_loss: 0.023  loss_cls: 0.023  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0204  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:15:19 d2.utils.events]: [0meta: 0:20:27  iter: 9179  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0203  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:15:23 d2.utils.events]: [0meta: 0:20:23  iter: 9199  total_loss: 0.004  loss_cls: 0.004  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0204  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:15:27 d2.utils.events]: [0meta: 0:20:19  iter: 9219  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0204  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:15:32 d2.utils.events]: [0meta: 0:20:15  iter: 9239  total_loss: 0.010  loss_cls: 0.010  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0204  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:15:36 d2.utils.events]: [0meta: 0:20:11  iter: 9259  total_loss: 0.026  loss_cls: 0.026  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0204  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:15:41 d2.utils.events]: [0meta: 0:20:06  iter: 9279  total_loss: 0.025  loss_cls: 0.025  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0212  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:15:45 d2.utils.events]: [0meta: 0:20:02  iter: 9299  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0178  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:15:49 d2.utils.events]: [0meta: 0:19:58  iter: 9319  total_loss: 0.029  loss_cls: 0.029  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0171  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:15:54 d2.utils.events]: [0meta: 0:19:54  iter: 9339  total_loss: 0.027  loss_cls: 0.027  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0196  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:15:58 d2.utils.events]: [0meta: 0:19:50  iter: 9359  total_loss: 0.070  loss_cls: 0.070  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0203  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:16:02 d2.utils.events]: [0meta: 0:19:45  iter: 9379  total_loss: 0.024  loss_cls: 0.024  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0197  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:16:06 d2.utils.events]: [0meta: 0:19:41  iter: 9399  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0210  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:16:11 d2.utils.events]: [0meta: 0:19:37  iter: 9419  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0200  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:16:15 d2.utils.events]: [0meta: 0:19:33  iter: 9439  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0203  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:16:19 d2.utils.events]: [0meta: 0:19:29  iter: 9459  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0215  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:16:23 d2.utils.events]: [0meta: 0:19:25  iter: 9479  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0209  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:16:28 d2.utils.events]: [0meta: 0:19:21  iter: 9499  total_loss: 0.023  loss_cls: 0.023  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0210  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:16:32 d2.utils.events]: [0meta: 0:19:17  iter: 9519  total_loss: 0.010  loss_cls: 0.010  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0206  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:16:36 d2.utils.events]: [0meta: 0:19:13  iter: 9539  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0208  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:16:41 d2.utils.events]: [0meta: 0:19:09  iter: 9559  total_loss: 0.026  loss_cls: 0.026  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0207  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:16:45 d2.utils.events]: [0meta: 0:19:04  iter: 9579  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0206  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:16:49 d2.utils.events]: [0meta: 0:19:00  iter: 9599  total_loss: 0.034  loss_cls: 0.034  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0203  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:16:53 d2.utils.events]: [0meta: 0:18:56  iter: 9619  total_loss: 0.017  loss_cls: 0.017  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0170  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:16:58 d2.utils.events]: [0meta: 0:18:51  iter: 9639  total_loss: 0.006  loss_cls: 0.006  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0169  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:17:02 d2.utils.events]: [0meta: 0:18:47  iter: 9659  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0197  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:17:06 d2.utils.events]: [0meta: 0:18:43  iter: 9679  total_loss: 0.031  loss_cls: 0.031  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0203  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:17:10 d2.utils.events]: [0meta: 0:18:38  iter: 9699  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0204  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:17:15 d2.utils.events]: [0meta: 0:18:34  iter: 9719  total_loss: 0.019  loss_cls: 0.019  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0205  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:17:19 d2.utils.events]: [0meta: 0:18:30  iter: 9739  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0204  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:17:23 d2.utils.events]: [0meta: 0:18:25  iter: 9759  total_loss: 0.030  loss_cls: 0.030  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0205  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:17:27 d2.utils.events]: [0meta: 0:18:21  iter: 9779  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0204  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:17:32 d2.utils.events]: [0meta: 0:18:17  iter: 9799  total_loss: 0.026  loss_cls: 0.026  loss_box_reg: 0.000  time: 0.2127  data_time: 0.0213  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:17:37 d2.utils.events]: [0meta: 0:18:14  iter: 9819  total_loss: 0.017  loss_cls: 0.017  loss_box_reg: 0.000  time: 0.2128  data_time: 0.0225  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:17:42 d2.utils.events]: [0meta: 0:18:10  iter: 9839  total_loss: 0.020  loss_cls: 0.020  loss_box_reg: 0.000  time: 0.2129  data_time: 0.0209  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:17:47 d2.utils.events]: [0meta: 0:18:06  iter: 9859  total_loss: 0.025  loss_cls: 0.025  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0205  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:17:52 d2.utils.events]: [0meta: 0:18:02  iter: 9879  total_loss: 0.026  loss_cls: 0.026  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0195  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:17:56 d2.utils.events]: [0meta: 0:17:57  iter: 9899  total_loss: 0.025  loss_cls: 0.025  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0202  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:18:00 d2.utils.events]: [0meta: 0:17:53  iter: 9919  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0205  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:18:05 d2.utils.events]: [0meta: 0:17:49  iter: 9939  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0212  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:18:09 d2.utils.events]: [0meta: 0:17:45  iter: 9959  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0205  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:18:14 d2.utils.events]: [0meta: 0:17:41  iter: 9979  total_loss: 0.017  loss_cls: 0.017  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0211  lr: 0.000300  max_mem: 1829M
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
[32m[12/19 01:18:20 d2.data.common]: [0mSerializing 3618 elements to byte tensors and concatenating them all ...
[32m[12/19 01:18:20 d2.data.common]: [0mSerialized dataset takes 33.05 MiB
[32m[12/19 01:18:20 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 01:18:20 d2.evaluation.evaluator]: [0mStart inference on 3618 images
[32m[12/19 01:18:21 d2.evaluation.evaluator]: [0mInference done 11/3618. 0.0502 s / img. ETA=0:03:05
[32m[12/19 01:18:31 d2.evaluation.evaluator]: [0mInference done 145/3618. 0.0712 s / img. ETA=0:04:17
[32m[12/19 01:18:41 d2.evaluation.evaluator]: [0mInference done 273/3618. 0.0724 s / img. ETA=0:04:14
[32m[12/19 01:18:51 d2.evaluation.evaluator]: [0mInference done 387/3618. 0.0753 s / img. ETA=0:04:17
[32m[12/19 01:19:01 d2.evaluation.evaluator]: [0mInference done 548/3618. 0.0698 s / img. ETA=0:03:48
[32m[12/19 01:19:11 d2.evaluation.evaluator]: [0mInference done 728/3618. 0.0649 s / img. ETA=0:03:21
[32m[12/19 01:19:21 d2.evaluation.evaluator]: [0mInference done 905/3618. 0.0620 s / img. ETA=0:03:02
[32m[12/19 01:19:31 d2.evaluation.evaluator]: [0mInference done 1070/3618. 0.0605 s / img. ETA=0:02:48
[32m[12/19 01:19:41 d2.evaluation.evaluator]: [0mInference done 1245/3618. 0.0588 s / img. ETA=0:02:34
[32m[12/19 01:19:51 d2.evaluation.evaluator]: [0mInference done 1417/3618. 0.0576 s / img. ETA=0:02:21
[32m[12/19 01:20:01 d2.evaluation.evaluator]: [0mInference done 1585/3618. 0.0566 s / img. ETA=0:02:09
[32m[12/19 01:20:11 d2.evaluation.evaluator]: [0mInference done 1750/3618. 0.0559 s / img. ETA=0:01:58
[32m[12/19 01:20:21 d2.evaluation.evaluator]: [0mInference done 1912/3618. 0.0553 s / img. ETA=0:01:47
[32m[12/19 01:20:31 d2.evaluation.evaluator]: [0mInference done 2072/3618. 0.0548 s / img. ETA=0:01:37
[32m[12/19 01:20:41 d2.evaluation.evaluator]: [0mInference done 2229/3618. 0.0543 s / img. ETA=0:01:27
[32m[12/19 01:20:51 d2.evaluation.evaluator]: [0mInference done 2362/3618. 0.0544 s / img. ETA=0:01:20
[32m[12/19 01:21:01 d2.evaluation.evaluator]: [0mInference done 2487/3618. 0.0546 s / img. ETA=0:01:13
[32m[12/19 01:21:11 d2.evaluation.evaluator]: [0mInference done 2614/3618. 0.0547 s / img. ETA=0:01:05
[32m[12/19 01:21:22 d2.evaluation.evaluator]: [0mInference done 2761/3618. 0.0544 s / img. ETA=0:00:56
[32m[12/19 01:21:32 d2.evaluation.evaluator]: [0mInference done 2885/3618. 0.0544 s / img. ETA=0:00:48
[32m[12/19 01:21:42 d2.evaluation.evaluator]: [0mInference done 2986/3618. 0.0548 s / img. ETA=0:00:42
[32m[12/19 01:21:52 d2.evaluation.evaluator]: [0mInference done 3115/3618. 0.0547 s / img. ETA=0:00:34
[32m[12/19 01:22:02 d2.evaluation.evaluator]: [0mInference done 3220/3618. 0.0550 s / img. ETA=0:00:27
[32m[12/19 01:22:12 d2.evaluation.evaluator]: [0mInference done 3351/3618. 0.0548 s / img. ETA=0:00:18
[32m[12/19 01:22:22 d2.evaluation.evaluator]: [0mInference done 3447/3618. 0.0552 s / img. ETA=0:00:11
[32m[12/19 01:22:32 d2.evaluation.evaluator]: [0mInference done 3583/3618. 0.0549 s / img. ETA=0:00:02
[32m[12/19 01:22:36 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:15.125892 (0.070613 s / img per device, on 1 devices)
[32m[12/19 01:22:36 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:03:18 (0.054852 s / img per device, on 1 devices)
Evaluate predictions
We have 2442 single comp cutouts and 1176 multi
Plot predictions
Baseline assumption cat is 67.5% correct
train cat is 90.8% correct
34.52% improvement
[32m[12/19 01:22:38 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 01:22:38 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 01:22:38 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:22:38 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 01:22:38 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:22:38 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 01:22:38 d2.evaluation.evaluator]: [0m4.14%
[32m[12/19 01:22:38 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 01:22:38 d2.evaluation.evaluator]: [0m19.73%
[32m[12/19 01:22:38 d2.evaluation.evaluator]: [0mCatalogue is 0.9079601990049752 correct.
[32m[12/19 01:22:38 d2.engine.defaults]: [0mEvaluation results for train in csv format:
[32m[12/19 01:22:38 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 01:22:38 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 01:22:38 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0414,0.1973,0.9080
len dataset is: 1205 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_val.pkl
[32m[12/19 01:22:38 d2.data.common]: [0mSerializing 1205 elements to byte tensors and concatenating them all ...
[32m[12/19 01:22:38 d2.data.common]: [0mSerialized dataset takes 11.07 MiB
[32m[12/19 01:22:38 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 01:22:38 d2.evaluation.evaluator]: [0mStart inference on 1205 images
[32m[12/19 01:22:42 d2.evaluation.evaluator]: [0mInference done 68/1205. 0.0499 s / img. ETA=0:00:58
[32m[12/19 01:22:52 d2.evaluation.evaluator]: [0mInference done 259/1205. 0.0499 s / img. ETA=0:00:49
[32m[12/19 01:23:02 d2.evaluation.evaluator]: [0mInference done 428/1205. 0.0520 s / img. ETA=0:00:42
[32m[12/19 01:23:12 d2.evaluation.evaluator]: [0mInference done 586/1205. 0.0534 s / img. ETA=0:00:35
[32m[12/19 01:23:22 d2.evaluation.evaluator]: [0mInference done 768/1205. 0.0525 s / img. ETA=0:00:24
[32m[12/19 01:23:32 d2.evaluation.evaluator]: [0mInference done 948/1205. 0.0517 s / img. ETA=0:00:14
[32m[12/19 01:23:42 d2.evaluation.evaluator]: [0mInference done 1119/1205. 0.0514 s / img. ETA=0:00:04
[32m[12/19 01:23:47 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:08.807203 (0.057339 s / img per device, on 1 devices)
[32m[12/19 01:23:47 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:01 (0.051235 s / img per device, on 1 devices)
Evaluate predictions
We have 817 single comp cutouts and 388 multi
Plot predictions
Baseline assumption cat is 67.8% correct
val cat is 88.8% correct
30.97% improvement
[32m[12/19 01:23:48 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 01:23:48 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 01:23:48 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:23:48 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 01:23:48 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:23:48 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 01:23:48 d2.evaluation.evaluator]: [0m6.00%
[32m[12/19 01:23:48 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 01:23:48 d2.evaluation.evaluator]: [0m22.16%
[32m[12/19 01:23:48 d2.evaluation.evaluator]: [0mCatalogue is 0.8879668049792531 correct.
[32m[12/19 01:23:48 d2.engine.defaults]: [0mEvaluation results for val in csv format:
[32m[12/19 01:23:48 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 01:23:48 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 01:23:48 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0600,0.2216,0.8880
len dataset is: 1211 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_test.pkl
[32m[12/19 01:23:48 d2.data.common]: [0mSerializing 1211 elements to byte tensors and concatenating them all ...
[32m[12/19 01:23:49 d2.data.common]: [0mSerialized dataset takes 11.23 MiB
[32m[12/19 01:23:49 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 01:23:49 d2.evaluation.evaluator]: [0mStart inference on 1211 images
[32m[12/19 01:23:52 d2.evaluation.evaluator]: [0mInference done 69/1211. 0.0499 s / img. ETA=0:00:58
[32m[12/19 01:24:02 d2.evaluation.evaluator]: [0mInference done 232/1211. 0.0561 s / img. ETA=0:00:57
[32m[12/19 01:24:12 d2.evaluation.evaluator]: [0mInference done 391/1211. 0.0572 s / img. ETA=0:00:49
[32m[12/19 01:24:22 d2.evaluation.evaluator]: [0mInference done 558/1211. 0.0565 s / img. ETA=0:00:39
[32m[12/19 01:24:32 d2.evaluation.evaluator]: [0mInference done 743/1211. 0.0545 s / img. ETA=0:00:27
[32m[12/19 01:24:42 d2.evaluation.evaluator]: [0mInference done 925/1211. 0.0533 s / img. ETA=0:00:16
[32m[12/19 01:24:52 d2.evaluation.evaluator]: [0mInference done 1103/1211. 0.0525 s / img. ETA=0:00:06
[32m[12/19 01:24:59 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:10.175625 (0.058189 s / img per device, on 1 devices)
[32m[12/19 01:24:59 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:02 (0.052177 s / img per device, on 1 devices)
Evaluate predictions
We have 819 single comp cutouts and 392 multi
Plot predictions
Baseline assumption cat is 67.6% correct
test cat is 88.5% correct
30.89% improvement
[32m[12/19 01:25:00 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 01:25:00 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 01:25:00 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:25:00 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 01:25:00 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:25:00 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 01:25:00 d2.evaluation.evaluator]: [0m5.62%
[32m[12/19 01:25:00 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 01:25:00 d2.evaluation.evaluator]: [0m23.72%
[32m[12/19 01:25:00 d2.evaluation.evaluator]: [0mCatalogue is 0.8852188274153592 correct.
[32m[12/19 01:25:00 d2.engine.defaults]: [0mEvaluation results for test in csv format:
[32m[12/19 01:25:00 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 01:25:00 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 01:25:00 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0562,0.2372,0.8852
[32m[12/19 01:25:00 d2.utils.events]: [0meta: 0:17:37  iter: 9999  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0209  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:25:04 d2.utils.events]: [0meta: 0:17:32  iter: 10019  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0211  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:25:09 d2.utils.events]: [0meta: 0:17:28  iter: 10039  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0209  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:25:13 d2.utils.events]: [0meta: 0:17:24  iter: 10059  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0206  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:25:17 d2.utils.events]: [0meta: 0:17:19  iter: 10079  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0210  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:25:21 d2.utils.events]: [0meta: 0:17:15  iter: 10099  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0214  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:25:26 d2.utils.events]: [0meta: 0:17:11  iter: 10119  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0211  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:25:30 d2.utils.events]: [0meta: 0:17:07  iter: 10139  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0211  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:25:34 d2.utils.events]: [0meta: 0:17:02  iter: 10159  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0206  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:25:38 d2.utils.events]: [0meta: 0:16:58  iter: 10179  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0195  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:25:43 d2.utils.events]: [0meta: 0:16:54  iter: 10199  total_loss: 0.006  loss_cls: 0.006  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0170  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:25:47 d2.utils.events]: [0meta: 0:16:49  iter: 10219  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0173  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:25:51 d2.utils.events]: [0meta: 0:16:45  iter: 10239  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0205  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:25:55 d2.utils.events]: [0meta: 0:16:41  iter: 10259  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0210  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:26:00 d2.utils.events]: [0meta: 0:16:37  iter: 10279  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0214  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:26:04 d2.utils.events]: [0meta: 0:16:33  iter: 10299  total_loss: 0.022  loss_cls: 0.022  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0206  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:26:09 d2.utils.events]: [0meta: 0:16:29  iter: 10319  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0202  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:26:13 d2.utils.events]: [0meta: 0:16:24  iter: 10339  total_loss: 0.003  loss_cls: 0.003  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0194  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:26:17 d2.utils.events]: [0meta: 0:16:20  iter: 10359  total_loss: 0.015  loss_cls: 0.015  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0183  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:26:21 d2.utils.events]: [0meta: 0:16:16  iter: 10379  total_loss: 0.015  loss_cls: 0.015  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0177  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:26:26 d2.utils.events]: [0meta: 0:16:12  iter: 10399  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0216  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:26:31 d2.utils.events]: [0meta: 0:16:08  iter: 10419  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0178  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:26:35 d2.utils.events]: [0meta: 0:16:03  iter: 10439  total_loss: 0.010  loss_cls: 0.010  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0161  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:26:39 d2.utils.events]: [0meta: 0:15:59  iter: 10459  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0164  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:26:43 d2.utils.events]: [0meta: 0:15:54  iter: 10479  total_loss: 0.010  loss_cls: 0.010  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0161  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:26:48 d2.utils.events]: [0meta: 0:15:50  iter: 10499  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:26:52 d2.utils.events]: [0meta: 0:15:46  iter: 10519  total_loss: 0.019  loss_cls: 0.019  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0167  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:26:56 d2.utils.events]: [0meta: 0:15:41  iter: 10539  total_loss: 0.021  loss_cls: 0.021  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:27:00 d2.utils.events]: [0meta: 0:15:37  iter: 10559  total_loss: 0.022  loss_cls: 0.022  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:27:05 d2.utils.events]: [0meta: 0:15:32  iter: 10579  total_loss: 0.030  loss_cls: 0.030  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:27:09 d2.utils.events]: [0meta: 0:15:28  iter: 10599  total_loss: 0.021  loss_cls: 0.021  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0168  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:27:13 d2.utils.events]: [0meta: 0:15:24  iter: 10619  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0162  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:27:17 d2.utils.events]: [0meta: 0:15:20  iter: 10639  total_loss: 0.026  loss_cls: 0.026  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0161  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:27:22 d2.utils.events]: [0meta: 0:15:15  iter: 10659  total_loss: 0.023  loss_cls: 0.023  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0160  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:27:26 d2.utils.events]: [0meta: 0:15:11  iter: 10679  total_loss: 0.017  loss_cls: 0.017  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0161  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:27:30 d2.utils.events]: [0meta: 0:15:07  iter: 10699  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0159  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:27:34 d2.utils.events]: [0meta: 0:15:02  iter: 10719  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0161  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:27:39 d2.utils.events]: [0meta: 0:14:58  iter: 10739  total_loss: 0.021  loss_cls: 0.021  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:27:43 d2.utils.events]: [0meta: 0:14:53  iter: 10759  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:27:47 d2.utils.events]: [0meta: 0:14:48  iter: 10779  total_loss: 0.006  loss_cls: 0.006  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:27:51 d2.utils.events]: [0meta: 0:14:43  iter: 10799  total_loss: 0.010  loss_cls: 0.010  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:27:55 d2.utils.events]: [0meta: 0:14:39  iter: 10819  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:27:59 d2.utils.events]: [0meta: 0:14:34  iter: 10839  total_loss: 0.019  loss_cls: 0.019  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:28:04 d2.utils.events]: [0meta: 0:14:29  iter: 10859  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:28:08 d2.utils.events]: [0meta: 0:14:24  iter: 10879  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:28:12 d2.utils.events]: [0meta: 0:14:19  iter: 10899  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:28:16 d2.utils.events]: [0meta: 0:14:15  iter: 10919  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0189  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:28:21 d2.utils.events]: [0meta: 0:14:10  iter: 10939  total_loss: 0.010  loss_cls: 0.010  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0197  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:28:25 d2.utils.events]: [0meta: 0:14:06  iter: 10959  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0196  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:28:29 d2.utils.events]: [0meta: 0:14:01  iter: 10979  total_loss: 0.023  loss_cls: 0.023  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0197  lr: 0.000300  max_mem: 1829M
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
[32m[12/19 01:28:35 d2.data.common]: [0mSerializing 3618 elements to byte tensors and concatenating them all ...
[32m[12/19 01:28:35 d2.data.common]: [0mSerialized dataset takes 33.05 MiB
[32m[12/19 01:28:35 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 01:28:35 d2.evaluation.evaluator]: [0mStart inference on 3618 images
[32m[12/19 01:28:36 d2.evaluation.evaluator]: [0mInference done 11/3618. 0.0502 s / img. ETA=0:03:05
[32m[12/19 01:28:46 d2.evaluation.evaluator]: [0mInference done 203/3618. 0.0501 s / img. ETA=0:02:58
[32m[12/19 01:28:56 d2.evaluation.evaluator]: [0mInference done 392/3618. 0.0499 s / img. ETA=0:02:49
[32m[12/19 01:29:06 d2.evaluation.evaluator]: [0mInference done 580/3618. 0.0496 s / img. ETA=0:02:40
[32m[12/19 01:29:16 d2.evaluation.evaluator]: [0mInference done 757/3618. 0.0498 s / img. ETA=0:02:33
[32m[12/19 01:29:26 d2.evaluation.evaluator]: [0mInference done 930/3618. 0.0500 s / img. ETA=0:02:26
[32m[12/19 01:29:36 d2.evaluation.evaluator]: [0mInference done 1103/3618. 0.0500 s / img. ETA=0:02:18
[32m[12/19 01:29:46 d2.evaluation.evaluator]: [0mInference done 1277/3618. 0.0498 s / img. ETA=0:02:09
[32m[12/19 01:29:56 d2.evaluation.evaluator]: [0mInference done 1448/3618. 0.0496 s / img. ETA=0:02:01
[32m[12/19 01:30:06 d2.evaluation.evaluator]: [0mInference done 1616/3618. 0.0495 s / img. ETA=0:01:52
[32m[12/19 01:30:16 d2.evaluation.evaluator]: [0mInference done 1780/3618. 0.0495 s / img. ETA=0:01:44
[32m[12/19 01:30:26 d2.evaluation.evaluator]: [0mInference done 1941/3618. 0.0494 s / img. ETA=0:01:35
[32m[12/19 01:30:36 d2.evaluation.evaluator]: [0mInference done 2100/3618. 0.0493 s / img. ETA=0:01:27
[32m[12/19 01:30:46 d2.evaluation.evaluator]: [0mInference done 2256/3618. 0.0493 s / img. ETA=0:01:19
[32m[12/19 01:30:56 d2.evaluation.evaluator]: [0mInference done 2410/3618. 0.0493 s / img. ETA=0:01:10
[32m[12/19 01:31:06 d2.evaluation.evaluator]: [0mInference done 2561/3618. 0.0492 s / img. ETA=0:01:02
[32m[12/19 01:31:16 d2.evaluation.evaluator]: [0mInference done 2710/3618. 0.0492 s / img. ETA=0:00:53
[32m[12/19 01:31:27 d2.evaluation.evaluator]: [0mInference done 2856/3618. 0.0492 s / img. ETA=0:00:45
[32m[12/19 01:31:37 d2.evaluation.evaluator]: [0mInference done 2961/3618. 0.0497 s / img. ETA=0:00:40
[32m[12/19 01:31:47 d2.evaluation.evaluator]: [0mInference done 3085/3618. 0.0499 s / img. ETA=0:00:33
[32m[12/19 01:31:57 d2.evaluation.evaluator]: [0mInference done 3178/3618. 0.0505 s / img. ETA=0:00:27
[32m[12/19 01:32:07 d2.evaluation.evaluator]: [0mInference done 3277/3618. 0.0510 s / img. ETA=0:00:22
[32m[12/19 01:32:17 d2.evaluation.evaluator]: [0mInference done 3369/3618. 0.0515 s / img. ETA=0:00:16
[32m[12/19 01:32:27 d2.evaluation.evaluator]: [0mInference done 3443/3618. 0.0522 s / img. ETA=0:00:11
[32m[12/19 01:32:37 d2.evaluation.evaluator]: [0mInference done 3580/3618. 0.0521 s / img. ETA=0:00:02
[32m[12/19 01:32:41 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:05.190637 (0.067863 s / img per device, on 1 devices)
[32m[12/19 01:32:41 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:03:07 (0.052034 s / img per device, on 1 devices)
Evaluate predictions
We have 2442 single comp cutouts and 1176 multi
Plot predictions
Baseline assumption cat is 67.5% correct
train cat is 90.2% correct
33.58% improvement
[32m[12/19 01:32:43 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 01:32:43 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 01:32:43 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:32:43 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 01:32:43 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:32:43 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 01:32:43 d2.evaluation.evaluator]: [0m5.32%
[32m[12/19 01:32:43 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 01:32:43 d2.evaluation.evaluator]: [0m19.22%
[32m[12/19 01:32:43 d2.evaluation.evaluator]: [0mCatalogue is 0.9016030956329464 correct.
[32m[12/19 01:32:43 d2.engine.defaults]: [0mEvaluation results for train in csv format:
[32m[12/19 01:32:43 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 01:32:43 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 01:32:43 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0532,0.1922,0.9016
len dataset is: 1205 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_val.pkl
[32m[12/19 01:32:44 d2.data.common]: [0mSerializing 1205 elements to byte tensors and concatenating them all ...
[32m[12/19 01:32:44 d2.data.common]: [0mSerialized dataset takes 11.07 MiB
[32m[12/19 01:32:44 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 01:32:44 d2.evaluation.evaluator]: [0mStart inference on 1205 images
[32m[12/19 01:32:47 d2.evaluation.evaluator]: [0mInference done 64/1205. 0.0499 s / img. ETA=0:00:58
[32m[12/19 01:32:57 d2.evaluation.evaluator]: [0mInference done 255/1205. 0.0499 s / img. ETA=0:00:49
[32m[12/19 01:33:07 d2.evaluation.evaluator]: [0mInference done 442/1205. 0.0499 s / img. ETA=0:00:40
[32m[12/19 01:33:17 d2.evaluation.evaluator]: [0mInference done 625/1205. 0.0499 s / img. ETA=0:00:31
[32m[12/19 01:33:27 d2.evaluation.evaluator]: [0mInference done 804/1205. 0.0499 s / img. ETA=0:00:21
[32m[12/19 01:33:37 d2.evaluation.evaluator]: [0mInference done 980/1205. 0.0499 s / img. ETA=0:00:12
[32m[12/19 01:33:47 d2.evaluation.evaluator]: [0mInference done 1154/1205. 0.0498 s / img. ETA=0:00:02
[32m[12/19 01:33:51 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:06.636875 (0.055531 s / img per device, on 1 devices)
[32m[12/19 01:33:51 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:59 (0.049767 s / img per device, on 1 devices)
Evaluate predictions
We have 817 single comp cutouts and 388 multi
Plot predictions
Baseline assumption cat is 67.8% correct
val cat is 88.0% correct
29.87% improvement
[32m[12/19 01:33:52 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 01:33:52 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 01:33:52 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:33:52 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 01:33:52 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:33:52 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 01:33:52 d2.evaluation.evaluator]: [0m7.59%
[32m[12/19 01:33:52 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 01:33:52 d2.evaluation.evaluator]: [0m21.13%
[32m[12/19 01:33:52 d2.evaluation.evaluator]: [0mCatalogue is 0.8804979253112033 correct.
[32m[12/19 01:33:52 d2.engine.defaults]: [0mEvaluation results for val in csv format:
[32m[12/19 01:33:52 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 01:33:52 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 01:33:52 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0759,0.2113,0.8805
len dataset is: 1211 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_test.pkl
[32m[12/19 01:33:52 d2.data.common]: [0mSerializing 1211 elements to byte tensors and concatenating them all ...
[32m[12/19 01:33:52 d2.data.common]: [0mSerialized dataset takes 11.23 MiB
[32m[12/19 01:33:52 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 01:33:52 d2.evaluation.evaluator]: [0mStart inference on 1211 images
[32m[12/19 01:33:57 d2.evaluation.evaluator]: [0mInference done 105/1211. 0.0500 s / img. ETA=0:00:57
[32m[12/19 01:34:07 d2.evaluation.evaluator]: [0mInference done 295/1211. 0.0501 s / img. ETA=0:00:48
[32m[12/19 01:34:17 d2.evaluation.evaluator]: [0mInference done 480/1211. 0.0501 s / img. ETA=0:00:38
[32m[12/19 01:34:27 d2.evaluation.evaluator]: [0mInference done 662/1211. 0.0500 s / img. ETA=0:00:29
[32m[12/19 01:34:37 d2.evaluation.evaluator]: [0mInference done 840/1211. 0.0500 s / img. ETA=0:00:20
[32m[12/19 01:34:47 d2.evaluation.evaluator]: [0mInference done 1015/1211. 0.0500 s / img. ETA=0:00:10
[32m[12/19 01:34:57 d2.evaluation.evaluator]: [0mInference done 1187/1211. 0.0499 s / img. ETA=0:00:01
[32m[12/19 01:34:59 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:07.168779 (0.055696 s / img per device, on 1 devices)
[32m[12/19 01:34:59 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:00 (0.049938 s / img per device, on 1 devices)
Evaluate predictions
We have 819 single comp cutouts and 392 multi
Plot predictions
Baseline assumption cat is 67.6% correct
test cat is 88.5% correct
30.89% improvement
[32m[12/19 01:35:00 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 01:35:00 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 01:35:00 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:35:00 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 01:35:00 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:35:00 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 01:35:00 d2.evaluation.evaluator]: [0m6.35%
[32m[12/19 01:35:00 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 01:35:00 d2.evaluation.evaluator]: [0m22.19%
[32m[12/19 01:35:00 d2.evaluation.evaluator]: [0mCatalogue is 0.8852188274153592 correct.
[32m[12/19 01:35:00 d2.engine.defaults]: [0mEvaluation results for test in csv format:
[32m[12/19 01:35:00 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 01:35:00 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 01:35:00 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0635,0.2219,0.8852
[32m[12/19 01:35:00 d2.utils.events]: [0meta: 0:13:57  iter: 10999  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0199  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:35:04 d2.utils.events]: [0meta: 0:13:52  iter: 11019  total_loss: 0.010  loss_cls: 0.010  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0171  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:35:09 d2.utils.events]: [0meta: 0:13:47  iter: 11039  total_loss: 0.004  loss_cls: 0.004  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0163  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:35:13 d2.utils.events]: [0meta: 0:13:43  iter: 11059  total_loss: 0.015  loss_cls: 0.015  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0159  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:35:17 d2.utils.events]: [0meta: 0:13:38  iter: 11079  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2129  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:35:21 d2.utils.events]: [0meta: 0:13:33  iter: 11099  total_loss: 0.006  loss_cls: 0.006  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0169  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:35:26 d2.utils.events]: [0meta: 0:13:29  iter: 11119  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0161  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:35:30 d2.utils.events]: [0meta: 0:13:24  iter: 11139  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0172  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:35:34 d2.utils.events]: [0meta: 0:13:19  iter: 11159  total_loss: 0.003  loss_cls: 0.003  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0173  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:35:38 d2.utils.events]: [0meta: 0:13:14  iter: 11179  total_loss: 0.021  loss_cls: 0.021  loss_box_reg: 0.000  time: 0.2129  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:35:43 d2.utils.events]: [0meta: 0:13:11  iter: 11199  total_loss: 0.010  loss_cls: 0.010  loss_box_reg: 0.000  time: 0.2129  data_time: 0.0177  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:35:47 d2.utils.events]: [0meta: 0:13:07  iter: 11219  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2129  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:35:51 d2.utils.events]: [0meta: 0:13:02  iter: 11239  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2129  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:35:55 d2.utils.events]: [0meta: 0:12:58  iter: 11259  total_loss: 0.003  loss_cls: 0.003  loss_box_reg: 0.000  time: 0.2129  data_time: 0.0157  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:36:00 d2.utils.events]: [0meta: 0:12:54  iter: 11279  total_loss: 0.010  loss_cls: 0.010  loss_box_reg: 0.000  time: 0.2129  data_time: 0.0188  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:36:04 d2.utils.events]: [0meta: 0:12:49  iter: 11299  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2129  data_time: 0.0173  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:36:08 d2.utils.events]: [0meta: 0:12:45  iter: 11319  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2129  data_time: 0.0179  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:36:13 d2.utils.events]: [0meta: 0:12:41  iter: 11339  total_loss: 0.010  loss_cls: 0.010  loss_box_reg: 0.000  time: 0.2129  data_time: 0.0190  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:36:17 d2.utils.events]: [0meta: 0:12:37  iter: 11359  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0208  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:36:22 d2.utils.events]: [0meta: 0:12:33  iter: 11379  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0211  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:36:27 d2.utils.events]: [0meta: 0:12:29  iter: 11399  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0206  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:36:31 d2.utils.events]: [0meta: 0:12:25  iter: 11419  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0201  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:36:35 d2.utils.events]: [0meta: 0:12:21  iter: 11439  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0200  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:36:40 d2.utils.events]: [0meta: 0:12:18  iter: 11459  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0202  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:36:44 d2.utils.events]: [0meta: 0:12:14  iter: 11479  total_loss: 0.036  loss_cls: 0.036  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0208  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:36:48 d2.utils.events]: [0meta: 0:12:11  iter: 11499  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0205  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:36:53 d2.utils.events]: [0meta: 0:12:07  iter: 11519  total_loss: 0.002  loss_cls: 0.002  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0206  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:36:57 d2.utils.events]: [0meta: 0:12:03  iter: 11539  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0201  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:37:02 d2.utils.events]: [0meta: 0:11:59  iter: 11559  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0204  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:37:06 d2.utils.events]: [0meta: 0:11:56  iter: 11579  total_loss: 0.003  loss_cls: 0.003  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0211  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:37:11 d2.utils.events]: [0meta: 0:11:52  iter: 11599  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0217  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:37:15 d2.utils.events]: [0meta: 0:11:49  iter: 11619  total_loss: 0.006  loss_cls: 0.006  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0223  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:37:20 d2.utils.events]: [0meta: 0:11:45  iter: 11639  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0220  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:37:24 d2.utils.events]: [0meta: 0:11:42  iter: 11659  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0207  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:37:29 d2.utils.events]: [0meta: 0:11:37  iter: 11679  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0161  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:37:33 d2.utils.events]: [0meta: 0:11:33  iter: 11699  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0172  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:37:37 d2.utils.events]: [0meta: 0:11:29  iter: 11719  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0164  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:37:41 d2.utils.events]: [0meta: 0:11:25  iter: 11739  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0164  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:37:46 d2.utils.events]: [0meta: 0:11:21  iter: 11759  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0171  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:37:50 d2.utils.events]: [0meta: 0:11:17  iter: 11779  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0160  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:37:54 d2.utils.events]: [0meta: 0:11:13  iter: 11799  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0174  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:37:58 d2.utils.events]: [0meta: 0:11:09  iter: 11819  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0161  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:38:03 d2.utils.events]: [0meta: 0:11:05  iter: 11839  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0177  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:38:07 d2.utils.events]: [0meta: 0:11:01  iter: 11859  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0176  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:38:11 d2.utils.events]: [0meta: 0:10:57  iter: 11879  total_loss: 0.004  loss_cls: 0.004  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0171  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:38:15 d2.utils.events]: [0meta: 0:10:53  iter: 11899  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0172  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:38:20 d2.utils.events]: [0meta: 0:10:49  iter: 11919  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0194  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:38:24 d2.utils.events]: [0meta: 0:10:45  iter: 11939  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0194  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:38:28 d2.utils.events]: [0meta: 0:10:41  iter: 11959  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0189  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:38:32 d2.utils.events]: [0meta: 0:10:37  iter: 11979  total_loss: 0.006  loss_cls: 0.006  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0179  lr: 0.000300  max_mem: 1829M
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
[32m[12/19 01:38:38 d2.data.common]: [0mSerializing 3618 elements to byte tensors and concatenating them all ...
[32m[12/19 01:38:39 d2.data.common]: [0mSerialized dataset takes 33.05 MiB
[32m[12/19 01:38:39 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 01:38:39 d2.evaluation.evaluator]: [0mStart inference on 3618 images
[32m[12/19 01:38:39 d2.evaluation.evaluator]: [0mInference done 11/3618. 0.0502 s / img. ETA=0:03:05
[32m[12/19 01:38:49 d2.evaluation.evaluator]: [0mInference done 199/3618. 0.0511 s / img. ETA=0:03:02
[32m[12/19 01:38:59 d2.evaluation.evaluator]: [0mInference done 370/3618. 0.0529 s / img. ETA=0:03:01
[32m[12/19 01:39:09 d2.evaluation.evaluator]: [0mInference done 559/3618. 0.0514 s / img. ETA=0:02:47
[32m[12/19 01:39:19 d2.evaluation.evaluator]: [0mInference done 744/3618. 0.0507 s / img. ETA=0:02:37
[32m[12/19 01:39:29 d2.evaluation.evaluator]: [0mInference done 925/3618. 0.0503 s / img. ETA=0:02:27
[32m[12/19 01:39:39 d2.evaluation.evaluator]: [0mInference done 1102/3618. 0.0501 s / img. ETA=0:02:18
[32m[12/19 01:39:50 d2.evaluation.evaluator]: [0mInference done 1276/3618. 0.0499 s / img. ETA=0:02:09
[32m[12/19 01:40:00 d2.evaluation.evaluator]: [0mInference done 1447/3618. 0.0497 s / img. ETA=0:02:01
[32m[12/19 01:40:10 d2.evaluation.evaluator]: [0mInference done 1611/3618. 0.0497 s / img. ETA=0:01:53
[32m[12/19 01:40:20 d2.evaluation.evaluator]: [0mInference done 1767/3618. 0.0499 s / img. ETA=0:01:45
[32m[12/19 01:40:30 d2.evaluation.evaluator]: [0mInference done 1907/3618. 0.0503 s / img. ETA=0:01:39
[32m[12/19 01:40:40 d2.evaluation.evaluator]: [0mInference done 2062/3618. 0.0503 s / img. ETA=0:01:31
[32m[12/19 01:40:50 d2.evaluation.evaluator]: [0mInference done 2148/3618. 0.0517 s / img. ETA=0:01:29
[32m[12/19 01:41:00 d2.evaluation.evaluator]: [0mInference done 2268/3618. 0.0522 s / img. ETA=0:01:23
[32m[12/19 01:41:10 d2.evaluation.evaluator]: [0mInference done 2415/3618. 0.0521 s / img. ETA=0:01:15
[32m[12/19 01:41:20 d2.evaluation.evaluator]: [0mInference done 2559/3618. 0.0520 s / img. ETA=0:01:06
[32m[12/19 01:41:30 d2.evaluation.evaluator]: [0mInference done 2707/3618. 0.0518 s / img. ETA=0:00:57
[32m[12/19 01:41:40 d2.evaluation.evaluator]: [0mInference done 2852/3618. 0.0517 s / img. ETA=0:00:48
[32m[12/19 01:41:50 d2.evaluation.evaluator]: [0mInference done 2995/3618. 0.0515 s / img. ETA=0:00:39
[32m[12/19 01:42:00 d2.evaluation.evaluator]: [0mInference done 3136/3618. 0.0514 s / img. ETA=0:00:30
[32m[12/19 01:42:10 d2.evaluation.evaluator]: [0mInference done 3274/3618. 0.0513 s / img. ETA=0:00:22
[32m[12/19 01:42:20 d2.evaluation.evaluator]: [0mInference done 3393/3618. 0.0514 s / img. ETA=0:00:14
[32m[12/19 01:42:30 d2.evaluation.evaluator]: [0mInference done 3511/3618. 0.0515 s / img. ETA=0:00:07
[32m[12/19 01:42:39 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:00.019158 (0.066432 s / img per device, on 1 devices)
[32m[12/19 01:42:39 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:03:05 (0.051425 s / img per device, on 1 devices)
Evaluate predictions
We have 2442 single comp cutouts and 1176 multi
Plot predictions
Baseline assumption cat is 67.5% correct
train cat is 90.5% correct
34.15% improvement
[32m[12/19 01:42:42 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 01:42:42 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 01:42:42 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:42:42 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 01:42:42 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:42:42 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 01:42:42 d2.evaluation.evaluator]: [0m4.34%
[32m[12/19 01:42:42 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 01:42:42 d2.evaluation.evaluator]: [0m20.07%
[32m[12/19 01:42:42 d2.evaluation.evaluator]: [0mCatalogue is 0.9054726368159204 correct.
[32m[12/19 01:42:42 d2.engine.defaults]: [0mEvaluation results for train in csv format:
[32m[12/19 01:42:42 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 01:42:42 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 01:42:42 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0434,0.2007,0.9055
len dataset is: 1205 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_val.pkl
[32m[12/19 01:42:42 d2.data.common]: [0mSerializing 1205 elements to byte tensors and concatenating them all ...
[32m[12/19 01:42:42 d2.data.common]: [0mSerialized dataset takes 11.07 MiB
[32m[12/19 01:42:42 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 01:42:42 d2.evaluation.evaluator]: [0mStart inference on 1205 images
[32m[12/19 01:42:42 d2.evaluation.evaluator]: [0mInference done 11/1205. 0.0486 s / img. ETA=0:00:59
[32m[12/19 01:42:53 d2.evaluation.evaluator]: [0mInference done 209/1205. 0.0485 s / img. ETA=0:00:50
[32m[12/19 01:43:03 d2.evaluation.evaluator]: [0mInference done 390/1205. 0.0499 s / img. ETA=0:00:43
[32m[12/19 01:43:13 d2.evaluation.evaluator]: [0mInference done 549/1205. 0.0521 s / img. ETA=0:00:36
[32m[12/19 01:43:23 d2.evaluation.evaluator]: [0mInference done 722/1205. 0.0520 s / img. ETA=0:00:27
[32m[12/19 01:43:33 d2.evaluation.evaluator]: [0mInference done 843/1205. 0.0548 s / img. ETA=0:00:21
[32m[12/19 01:43:43 d2.evaluation.evaluator]: [0mInference done 989/1205. 0.0554 s / img. ETA=0:00:13
[32m[12/19 01:43:53 d2.evaluation.evaluator]: [0mInference done 1165/1205. 0.0543 s / img. ETA=0:00:02
[32m[12/19 01:43:55 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:13.227065 (0.061023 s / img per device, on 1 devices)
[32m[12/19 01:43:55 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:04 (0.054150 s / img per device, on 1 devices)
Evaluate predictions
We have 817 single comp cutouts and 388 multi
Plot predictions
Baseline assumption cat is 67.8% correct
val cat is 88.3% correct
30.23% improvement
[32m[12/19 01:43:56 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 01:43:56 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 01:43:56 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:43:56 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 01:43:56 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:43:56 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 01:43:56 d2.evaluation.evaluator]: [0m6.36%
[32m[12/19 01:43:56 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 01:43:56 d2.evaluation.evaluator]: [0m22.94%
[32m[12/19 01:43:56 d2.evaluation.evaluator]: [0mCatalogue is 0.8829875518672199 correct.
[32m[12/19 01:43:56 d2.engine.defaults]: [0mEvaluation results for val in csv format:
[32m[12/19 01:43:56 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 01:43:56 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 01:43:56 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0636,0.2294,0.8830
len dataset is: 1211 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_test.pkl
[32m[12/19 01:43:56 d2.data.common]: [0mSerializing 1211 elements to byte tensors and concatenating them all ...
[32m[12/19 01:43:57 d2.data.common]: [0mSerialized dataset takes 11.23 MiB
[32m[12/19 01:43:57 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 01:43:57 d2.evaluation.evaluator]: [0mStart inference on 1211 images
[32m[12/19 01:44:03 d2.evaluation.evaluator]: [0mInference done 116/1211. 0.0501 s / img. ETA=0:00:56
[32m[12/19 01:44:13 d2.evaluation.evaluator]: [0mInference done 305/1211. 0.0502 s / img. ETA=0:00:47
[32m[12/19 01:44:23 d2.evaluation.evaluator]: [0mInference done 467/1211. 0.0526 s / img. ETA=0:00:41
[32m[12/19 01:44:33 d2.evaluation.evaluator]: [0mInference done 620/1211. 0.0541 s / img. ETA=0:00:34
[32m[12/19 01:44:43 d2.evaluation.evaluator]: [0mInference done 803/1211. 0.0529 s / img. ETA=0:00:23
[32m[12/19 01:44:53 d2.evaluation.evaluator]: [0mInference done 981/1211. 0.0522 s / img. ETA=0:00:13
[32m[12/19 01:45:03 d2.evaluation.evaluator]: [0mInference done 1142/1211. 0.0522 s / img. ETA=0:00:03
[32m[12/19 01:45:09 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:11.644197 (0.059406 s / img per device, on 1 devices)
[32m[12/19 01:45:09 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:03 (0.052798 s / img per device, on 1 devices)
Evaluate predictions
We have 819 single comp cutouts and 392 multi
Plot predictions
Baseline assumption cat is 67.6% correct
test cat is 88.7% correct
31.14% improvement
[32m[12/19 01:45:09 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 01:45:09 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 01:45:09 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:45:09 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 01:45:09 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:45:09 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 01:45:09 d2.evaluation.evaluator]: [0m5.86%
[32m[12/19 01:45:09 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 01:45:09 d2.evaluation.evaluator]: [0m22.70%
[32m[12/19 01:45:09 d2.evaluation.evaluator]: [0mCatalogue is 0.8868703550784476 correct.
[32m[12/19 01:45:09 d2.engine.defaults]: [0mEvaluation results for test in csv format:
[32m[12/19 01:45:09 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 01:45:09 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 01:45:09 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0586,0.2270,0.8869
[32m[12/19 01:45:09 d2.utils.events]: [0meta: 0:10:33  iter: 11999  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0180  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:45:14 d2.utils.events]: [0meta: 0:10:28  iter: 12019  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0168  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:45:18 d2.utils.events]: [0meta: 0:10:24  iter: 12039  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0164  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:45:22 d2.utils.events]: [0meta: 0:10:20  iter: 12059  total_loss: 0.010  loss_cls: 0.010  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0162  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:45:26 d2.utils.events]: [0meta: 0:10:16  iter: 12079  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0198  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:45:31 d2.utils.events]: [0meta: 0:10:12  iter: 12099  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0164  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:45:35 d2.utils.events]: [0meta: 0:10:07  iter: 12119  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0163  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:45:39 d2.utils.events]: [0meta: 0:10:03  iter: 12139  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0162  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:45:43 d2.utils.events]: [0meta: 0:09:59  iter: 12159  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0176  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:45:47 d2.utils.events]: [0meta: 0:09:55  iter: 12179  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0169  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:45:52 d2.utils.events]: [0meta: 0:09:51  iter: 12199  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0165  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:45:56 d2.utils.events]: [0meta: 0:09:46  iter: 12219  total_loss: 0.002  loss_cls: 0.002  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0162  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:46:00 d2.utils.events]: [0meta: 0:09:43  iter: 12239  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0201  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:46:04 d2.utils.events]: [0meta: 0:09:38  iter: 12259  total_loss: 0.003  loss_cls: 0.003  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0197  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:46:09 d2.utils.events]: [0meta: 0:09:34  iter: 12279  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0212  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:46:13 d2.utils.events]: [0meta: 0:09:30  iter: 12299  total_loss: 0.003  loss_cls: 0.003  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0207  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:46:18 d2.utils.events]: [0meta: 0:09:26  iter: 12319  total_loss: 0.017  loss_cls: 0.017  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0197  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:46:22 d2.utils.events]: [0meta: 0:09:22  iter: 12339  total_loss: 0.002  loss_cls: 0.002  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0220  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:46:27 d2.utils.events]: [0meta: 0:09:17  iter: 12359  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0173  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:46:31 d2.utils.events]: [0meta: 0:09:13  iter: 12379  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0206  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:46:35 d2.utils.events]: [0meta: 0:09:09  iter: 12399  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0197  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:46:39 d2.utils.events]: [0meta: 0:09:04  iter: 12419  total_loss: 0.017  loss_cls: 0.017  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0195  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:46:44 d2.utils.events]: [0meta: 0:09:00  iter: 12439  total_loss: 0.006  loss_cls: 0.006  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0175  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:46:48 d2.utils.events]: [0meta: 0:08:56  iter: 12459  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0175  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:46:52 d2.utils.events]: [0meta: 0:08:51  iter: 12479  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0185  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:46:57 d2.utils.events]: [0meta: 0:08:47  iter: 12499  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0207  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:47:01 d2.utils.events]: [0meta: 0:08:43  iter: 12519  total_loss: 0.002  loss_cls: 0.002  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0172  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:47:06 d2.utils.events]: [0meta: 0:08:38  iter: 12539  total_loss: 0.010  loss_cls: 0.010  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:47:10 d2.utils.events]: [0meta: 0:08:33  iter: 12559  total_loss: 0.003  loss_cls: 0.003  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:47:14 d2.utils.events]: [0meta: 0:08:29  iter: 12579  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0166  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:47:19 d2.utils.events]: [0meta: 0:08:25  iter: 12599  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0186  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:47:23 d2.utils.events]: [0meta: 0:08:20  iter: 12619  total_loss: 0.006  loss_cls: 0.006  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0168  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:47:27 d2.utils.events]: [0meta: 0:08:15  iter: 12639  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0168  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:47:32 d2.utils.events]: [0meta: 0:08:11  iter: 12659  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0170  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:47:36 d2.utils.events]: [0meta: 0:08:07  iter: 12679  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0176  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:47:40 d2.utils.events]: [0meta: 0:08:03  iter: 12699  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0203  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:47:44 d2.utils.events]: [0meta: 0:07:59  iter: 12719  total_loss: 0.003  loss_cls: 0.003  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0204  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:47:49 d2.utils.events]: [0meta: 0:07:55  iter: 12739  total_loss: 0.006  loss_cls: 0.006  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0201  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:47:53 d2.utils.events]: [0meta: 0:07:52  iter: 12759  total_loss: 0.010  loss_cls: 0.010  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0203  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:47:57 d2.utils.events]: [0meta: 0:07:48  iter: 12779  total_loss: 0.006  loss_cls: 0.006  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0199  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:48:02 d2.utils.events]: [0meta: 0:07:44  iter: 12799  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0183  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:48:06 d2.utils.events]: [0meta: 0:07:39  iter: 12819  total_loss: 0.004  loss_cls: 0.004  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0170  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:48:11 d2.utils.events]: [0meta: 0:07:35  iter: 12839  total_loss: 0.015  loss_cls: 0.015  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0184  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:48:15 d2.utils.events]: [0meta: 0:07:31  iter: 12859  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0173  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:48:19 d2.utils.events]: [0meta: 0:07:27  iter: 12879  total_loss: 0.004  loss_cls: 0.004  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0193  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:48:24 d2.utils.events]: [0meta: 0:07:23  iter: 12899  total_loss: 0.004  loss_cls: 0.004  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0202  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:48:28 d2.utils.events]: [0meta: 0:07:19  iter: 12919  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0203  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:48:32 d2.utils.events]: [0meta: 0:07:15  iter: 12939  total_loss: 0.003  loss_cls: 0.003  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0202  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:48:37 d2.utils.events]: [0meta: 0:07:10  iter: 12959  total_loss: 0.004  loss_cls: 0.004  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0204  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:48:41 d2.utils.events]: [0meta: 0:07:06  iter: 12979  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0203  lr: 0.000300  max_mem: 1829M
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
[32m[12/19 01:48:46 d2.data.common]: [0mSerializing 3618 elements to byte tensors and concatenating them all ...
[32m[12/19 01:48:47 d2.data.common]: [0mSerialized dataset takes 33.05 MiB
[32m[12/19 01:48:47 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 01:48:47 d2.evaluation.evaluator]: [0mStart inference on 3618 images
[32m[12/19 01:48:48 d2.evaluation.evaluator]: [0mInference done 11/3618. 0.0523 s / img. ETA=0:03:13
[32m[12/19 01:48:58 d2.evaluation.evaluator]: [0mInference done 193/3618. 0.0528 s / img. ETA=0:03:08
[32m[12/19 01:49:08 d2.evaluation.evaluator]: [0mInference done 365/3618. 0.0537 s / img. ETA=0:03:04
[32m[12/19 01:49:18 d2.evaluation.evaluator]: [0mInference done 529/3618. 0.0544 s / img. ETA=0:02:59
[32m[12/19 01:49:28 d2.evaluation.evaluator]: [0mInference done 693/3618. 0.0545 s / img. ETA=0:02:51
[32m[12/19 01:49:38 d2.evaluation.evaluator]: [0mInference done 854/3618. 0.0545 s / img. ETA=0:02:44
[32m[12/19 01:49:48 d2.evaluation.evaluator]: [0mInference done 1012/3618. 0.0546 s / img. ETA=0:02:36
[32m[12/19 01:49:58 d2.evaluation.evaluator]: [0mInference done 1167/3618. 0.0546 s / img. ETA=0:02:28
[32m[12/19 01:50:08 d2.evaluation.evaluator]: [0mInference done 1339/3618. 0.0539 s / img. ETA=0:02:17
[32m[12/19 01:50:18 d2.evaluation.evaluator]: [0mInference done 1510/3618. 0.0533 s / img. ETA=0:02:06
[32m[12/19 01:50:28 d2.evaluation.evaluator]: [0mInference done 1678/3618. 0.0529 s / img. ETA=0:01:56
[32m[12/19 01:50:38 d2.evaluation.evaluator]: [0mInference done 1842/3618. 0.0525 s / img. ETA=0:01:46
[32m[12/19 01:50:48 d2.evaluation.evaluator]: [0mInference done 1999/3618. 0.0523 s / img. ETA=0:01:37
[32m[12/19 01:50:58 d2.evaluation.evaluator]: [0mInference done 2158/3618. 0.0520 s / img. ETA=0:01:28
[32m[12/19 01:51:08 d2.evaluation.evaluator]: [0mInference done 2314/3618. 0.0518 s / img. ETA=0:01:19
[32m[12/19 01:51:18 d2.evaluation.evaluator]: [0mInference done 2468/3618. 0.0516 s / img. ETA=0:01:10
[32m[12/19 01:51:28 d2.evaluation.evaluator]: [0mInference done 2619/3618. 0.0514 s / img. ETA=0:01:01
[32m[12/19 01:51:38 d2.evaluation.evaluator]: [0mInference done 2767/3618. 0.0513 s / img. ETA=0:00:52
[32m[12/19 01:51:48 d2.evaluation.evaluator]: [0mInference done 2913/3618. 0.0511 s / img. ETA=0:00:43
[32m[12/19 01:51:58 d2.evaluation.evaluator]: [0mInference done 3057/3618. 0.0510 s / img. ETA=0:00:35
[32m[12/19 01:52:08 d2.evaluation.evaluator]: [0mInference done 3178/3618. 0.0512 s / img. ETA=0:00:27
[32m[12/19 01:52:18 d2.evaluation.evaluator]: [0mInference done 3319/3618. 0.0511 s / img. ETA=0:00:19
[32m[12/19 01:52:28 d2.evaluation.evaluator]: [0mInference done 3458/3618. 0.0510 s / img. ETA=0:00:10
[32m[12/19 01:52:38 d2.evaluation.evaluator]: [0mInference done 3595/3618. 0.0509 s / img. ETA=0:00:01
[32m[12/19 01:52:41 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:53.812120 (0.064714 s / img per device, on 1 devices)
[32m[12/19 01:52:41 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:03:03 (0.050888 s / img per device, on 1 devices)
Evaluate predictions
We have 2442 single comp cutouts and 1176 multi
Plot predictions
Baseline assumption cat is 67.5% correct
train cat is 89.6% correct
32.80% improvement
[32m[12/19 01:52:44 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 01:52:44 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 01:52:44 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:52:44 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 01:52:44 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:52:44 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 01:52:44 d2.evaluation.evaluator]: [0m4.67%
[32m[12/19 01:52:44 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 01:52:44 d2.evaluation.evaluator]: [0m22.19%
[32m[12/19 01:52:44 d2.evaluation.evaluator]: [0mCatalogue is 0.896351575456053 correct.
[32m[12/19 01:52:44 d2.engine.defaults]: [0mEvaluation results for train in csv format:
[32m[12/19 01:52:44 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 01:52:44 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 01:52:44 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0467,0.2219,0.8964
len dataset is: 1205 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_val.pkl
[32m[12/19 01:52:44 d2.data.common]: [0mSerializing 1205 elements to byte tensors and concatenating them all ...
[32m[12/19 01:52:44 d2.data.common]: [0mSerialized dataset takes 11.07 MiB
[32m[12/19 01:52:44 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 01:52:44 d2.evaluation.evaluator]: [0mStart inference on 1205 images
[32m[12/19 01:52:48 d2.evaluation.evaluator]: [0mInference done 85/1205. 0.0499 s / img. ETA=0:00:57
[32m[12/19 01:52:58 d2.evaluation.evaluator]: [0mInference done 279/1205. 0.0492 s / img. ETA=0:00:47
[32m[12/19 01:53:08 d2.evaluation.evaluator]: [0mInference done 470/1205. 0.0489 s / img. ETA=0:00:38
[32m[12/19 01:53:18 d2.evaluation.evaluator]: [0mInference done 657/1205. 0.0488 s / img. ETA=0:00:28
[32m[12/19 01:53:28 d2.evaluation.evaluator]: [0mInference done 840/1205. 0.0488 s / img. ETA=0:00:19
[32m[12/19 01:53:38 d2.evaluation.evaluator]: [0mInference done 1019/1205. 0.0487 s / img. ETA=0:00:09
[32m[12/19 01:53:48 d2.evaluation.evaluator]: [0mInference done 1195/1205. 0.0487 s / img. ETA=0:00:00
[32m[12/19 01:53:49 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:05.313869 (0.054428 s / img per device, on 1 devices)
[32m[12/19 01:53:49 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:58 (0.048686 s / img per device, on 1 devices)
Evaluate predictions
We have 817 single comp cutouts and 388 multi
Plot predictions
Baseline assumption cat is 67.8% correct
val cat is 87.7% correct
29.38% improvement
[32m[12/19 01:53:50 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 01:53:50 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 01:53:50 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:53:50 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 01:53:50 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:53:50 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 01:53:50 d2.evaluation.evaluator]: [0m6.00%
[32m[12/19 01:53:50 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 01:53:50 d2.evaluation.evaluator]: [0m25.52%
[32m[12/19 01:53:50 d2.evaluation.evaluator]: [0mCatalogue is 0.8771784232365145 correct.
[32m[12/19 01:53:50 d2.engine.defaults]: [0mEvaluation results for val in csv format:
[32m[12/19 01:53:50 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 01:53:50 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 01:53:50 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0600,0.2552,0.8772
len dataset is: 1211 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_test.pkl
[32m[12/19 01:53:50 d2.data.common]: [0mSerializing 1211 elements to byte tensors and concatenating them all ...
[32m[12/19 01:53:51 d2.data.common]: [0mSerialized dataset takes 11.23 MiB
[32m[12/19 01:53:51 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 01:53:51 d2.evaluation.evaluator]: [0mStart inference on 1211 images
[32m[12/19 01:53:58 d2.evaluation.evaluator]: [0mInference done 151/1211. 0.0499 s / img. ETA=0:00:55
[32m[12/19 01:54:08 d2.evaluation.evaluator]: [0mInference done 340/1211. 0.0500 s / img. ETA=0:00:45
[32m[12/19 01:54:18 d2.evaluation.evaluator]: [0mInference done 525/1211. 0.0500 s / img. ETA=0:00:36
[32m[12/19 01:54:29 d2.evaluation.evaluator]: [0mInference done 706/1211. 0.0500 s / img. ETA=0:00:27
[32m[12/19 01:54:39 d2.evaluation.evaluator]: [0mInference done 883/1211. 0.0500 s / img. ETA=0:00:17
[32m[12/19 01:54:49 d2.evaluation.evaluator]: [0mInference done 1055/1211. 0.0501 s / img. ETA=0:00:08
[32m[12/19 01:54:58 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:07.295728 (0.055801 s / img per device, on 1 devices)
[32m[12/19 01:54:58 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:00 (0.050072 s / img per device, on 1 devices)
Evaluate predictions
We have 819 single comp cutouts and 392 multi
Plot predictions
Baseline assumption cat is 67.6% correct
test cat is 87.8% correct
29.79% improvement
[32m[12/19 01:54:59 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 01:54:59 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 01:54:59 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:54:59 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 01:54:59 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 01:54:59 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 01:54:59 d2.evaluation.evaluator]: [0m5.86%
[32m[12/19 01:54:59 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 01:54:59 d2.evaluation.evaluator]: [0m25.51%
[32m[12/19 01:54:59 d2.evaluation.evaluator]: [0mCatalogue is 0.8777869529314616 correct.
[32m[12/19 01:54:59 d2.engine.defaults]: [0mEvaluation results for test in csv format:
[32m[12/19 01:54:59 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 01:54:59 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 01:54:59 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0586,0.2551,0.8778
[32m[12/19 01:54:59 d2.utils.events]: [0meta: 0:07:02  iter: 12999  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0202  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:55:03 d2.utils.events]: [0meta: 0:06:57  iter: 13019  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0160  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:55:07 d2.utils.events]: [0meta: 0:06:53  iter: 13039  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:55:12 d2.utils.events]: [0meta: 0:06:49  iter: 13059  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:55:16 d2.utils.events]: [0meta: 0:06:44  iter: 13079  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0179  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:55:20 d2.utils.events]: [0meta: 0:06:40  iter: 13099  total_loss: 0.015  loss_cls: 0.015  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0195  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:55:24 d2.utils.events]: [0meta: 0:06:36  iter: 13119  total_loss: 0.023  loss_cls: 0.023  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0195  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:55:29 d2.utils.events]: [0meta: 0:06:32  iter: 13139  total_loss: 0.020  loss_cls: 0.020  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0195  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:55:33 d2.utils.events]: [0meta: 0:06:28  iter: 13159  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0204  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:55:37 d2.utils.events]: [0meta: 0:06:24  iter: 13179  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0205  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:55:41 d2.utils.events]: [0meta: 0:06:20  iter: 13199  total_loss: 0.031  loss_cls: 0.031  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0205  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:55:46 d2.utils.events]: [0meta: 0:06:16  iter: 13219  total_loss: 0.037  loss_cls: 0.037  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0205  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:55:50 d2.utils.events]: [0meta: 0:06:11  iter: 13239  total_loss: 0.028  loss_cls: 0.028  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0205  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:55:54 d2.utils.events]: [0meta: 0:06:07  iter: 13259  total_loss: 0.028  loss_cls: 0.028  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0181  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:55:58 d2.utils.events]: [0meta: 0:06:03  iter: 13279  total_loss: 0.016  loss_cls: 0.016  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:56:03 d2.utils.events]: [0meta: 0:05:58  iter: 13299  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0160  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:56:07 d2.utils.events]: [0meta: 0:05:54  iter: 13319  total_loss: 0.019  loss_cls: 0.019  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0162  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:56:11 d2.utils.events]: [0meta: 0:05:50  iter: 13339  total_loss: 0.022  loss_cls: 0.022  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0158  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:56:15 d2.utils.events]: [0meta: 0:05:45  iter: 13359  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0163  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:56:19 d2.utils.events]: [0meta: 0:05:41  iter: 13379  total_loss: 0.025  loss_cls: 0.025  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0162  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:56:24 d2.utils.events]: [0meta: 0:05:37  iter: 13399  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0153  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:56:28 d2.utils.events]: [0meta: 0:05:33  iter: 13419  total_loss: 0.006  loss_cls: 0.006  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0152  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:56:32 d2.utils.events]: [0meta: 0:05:28  iter: 13439  total_loss: 0.027  loss_cls: 0.027  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0171  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:56:37 d2.utils.events]: [0meta: 0:05:24  iter: 13459  total_loss: 0.028  loss_cls: 0.028  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0168  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:56:41 d2.utils.events]: [0meta: 0:05:20  iter: 13479  total_loss: 0.022  loss_cls: 0.022  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0169  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:56:45 d2.utils.events]: [0meta: 0:05:16  iter: 13499  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0163  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:56:50 d2.utils.events]: [0meta: 0:05:12  iter: 13519  total_loss: 0.022  loss_cls: 0.022  loss_box_reg: 0.000  time: 0.2132  data_time: 0.0157  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:56:54 d2.utils.events]: [0meta: 0:05:07  iter: 13539  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0162  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:56:58 d2.utils.events]: [0meta: 0:05:03  iter: 13559  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0161  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:57:02 d2.utils.events]: [0meta: 0:04:59  iter: 13579  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:57:06 d2.utils.events]: [0meta: 0:04:55  iter: 13599  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0158  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:57:10 d2.utils.events]: [0meta: 0:04:50  iter: 13619  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0165  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:57:15 d2.utils.events]: [0meta: 0:04:46  iter: 13639  total_loss: 0.010  loss_cls: 0.010  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0168  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:57:19 d2.utils.events]: [0meta: 0:04:42  iter: 13659  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0163  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:57:23 d2.utils.events]: [0meta: 0:04:37  iter: 13679  total_loss: 0.019  loss_cls: 0.019  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0166  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:57:27 d2.utils.events]: [0meta: 0:04:33  iter: 13699  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0165  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:57:31 d2.utils.events]: [0meta: 0:04:29  iter: 13719  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0171  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:57:36 d2.utils.events]: [0meta: 0:04:24  iter: 13739  total_loss: 0.010  loss_cls: 0.010  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0175  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:57:40 d2.utils.events]: [0meta: 0:04:20  iter: 13759  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:57:44 d2.utils.events]: [0meta: 0:04:16  iter: 13779  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:57:49 d2.utils.events]: [0meta: 0:04:11  iter: 13799  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0163  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:57:53 d2.utils.events]: [0meta: 0:04:07  iter: 13819  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0167  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:57:57 d2.utils.events]: [0meta: 0:04:03  iter: 13839  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0171  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:58:01 d2.utils.events]: [0meta: 0:03:59  iter: 13859  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0165  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:58:05 d2.utils.events]: [0meta: 0:03:54  iter: 13879  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:58:09 d2.utils.events]: [0meta: 0:03:50  iter: 13899  total_loss: 0.003  loss_cls: 0.003  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:58:14 d2.utils.events]: [0meta: 0:03:45  iter: 13919  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:58:18 d2.utils.events]: [0meta: 0:03:41  iter: 13939  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:58:22 d2.utils.events]: [0meta: 0:03:37  iter: 13959  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0154  lr: 0.000300  max_mem: 1829M
[32m[12/19 01:58:26 d2.utils.events]: [0meta: 0:03:33  iter: 13979  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0157  lr: 0.000300  max_mem: 1829M
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
[32m[12/19 01:58:32 d2.data.common]: [0mSerializing 3618 elements to byte tensors and concatenating them all ...
[32m[12/19 01:58:33 d2.data.common]: [0mSerialized dataset takes 33.05 MiB
[32m[12/19 01:58:33 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 01:58:33 d2.evaluation.evaluator]: [0mStart inference on 3618 images
[32m[12/19 01:58:34 d2.evaluation.evaluator]: [0mInference done 11/3618. 0.0502 s / img. ETA=0:03:05
[32m[12/19 01:58:44 d2.evaluation.evaluator]: [0mInference done 203/3618. 0.0501 s / img. ETA=0:02:58
[32m[12/19 01:58:54 d2.evaluation.evaluator]: [0mInference done 391/3618. 0.0500 s / img. ETA=0:02:50
[32m[12/19 01:59:04 d2.evaluation.evaluator]: [0mInference done 574/3618. 0.0501 s / img. ETA=0:02:42
[32m[12/19 01:59:14 d2.evaluation.evaluator]: [0mInference done 743/3618. 0.0507 s / img. ETA=0:02:37
[32m[12/19 01:59:24 d2.evaluation.evaluator]: [0mInference done 913/3618. 0.0509 s / img. ETA=0:02:30
[32m[12/19 01:59:34 d2.evaluation.evaluator]: [0mInference done 1091/3618. 0.0505 s / img. ETA=0:02:20
[32m[12/19 01:59:44 d2.evaluation.evaluator]: [0mInference done 1264/3618. 0.0504 s / img. ETA=0:02:11
[32m[12/19 01:59:54 d2.evaluation.evaluator]: [0mInference done 1436/3618. 0.0502 s / img. ETA=0:02:02
[32m[12/19 02:00:04 d2.evaluation.evaluator]: [0mInference done 1605/3618. 0.0500 s / img. ETA=0:01:53
[32m[12/19 02:00:14 d2.evaluation.evaluator]: [0mInference done 1771/3618. 0.0499 s / img. ETA=0:01:45
[32m[12/19 02:00:24 d2.evaluation.evaluator]: [0mInference done 1934/3618. 0.0498 s / img. ETA=0:01:36
[32m[12/19 02:00:34 d2.evaluation.evaluator]: [0mInference done 2094/3618. 0.0497 s / img. ETA=0:01:28
[32m[12/19 02:00:44 d2.evaluation.evaluator]: [0mInference done 2251/3618. 0.0496 s / img. ETA=0:01:19
[32m[12/19 02:00:54 d2.evaluation.evaluator]: [0mInference done 2406/3618. 0.0495 s / img. ETA=0:01:11
[32m[12/19 02:01:04 d2.evaluation.evaluator]: [0mInference done 2558/3618. 0.0495 s / img. ETA=0:01:02
[32m[12/19 02:01:14 d2.evaluation.evaluator]: [0mInference done 2708/3618. 0.0494 s / img. ETA=0:00:54
[32m[12/19 02:01:25 d2.evaluation.evaluator]: [0mInference done 2855/3618. 0.0494 s / img. ETA=0:00:45
[32m[12/19 02:01:35 d2.evaluation.evaluator]: [0mInference done 3000/3618. 0.0493 s / img. ETA=0:00:37
[32m[12/19 02:01:45 d2.evaluation.evaluator]: [0mInference done 3142/3618. 0.0493 s / img. ETA=0:00:28
[32m[12/19 02:01:55 d2.evaluation.evaluator]: [0mInference done 3275/3618. 0.0494 s / img. ETA=0:00:21
[32m[12/19 02:02:05 d2.evaluation.evaluator]: [0mInference done 3398/3618. 0.0495 s / img. ETA=0:00:13
[32m[12/19 02:02:15 d2.evaluation.evaluator]: [0mInference done 3517/3618. 0.0497 s / img. ETA=0:00:06
[32m[12/19 02:02:25 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:50.983827 (0.063931 s / img per device, on 1 devices)
[32m[12/19 02:02:25 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:03:00 (0.049836 s / img per device, on 1 devices)
Evaluate predictions
We have 2442 single comp cutouts and 1176 multi
Plot predictions
Baseline assumption cat is 67.5% correct
train cat is 90.6% correct
34.23% improvement
[32m[12/19 02:02:27 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 02:02:27 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 02:02:27 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 02:02:27 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 02:02:27 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 02:02:27 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 02:02:27 d2.evaluation.evaluator]: [0m4.67%
[32m[12/19 02:02:27 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 02:02:27 d2.evaluation.evaluator]: [0m19.22%
[32m[12/19 02:02:27 d2.evaluation.evaluator]: [0mCatalogue is 0.9060254284134881 correct.
[32m[12/19 02:02:27 d2.engine.defaults]: [0mEvaluation results for train in csv format:
[32m[12/19 02:02:27 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 02:02:27 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 02:02:27 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0467,0.1922,0.9060
len dataset is: 1205 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_val.pkl
[32m[12/19 02:02:27 d2.data.common]: [0mSerializing 1205 elements to byte tensors and concatenating them all ...
[32m[12/19 02:02:28 d2.data.common]: [0mSerialized dataset takes 11.07 MiB
[32m[12/19 02:02:28 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 02:02:28 d2.evaluation.evaluator]: [0mStart inference on 1205 images
[32m[12/19 02:02:28 d2.evaluation.evaluator]: [0mInference done 11/1205. 0.0500 s / img. ETA=0:01:01
[32m[12/19 02:02:38 d2.evaluation.evaluator]: [0mInference done 203/1205. 0.0500 s / img. ETA=0:00:52
[32m[12/19 02:02:48 d2.evaluation.evaluator]: [0mInference done 390/1205. 0.0500 s / img. ETA=0:00:43
[32m[12/19 02:02:58 d2.evaluation.evaluator]: [0mInference done 573/1205. 0.0500 s / img. ETA=0:00:33
[32m[12/19 02:03:08 d2.evaluation.evaluator]: [0mInference done 751/1205. 0.0501 s / img. ETA=0:00:24
[32m[12/19 02:03:18 d2.evaluation.evaluator]: [0mInference done 922/1205. 0.0504 s / img. ETA=0:00:15
[32m[12/19 02:03:28 d2.evaluation.evaluator]: [0mInference done 1092/1205. 0.0504 s / img. ETA=0:00:06
[32m[12/19 02:03:36 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:07.736822 (0.056447 s / img per device, on 1 devices)
[32m[12/19 02:03:36 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:00 (0.050425 s / img per device, on 1 devices)
Evaluate predictions
We have 817 single comp cutouts and 388 multi
Plot predictions
Baseline assumption cat is 67.8% correct
val cat is 88.3% correct
30.23% improvement
[32m[12/19 02:03:37 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 02:03:37 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 02:03:37 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 02:03:37 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 02:03:37 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 02:03:37 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 02:03:37 d2.evaluation.evaluator]: [0m6.24%
[32m[12/19 02:03:37 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 02:03:37 d2.evaluation.evaluator]: [0m23.20%
[32m[12/19 02:03:37 d2.evaluation.evaluator]: [0mCatalogue is 0.8829875518672199 correct.
[32m[12/19 02:03:37 d2.engine.defaults]: [0mEvaluation results for val in csv format:
[32m[12/19 02:03:37 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 02:03:37 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 02:03:37 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0624,0.2320,0.8830
len dataset is: 1211 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_test.pkl
[32m[12/19 02:03:37 d2.data.common]: [0mSerializing 1211 elements to byte tensors and concatenating them all ...
[32m[12/19 02:03:37 d2.data.common]: [0mSerialized dataset takes 11.23 MiB
[32m[12/19 02:03:37 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 02:03:37 d2.evaluation.evaluator]: [0mStart inference on 1211 images
[32m[12/19 02:03:38 d2.evaluation.evaluator]: [0mInference done 32/1211. 0.0489 s / img. ETA=0:00:59
[32m[12/19 02:03:48 d2.evaluation.evaluator]: [0mInference done 228/1211. 0.0488 s / img. ETA=0:00:50
[32m[12/19 02:03:59 d2.evaluation.evaluator]: [0mInference done 374/1211. 0.0547 s / img. ETA=0:00:48
[32m[12/19 02:04:09 d2.evaluation.evaluator]: [0mInference done 520/1211. 0.0569 s / img. ETA=0:00:42
[32m[12/19 02:04:19 d2.evaluation.evaluator]: [0mInference done 672/1211. 0.0574 s / img. ETA=0:00:33
[32m[12/19 02:04:29 d2.evaluation.evaluator]: [0mInference done 848/1211. 0.0560 s / img. ETA=0:00:22
[32m[12/19 02:04:39 d2.evaluation.evaluator]: [0mInference done 1020/1211. 0.0551 s / img. ETA=0:00:11
[32m[12/19 02:04:49 d2.evaluation.evaluator]: [0mInference done 1179/1211. 0.0548 s / img. ETA=0:00:01
[32m[12/19 02:04:51 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:14.214317 (0.061538 s / img per device, on 1 devices)
[32m[12/19 02:04:51 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:06 (0.054845 s / img per device, on 1 devices)
Evaluate predictions
We have 819 single comp cutouts and 392 multi
Plot predictions
Baseline assumption cat is 67.6% correct
test cat is 88.6% correct
31.01% improvement
[32m[12/19 02:04:52 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 02:04:52 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 02:04:52 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 02:04:52 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 02:04:52 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 02:04:52 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 02:04:52 d2.evaluation.evaluator]: [0m5.74%
[32m[12/19 02:04:52 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 02:04:52 d2.evaluation.evaluator]: [0m23.21%
[32m[12/19 02:04:52 d2.evaluation.evaluator]: [0mCatalogue is 0.8860445912469034 correct.
[32m[12/19 02:04:52 d2.engine.defaults]: [0mEvaluation results for test in csv format:
[32m[12/19 02:04:52 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 02:04:52 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 02:04:52 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0574,0.2321,0.8860
[32m[12/19 02:04:52 d2.utils.events]: [0meta: 0:03:28  iter: 13999  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0154  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:04:57 d2.utils.events]: [0meta: 0:03:24  iter: 14019  total_loss: 0.010  loss_cls: 0.010  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0209  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:05:01 d2.utils.events]: [0meta: 0:03:20  iter: 14039  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0190  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:05:05 d2.utils.events]: [0meta: 0:03:16  iter: 14059  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0188  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:05:10 d2.utils.events]: [0meta: 0:03:12  iter: 14079  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0198  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:05:14 d2.utils.events]: [0meta: 0:03:08  iter: 14099  total_loss: 0.010  loss_cls: 0.010  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0206  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:05:18 d2.utils.events]: [0meta: 0:03:03  iter: 14119  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0203  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:05:23 d2.utils.events]: [0meta: 0:02:59  iter: 14139  total_loss: 0.006  loss_cls: 0.006  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0213  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:05:27 d2.utils.events]: [0meta: 0:02:55  iter: 14159  total_loss: 0.003  loss_cls: 0.003  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0203  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:05:31 d2.utils.events]: [0meta: 0:02:51  iter: 14179  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0203  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:05:35 d2.utils.events]: [0meta: 0:02:47  iter: 14199  total_loss: 0.002  loss_cls: 0.002  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0203  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:05:40 d2.utils.events]: [0meta: 0:02:43  iter: 14219  total_loss: 0.013  loss_cls: 0.013  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0189  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:05:44 d2.utils.events]: [0meta: 0:02:38  iter: 14239  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0198  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:05:49 d2.utils.events]: [0meta: 0:02:34  iter: 14259  total_loss: 0.010  loss_cls: 0.010  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0198  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:05:53 d2.utils.events]: [0meta: 0:02:30  iter: 14279  total_loss: 0.004  loss_cls: 0.004  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0197  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:05:58 d2.utils.events]: [0meta: 0:02:26  iter: 14299  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0197  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:06:02 d2.utils.events]: [0meta: 0:02:22  iter: 14319  total_loss: 0.020  loss_cls: 0.020  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0184  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:06:06 d2.utils.events]: [0meta: 0:02:18  iter: 14339  total_loss: 0.015  loss_cls: 0.015  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0161  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:06:10 d2.utils.events]: [0meta: 0:02:14  iter: 14359  total_loss: 0.018  loss_cls: 0.018  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0160  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:06:15 d2.utils.events]: [0meta: 0:02:09  iter: 14379  total_loss: 0.004  loss_cls: 0.004  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0157  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:06:19 d2.utils.events]: [0meta: 0:02:05  iter: 14399  total_loss: 0.015  loss_cls: 0.015  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0161  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:06:23 d2.utils.events]: [0meta: 0:02:01  iter: 14419  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0168  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:06:28 d2.utils.events]: [0meta: 0:01:57  iter: 14439  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0160  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:06:32 d2.utils.events]: [0meta: 0:01:53  iter: 14459  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2131  data_time: 0.0164  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:06:36 d2.utils.events]: [0meta: 0:01:48  iter: 14479  total_loss: 0.003  loss_cls: 0.003  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:06:40 d2.utils.events]: [0meta: 0:01:44  iter: 14499  total_loss: 0.004  loss_cls: 0.004  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0160  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:06:44 d2.utils.events]: [0meta: 0:01:40  iter: 14519  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0167  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:06:49 d2.utils.events]: [0meta: 0:01:36  iter: 14539  total_loss: 0.002  loss_cls: 0.002  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0167  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:06:53 d2.utils.events]: [0meta: 0:01:32  iter: 14559  total_loss: 0.009  loss_cls: 0.009  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0176  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:06:57 d2.utils.events]: [0meta: 0:01:28  iter: 14579  total_loss: 0.005  loss_cls: 0.005  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0164  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:07:02 d2.utils.events]: [0meta: 0:01:24  iter: 14599  total_loss: 0.004  loss_cls: 0.004  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0161  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:07:06 d2.utils.events]: [0meta: 0:01:19  iter: 14619  total_loss: 0.004  loss_cls: 0.004  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0152  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:07:10 d2.utils.events]: [0meta: 0:01:15  iter: 14639  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0155  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:07:14 d2.utils.events]: [0meta: 0:01:11  iter: 14659  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:07:18 d2.utils.events]: [0meta: 0:01:07  iter: 14679  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:07:23 d2.utils.events]: [0meta: 0:01:03  iter: 14699  total_loss: 0.028  loss_cls: 0.028  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0164  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:07:27 d2.utils.events]: [0meta: 0:00:58  iter: 14719  total_loss: 0.004  loss_cls: 0.004  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0157  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:07:31 d2.utils.events]: [0meta: 0:00:54  iter: 14739  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:07:36 d2.utils.events]: [0meta: 0:00:50  iter: 14759  total_loss: 0.006  loss_cls: 0.006  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0162  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:07:40 d2.utils.events]: [0meta: 0:00:46  iter: 14779  total_loss: 0.006  loss_cls: 0.006  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0154  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:07:44 d2.utils.events]: [0meta: 0:00:42  iter: 14799  total_loss: 0.012  loss_cls: 0.012  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0154  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:07:49 d2.utils.events]: [0meta: 0:00:38  iter: 14819  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0162  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:07:53 d2.utils.events]: [0meta: 0:00:33  iter: 14839  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0153  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:07:57 d2.utils.events]: [0meta: 0:00:29  iter: 14859  total_loss: 0.003  loss_cls: 0.003  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0162  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:08:01 d2.utils.events]: [0meta: 0:00:25  iter: 14879  total_loss: 0.003  loss_cls: 0.003  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0160  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:08:06 d2.utils.events]: [0meta: 0:00:21  iter: 14899  total_loss: 0.004  loss_cls: 0.004  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0156  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:08:10 d2.utils.events]: [0meta: 0:00:17  iter: 14919  total_loss: 0.008  loss_cls: 0.008  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0152  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:08:14 d2.utils.events]: [0meta: 0:00:12  iter: 14939  total_loss: 0.014  loss_cls: 0.014  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0151  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:08:18 d2.utils.events]: [0meta: 0:00:08  iter: 14959  total_loss: 0.007  loss_cls: 0.007  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0152  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:08:22 d2.utils.events]: [0meta: 0:00:04  iter: 14979  total_loss: 0.011  loss_cls: 0.011  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0152  lr: 0.000300  max_mem: 1829M
len dataset is: 3618 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_train.pkl
[32m[12/19 02:08:28 d2.data.common]: [0mSerializing 3618 elements to byte tensors and concatenating them all ...
[32m[12/19 02:08:29 d2.data.common]: [0mSerialized dataset takes 33.05 MiB
[32m[12/19 02:08:29 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 02:08:29 d2.evaluation.evaluator]: [0mStart inference on 3618 images
[32m[12/19 02:08:29 d2.evaluation.evaluator]: [0mInference done 11/3618. 0.0502 s / img. ETA=0:03:05
[32m[12/19 02:08:39 d2.evaluation.evaluator]: [0mInference done 187/3618. 0.0544 s / img. ETA=0:03:14
[32m[12/19 02:08:49 d2.evaluation.evaluator]: [0mInference done 368/3618. 0.0531 s / img. ETA=0:03:02
[32m[12/19 02:08:59 d2.evaluation.evaluator]: [0mInference done 552/3618. 0.0521 s / img. ETA=0:02:50
[32m[12/19 02:09:09 d2.evaluation.evaluator]: [0mInference done 737/3618. 0.0512 s / img. ETA=0:02:38
[32m[12/19 02:09:19 d2.evaluation.evaluator]: [0mInference done 918/3618. 0.0507 s / img. ETA=0:02:28
[32m[12/19 02:09:29 d2.evaluation.evaluator]: [0mInference done 1096/3618. 0.0504 s / img. ETA=0:02:19
[32m[12/19 02:09:39 d2.evaluation.evaluator]: [0mInference done 1268/3618. 0.0502 s / img. ETA=0:02:11
[32m[12/19 02:09:50 d2.evaluation.evaluator]: [0mInference done 1425/3618. 0.0505 s / img. ETA=0:02:04
[32m[12/19 02:10:00 d2.evaluation.evaluator]: [0mInference done 1588/3618. 0.0505 s / img. ETA=0:01:56
[32m[12/19 02:10:10 d2.evaluation.evaluator]: [0mInference done 1735/3618. 0.0508 s / img. ETA=0:01:49
[32m[12/19 02:10:20 d2.evaluation.evaluator]: [0mInference done 1898/3618. 0.0506 s / img. ETA=0:01:40
[32m[12/19 02:10:30 d2.evaluation.evaluator]: [0mInference done 2058/3618. 0.0505 s / img. ETA=0:01:31
[32m[12/19 02:10:40 d2.evaluation.evaluator]: [0mInference done 2215/3618. 0.0503 s / img. ETA=0:01:22
[32m[12/19 02:10:50 d2.evaluation.evaluator]: [0mInference done 2370/3618. 0.0502 s / img. ETA=0:01:14
[32m[12/19 02:11:00 d2.evaluation.evaluator]: [0mInference done 2522/3618. 0.0501 s / img. ETA=0:01:05
[32m[12/19 02:11:10 d2.evaluation.evaluator]: [0mInference done 2672/3618. 0.0501 s / img. ETA=0:00:57
[32m[12/19 02:11:20 d2.evaluation.evaluator]: [0mInference done 2818/3618. 0.0500 s / img. ETA=0:00:48
[32m[12/19 02:11:30 d2.evaluation.evaluator]: [0mInference done 2960/3618. 0.0499 s / img. ETA=0:00:40
[32m[12/19 02:11:40 d2.evaluation.evaluator]: [0mInference done 3102/3618. 0.0499 s / img. ETA=0:00:31
[32m[12/19 02:11:50 d2.evaluation.evaluator]: [0mInference done 3240/3618. 0.0498 s / img. ETA=0:00:23
[32m[12/19 02:12:00 d2.evaluation.evaluator]: [0mInference done 3368/3618. 0.0499 s / img. ETA=0:00:15
[32m[12/19 02:12:10 d2.evaluation.evaluator]: [0mInference done 3487/3618. 0.0501 s / img. ETA=0:00:08
[32m[12/19 02:12:20 d2.evaluation.evaluator]: [0mInference done 3608/3618. 0.0502 s / img. ETA=0:00:00
[32m[12/19 02:12:22 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:52.884709 (0.064457 s / img per device, on 1 devices)
[32m[12/19 02:12:22 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:03:01 (0.050211 s / img per device, on 1 devices)
Evaluate predictions
We have 2442 single comp cutouts and 1176 multi
Plot predictions
Baseline assumption cat is 67.5% correct
train cat is 91.1% correct
35.01% improvement
[32m[12/19 02:12:25 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 02:12:25 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 02:12:25 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 02:12:25 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 02:12:25 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 02:12:25 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 02:12:25 d2.evaluation.evaluator]: [0m3.89%
[32m[12/19 02:12:25 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 02:12:25 d2.evaluation.evaluator]: [0m19.22%
[32m[12/19 02:12:25 d2.evaluation.evaluator]: [0mCatalogue is 0.9112769485903814 correct.
[32m[12/19 02:12:25 d2.engine.defaults]: [0mEvaluation results for train in csv format:
[32m[12/19 02:12:25 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 02:12:25 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 02:12:25 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0389,0.1922,0.9113
len dataset is: 1205 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_val.pkl
[32m[12/19 02:12:25 d2.data.common]: [0mSerializing 1205 elements to byte tensors and concatenating them all ...
[32m[12/19 02:12:25 d2.data.common]: [0mSerialized dataset takes 11.07 MiB
[32m[12/19 02:12:25 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 02:12:25 d2.evaluation.evaluator]: [0mStart inference on 1205 images
[32m[12/19 02:12:30 d2.evaluation.evaluator]: [0mInference done 99/1205. 0.0501 s / img. ETA=0:00:57
[32m[12/19 02:12:40 d2.evaluation.evaluator]: [0mInference done 289/1205. 0.0500 s / img. ETA=0:00:48
[32m[12/19 02:12:50 d2.evaluation.evaluator]: [0mInference done 474/1205. 0.0500 s / img. ETA=0:00:38
[32m[12/19 02:13:00 d2.evaluation.evaluator]: [0mInference done 655/1205. 0.0500 s / img. ETA=0:00:29
[32m[12/19 02:13:10 d2.evaluation.evaluator]: [0mInference done 833/1205. 0.0500 s / img. ETA=0:00:20
[32m[12/19 02:13:20 d2.evaluation.evaluator]: [0mInference done 1007/1205. 0.0500 s / img. ETA=0:00:10
[32m[12/19 02:13:30 d2.evaluation.evaluator]: [0mInference done 1179/1205. 0.0500 s / img. ETA=0:00:01
[32m[12/19 02:13:32 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:06.956749 (0.055797 s / img per device, on 1 devices)
[32m[12/19 02:13:32 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:00:59 (0.049977 s / img per device, on 1 devices)
Evaluate predictions
We have 817 single comp cutouts and 388 multi
Plot predictions
Baseline assumption cat is 67.8% correct
val cat is 89.7% correct
32.31% improvement
[32m[12/19 02:13:33 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 02:13:33 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 02:13:33 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 02:13:33 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 02:13:33 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 02:13:33 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 02:13:33 d2.evaluation.evaluator]: [0m4.41%
[32m[12/19 02:13:33 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 02:13:33 d2.evaluation.evaluator]: [0m22.68%
[32m[12/19 02:13:33 d2.evaluation.evaluator]: [0mCatalogue is 0.8970954356846473 correct.
[32m[12/19 02:13:33 d2.engine.defaults]: [0mEvaluation results for val in csv format:
[32m[12/19 02:13:33 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 02:13:33 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 02:13:33 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0441,0.2268,0.8971
len dataset is: 1211 /data1/mostertrij/data/frcnn_images/precomputed_bboxes/LGZ_COCOstyle/annotations/VIA_json_test.pkl
[32m[12/19 02:13:33 d2.data.common]: [0mSerializing 1211 elements to byte tensors and concatenating them all ...
[32m[12/19 02:13:33 d2.data.common]: [0mSerialized dataset takes 11.23 MiB
[32m[12/19 02:13:33 d2.data.dataset_mapper]: [0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(200, 200), max_size=200, sample_style='choice')]
[32m[12/19 02:13:33 d2.evaluation.evaluator]: [0mStart inference on 1211 images
[32m[12/19 02:13:40 d2.evaluation.evaluator]: [0mInference done 132/1211. 0.0499 s / img. ETA=0:00:55
[32m[12/19 02:13:50 d2.evaluation.evaluator]: [0mInference done 321/1211. 0.0500 s / img. ETA=0:00:46
[32m[12/19 02:14:00 d2.evaluation.evaluator]: [0mInference done 506/1211. 0.0500 s / img. ETA=0:00:37
[32m[12/19 02:14:10 d2.evaluation.evaluator]: [0mInference done 688/1211. 0.0499 s / img. ETA=0:00:28
[32m[12/19 02:14:20 d2.evaluation.evaluator]: [0mInference done 866/1211. 0.0499 s / img. ETA=0:00:18
[32m[12/19 02:14:30 d2.evaluation.evaluator]: [0mInference done 1041/1211. 0.0499 s / img. ETA=0:00:09
[32m[12/19 02:14:41 d2.evaluation.evaluator]: [0mTotal inference time: 0:01:07.124020 (0.055658 s / img per device, on 1 devices)
[32m[12/19 02:14:41 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:01:00 (0.049906 s / img per device, on 1 devices)
Evaluate predictions
We have 819 single comp cutouts and 392 multi
Plot predictions
Baseline assumption cat is 67.6% correct
test cat is 89.0% correct
31.62% improvement
[32m[12/19 02:14:42 d2.evaluation.evaluator]: [0mLOFAR Evaluation metrics (for all values 0% is best, 100% is worst):
[32m[12/19 02:14:42 d2.evaluation.evaluator]: [0m1. Pred. that fail to cover a single comp. source.
[32m[12/19 02:14:42 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 02:14:42 d2.evaluation.evaluator]: [0m2. Pred. that fail to cover all comp. of a multi-comp, source.
[32m[12/19 02:14:42 d2.evaluation.evaluator]: [0m0.00%
[32m[12/19 02:14:42 d2.evaluation.evaluator]: [0m3. Pred. that include unassociated comp. for a single comp. source.
[32m[12/19 02:14:42 d2.evaluation.evaluator]: [0m5.01%
[32m[12/19 02:14:42 d2.evaluation.evaluator]: [0m4. Pred. that include unassociated comp. for a multi-comp. source.
[32m[12/19 02:14:42 d2.evaluation.evaluator]: [0m23.47%
[32m[12/19 02:14:42 d2.evaluation.evaluator]: [0mCatalogue is 0.8901734104046243 correct.
[32m[12/19 02:14:42 d2.engine.defaults]: [0mEvaluation results for test in csv format:
[32m[12/19 02:14:42 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[12/19 02:14:42 d2.evaluation.testing]: [0mcopypaste: assoc_single_fail_fraction,assoc_multi_fail_fraction,unassoc_single_fail_fraction,unassoc_multi_fail_fraction,correct_catalogue
[32m[12/19 02:14:42 d2.evaluation.testing]: [0mcopypaste: 0.0000,0.0000,0.0501,0.2347,0.8902
[32m[12/19 02:14:42 d2.utils.events]: [0meta: 0:00:00  iter: 14999  total_loss: 0.006  loss_cls: 0.006  loss_box_reg: 0.000  time: 0.2130  data_time: 0.0154  lr: 0.000300  max_mem: 1829M
[32m[12/19 02:14:42 d2.engine.hooks]: [0mOverall training speed: 14997 iterations in 0:53:14 (0.2130 s / it)
[32m[12/19 02:14:42 d2.engine.hooks]: [0mTotal training time: 2:39:26 (1:46:11 on hooks)
Done training.
